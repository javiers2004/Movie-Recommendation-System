{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this Jupyter Notebook, 1 will develop a movie recommendation system that leverages deep learning techniques to predict user preferences based on their past ratings.\n",
    "\n",
    "I will cover the following steps:\n",
    "\n",
    "1. **Download and preprocess the MovieLens dataset**\n",
    "\n",
    "2. **Split the dataset into training, validation, and testing sets**\n",
    "\n",
    "3. **Implement a neural network architecture for the recommendation system**\n",
    "    \n",
    "4. **Train and evaluate the model using different metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first of all, I will load the data in the different variables links, movies, ratings and tags from the .csv.\n",
    "\n",
    "I will also display the first 5 rows of each one with the head() function and some metrics with the describe() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset: \n",
      "              movieId\n",
      "count    9742.000000\n",
      "mean    42200.353623\n",
      "std     52160.494854\n",
      "min         1.000000\n",
      "25%      3248.250000\n",
      "50%      7300.000000\n",
      "75%     76232.000000\n",
      "max    193609.000000\n",
      "---------------------------------------------\n",
      "First 5 rows: \n",
      "    movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#MOVIES DATASET\n",
    "movies = pd.read_csv(\"ml-latest-small/movies.csv\")\n",
    "print(\"Summary of the dataset: \\n\", movies.describe())\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"First 5 rows: \\n\", movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset: \n",
      "               userId        movieId         rating     timestamp\n",
      "count  100836.000000  100836.000000  100836.000000  1.008360e+05\n",
      "mean      326.127564   19435.295718       3.501557  1.205946e+09\n",
      "std       182.618491   35530.987199       1.042529  2.162610e+08\n",
      "min         1.000000       1.000000       0.500000  8.281246e+08\n",
      "25%       177.000000    1199.000000       3.000000  1.019124e+09\n",
      "50%       325.000000    2991.000000       3.500000  1.186087e+09\n",
      "75%       477.000000    8122.000000       4.000000  1.435994e+09\n",
      "max       610.000000  193609.000000       5.000000  1.537799e+09\n",
      "---------------------------------------------\n",
      "First 5 rows: \n",
      "    userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n"
     ]
    }
   ],
   "source": [
    "#RATINGS DATASET\n",
    "ratings = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "print(\"Summary of the dataset: \\n\",ratings.describe())\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"First 5 rows: \\n\", ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset: \n",
      "             userId        movieId     timestamp\n",
      "count  3683.000000    3683.000000  3.683000e+03\n",
      "mean    431.149335   27252.013576  1.320032e+09\n",
      "std     158.472553   43490.558803  1.721025e+08\n",
      "min       2.000000       1.000000  1.137179e+09\n",
      "25%     424.000000    1262.500000  1.137521e+09\n",
      "50%     474.000000    4454.000000  1.269833e+09\n",
      "75%     477.000000   39263.000000  1.498457e+09\n",
      "max     610.000000  193565.000000  1.537099e+09\n",
      "---------------------------------------------\n",
      "First 5 rows: \n",
      "    userId  movieId              tag   timestamp\n",
      "0       2    60756            funny  1445714994\n",
      "1       2    60756  Highly quotable  1445714996\n",
      "2       2    60756     will ferrell  1445714992\n",
      "3       2    89774     Boxing story  1445715207\n",
      "4       2    89774              MMA  1445715200\n"
     ]
    }
   ],
   "source": [
    "#TAGS DATASET\n",
    "tags = pd.read_csv(\"ml-latest-small/tags.csv\")\n",
    "print(\"Summary of the dataset: \\n\",tags.describe())\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"First 5 rows: \\n\", tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset: \n",
      "              movieId        imdbId         tmdbId\n",
      "count    9742.000000  9.742000e+03    9734.000000\n",
      "mean    42200.353623  6.771839e+05   55162.123793\n",
      "std     52160.494854  1.107228e+06   93653.481487\n",
      "min         1.000000  4.170000e+02       2.000000\n",
      "25%      3248.250000  9.518075e+04    9665.500000\n",
      "50%      7300.000000  1.672605e+05   16529.000000\n",
      "75%     76232.000000  8.055685e+05   44205.750000\n",
      "max    193609.000000  8.391976e+06  525662.000000\n",
      "---------------------------------------------\n",
      "First 5 rows: \n",
      "    movieId  imdbId   tmdbId\n",
      "0        1  114709    862.0\n",
      "1        2  113497   8844.0\n",
      "2        3  113228  15602.0\n",
      "3        4  114885  31357.0\n",
      "4        5  113041  11862.0\n"
     ]
    }
   ],
   "source": [
    "#LINKS DATASET\n",
    "links = pd.read_csv(\"ml-latest-small/links.csv\")\n",
    "print(\"Summary of the dataset: \\n\",links.describe())\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"First 5 rows: \\n\", links.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FIRST MODEL**\n",
    "# Using only original data (genres and ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title  (no genres listed)  Action  \\\n",
      "0        1                    Toy Story (1995)                   0       0   \n",
      "1        2                      Jumanji (1995)                   0       0   \n",
      "2        3             Grumpier Old Men (1995)                   0       0   \n",
      "3        4            Waiting to Exhale (1995)                   0       0   \n",
      "4        5  Father of the Bride Part II (1995)                   0       0   \n",
      "\n",
      "   Adventure  Animation  Children  Comedy  Crime  Documentary  ...  Film-Noir  \\\n",
      "0          1          1         1       1      0            0  ...          0   \n",
      "1          1          0         1       0      0            0  ...          0   \n",
      "2          0          0         0       1      0            0  ...          0   \n",
      "3          0          0         0       1      0            0  ...          0   \n",
      "4          0          0         0       1      0            0  ...          0   \n",
      "\n",
      "   Horror  IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
      "0       0     0        0        0        0       0         0    0        0  \n",
      "1       0     0        0        0        0       0         0    0        0  \n",
      "2       0     0        0        0        1       0         0    0        0  \n",
      "3       0     0        0        0        1       0         0    0        0  \n",
      "4       0     0        0        0        0       0         0    0        0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "   userId  movieId  rating  timestamp\n",
      "0       1        1       4  964982703\n",
      "1       1        3       4  964981247\n",
      "2       1        6       4  964982224\n",
      "3       1       47       5  964983815\n",
      "4       1       50       5  964982931\n",
      "   movieId  imdbId  tmdbId\n",
      "0        1  114709     862\n",
      "1        2  113497    8844\n",
      "2        3  113228   15602\n",
      "3        4  114885   31357\n",
      "4        5  113041   11862\n",
      "   userId  movieId              tag   timestamp\n",
      "0       2    60756            funny  1445714994\n",
      "1       2    60756  Highly quotable  1445714996\n",
      "2       2    60756     will ferrell  1445714992\n",
      "3       2    89774     Boxing story  1445715207\n",
      "4       2    89774              MMA  1445715200\n"
     ]
    }
   ],
   "source": [
    "movies[\"genres\"] = movies[\"genres\"].fillna(\"\").astype(str)\n",
    "movies[\"genres\"] = movies[\"genres\"].str.split('|')\n",
    "moviesExploded = movies.explode(\"genres\")\n",
    "movies_dummies = pd.get_dummies(moviesExploded[\"genres\"], prefix=\"\", prefix_sep=\"\", dtype=int)\n",
    "movies_dummies = moviesExploded[[\"movieId\"]].join(movies_dummies).groupby(\"movieId\").max()\n",
    "movies_final = movies.drop(columns=[\"genres\"]).merge(movies_dummies, on=\"movieId\")\n",
    "print(movies_final.head())\n",
    "#The feature genres is dummyfied\n",
    "\n",
    "ratings[\"rating\"] = ratings[\"rating\"].astype(int)\n",
    "print(ratings.head())\n",
    "\n",
    "links[\"tmdbId\"] = links[\"tmdbId\"].fillna(-1).astype(int)\n",
    "print(links.head())\n",
    "\n",
    "print(tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80668, 22) (80668, 1)\n",
      "(10084, 22) (10084, 1)\n",
      "(10084, 22) (10084, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df = ratings.merge(movies_final, on=\"movieId\", how=\"left\")\n",
    "#Ratings is merged with the data from movies.csv\n",
    "\n",
    "df2 = df.copy()\n",
    "numerical_col = [col for col in df2.columns if col not in [\"title\"]]\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\", \"title\"])\n",
    "#Features timestamp and title are removed\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.8), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.8):int(len(df_shuffle) * 0.9), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.9):, :]\n",
    "#Data is splited into training, validation and test sets(80-10-10)\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns if col not in [\"rating\", \"timestamp\", \"title\"]]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "#Features transformed into numerical and  divided into input and output/target variable\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()\n",
    "#Sets transformed into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId                False\n",
      "movieId               False\n",
      "rating                False\n",
      "(no genres listed)    False\n",
      "Action                False\n",
      "Adventure             False\n",
      "Animation             False\n",
      "Children              False\n",
      "Comedy                False\n",
      "Crime                 False\n",
      "Documentary           False\n",
      "Drama                 False\n",
      "Fantasy               False\n",
      "Film-Noir             False\n",
      "Horror                False\n",
      "IMAX                  False\n",
      "Musical               False\n",
      "Mystery               False\n",
      "Romance               False\n",
      "Sci-Fi                False\n",
      "Thriller              False\n",
      "War                   False\n",
      "Western               False\n",
      "avg_movie_rating      False\n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "dtype: bool\n",
      "   userId  movieId  rating  timestamp                        title  \\\n",
      "0       1        1       4  964982703             Toy Story (1995)   \n",
      "1       1        3       4  964981247      Grumpier Old Men (1995)   \n",
      "2       1        6       4  964982224                  Heat (1995)   \n",
      "3       1       47       5  964983815  Seven (a.k.a. Se7en) (1995)   \n",
      "4       1       50       5  964982931   Usual Suspects, The (1995)   \n",
      "\n",
      "   (no genres listed)  Action  Adventure  Animation  Children  ...  Film-Noir  \\\n",
      "0                   0       0          1          1         1  ...          0   \n",
      "1                   0       0          0          0         0  ...          0   \n",
      "2                   0       1          0          0         0  ...          0   \n",
      "3                   0       0          0          0         0  ...          0   \n",
      "4                   0       0          0          0         0  ...          0   \n",
      "\n",
      "   Horror  IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
      "0       0     0        0        0        0       0         0    0        0  \n",
      "1       0     0        0        0        1       0         0    0        0  \n",
      "2       0     0        0        0        0       0         1    0        0  \n",
      "3       0     0        0        1        0       0         1    0        0  \n",
      "4       0     0        0        1        0       0         1    0        0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Here, I have created for each set (train, validation and testing) new 6 variables. For training and validation, the data related to the average of the \n",
    "#rating is taken from the set itself, but the columns of the testing set can not be created from the average of the rating of the training set (would be considered data leakage)\n",
    "#So the solution is using the average from the data of the training set\n",
    "#NAs are also removed\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_test).any())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = [\"movieId\", \"rating\", \"userId\", \"std_movie_rating\",  \"avg_movie_rating\",  \"count_movie_rating\", \"avg_user_rating\",  \"count_user_rating\",  \"std_user_rating\"]\n",
    "scaler = MinMaxScaler()\n",
    "#Only numerical columns are normalized\n",
    "\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.fit_transform(df_val[numerical_col])\n",
    "df_test[numerical_col] = scaler.fit_transform(df_test[numerical_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "#Tensors are transformed into TensorDatasets and then into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "#The neural network is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy()) \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.052538, MSE: 0.105261, RMSE: 0.324439, R²: -1.223391\n",
      "VALIDATION -> Loss: 0.017285, MSE: 0.034591, RMSE: 0.185986, R²: 0.286081\n",
      "Iteration 2/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019013, MSE: 0.038034, RMSE: 0.195022, R²: 0.196624\n",
      "VALIDATION -> Loss: 0.013096, MSE: 0.026218, RMSE: 0.161921, R²: 0.458879\n",
      "Iteration 3/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017171, MSE: 0.034358, RMSE: 0.185360, R²: 0.274259\n",
      "VALIDATION -> Loss: 0.011494, MSE: 0.023015, RMSE: 0.151708, R²: 0.524987\n",
      "Iteration 4/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016355, MSE: 0.032714, RMSE: 0.180870, R²: 0.308993\n",
      "VALIDATION -> Loss: 0.010864, MSE: 0.021754, RMSE: 0.147491, R²: 0.551029\n",
      "Iteration 5/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015886, MSE: 0.031773, RMSE: 0.178251, R²: 0.328862\n",
      "VALIDATION -> Loss: 0.010596, MSE: 0.021222, RMSE: 0.145676, R²: 0.562010\n",
      "Iteration 6/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015548, MSE: 0.031096, RMSE: 0.176340, R²: 0.343169\n",
      "VALIDATION -> Loss: 0.010442, MSE: 0.020912, RMSE: 0.144609, R²: 0.568403\n",
      "Iteration 7/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015337, MSE: 0.030674, RMSE: 0.175141, R²: 0.352075\n",
      "VALIDATION -> Loss: 0.010358, MSE: 0.020744, RMSE: 0.144026, R²: 0.571876\n",
      "Iteration 8/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015173, MSE: 0.030349, RMSE: 0.174210, R²: 0.358942\n",
      "VALIDATION -> Loss: 0.010314, MSE: 0.020658, RMSE: 0.143731, R²: 0.573631\n",
      "Iteration 9/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015048, MSE: 0.030093, RMSE: 0.173473, R²: 0.364360\n",
      "VALIDATION -> Loss: 0.010406, MSE: 0.020839, RMSE: 0.144358, R²: 0.569899\n",
      "Iteration 10/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014929, MSE: 0.029865, RMSE: 0.172814, R²: 0.369176\n",
      "VALIDATION -> Loss: 0.010265, MSE: 0.020561, RMSE: 0.143391, R²: 0.575643\n",
      "Iteration 11/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014815, MSE: 0.029634, RMSE: 0.172144, R²: 0.374057\n",
      "VALIDATION -> Loss: 0.010322, MSE: 0.020674, RMSE: 0.143784, R²: 0.573314\n",
      "Iteration 12/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014727, MSE: 0.029457, RMSE: 0.171630, R²: 0.377794\n",
      "VALIDATION -> Loss: 0.010578, MSE: 0.021188, RMSE: 0.145562, R²: 0.562694\n",
      "Iteration 13/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014683, MSE: 0.029363, RMSE: 0.171358, R²: 0.379765\n",
      "VALIDATION -> Loss: 0.010044, MSE: 0.020119, RMSE: 0.141842, R²: 0.584762\n",
      "Iteration 14/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014627, MSE: 0.029253, RMSE: 0.171035, R²: 0.382096\n",
      "VALIDATION -> Loss: 0.010101, MSE: 0.020231, RMSE: 0.142237, R²: 0.582443\n",
      "Iteration 15/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014599, MSE: 0.029204, RMSE: 0.170891, R²: 0.383136\n",
      "VALIDATION -> Loss: 0.009957, MSE: 0.019946, RMSE: 0.141232, R²: 0.588329\n",
      "Iteration 16/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014469, MSE: 0.028934, RMSE: 0.170101, R²: 0.388829\n",
      "VALIDATION -> Loss: 0.010505, MSE: 0.021044, RMSE: 0.145066, R²: 0.565669\n",
      "Iteration 17/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014392, MSE: 0.028791, RMSE: 0.169678, R²: 0.391861\n",
      "VALIDATION -> Loss: 0.010325, MSE: 0.020683, RMSE: 0.143816, R²: 0.573125\n",
      "Iteration 18/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014383, MSE: 0.028766, RMSE: 0.169606, R²: 0.392381\n",
      "VALIDATION -> Loss: 0.010075, MSE: 0.020184, RMSE: 0.142071, R²: 0.583420\n",
      "Iteration 19/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014376, MSE: 0.028755, RMSE: 0.169574, R²: 0.392609\n",
      "VALIDATION -> Loss: 0.010213, MSE: 0.020460, RMSE: 0.143037, R²: 0.577736\n",
      "Iteration 20/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014234, MSE: 0.028473, RMSE: 0.168739, R²: 0.398573\n",
      "VALIDATION -> Loss: 0.009810, MSE: 0.019652, RMSE: 0.140184, R²: 0.594410\n",
      "Iteration 21/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014217, MSE: 0.028429, RMSE: 0.168608, R²: 0.399508\n",
      "VALIDATION -> Loss: 0.010047, MSE: 0.020125, RMSE: 0.141864, R²: 0.584632\n",
      "Iteration 22/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014162, MSE: 0.028320, RMSE: 0.168285, R²: 0.401809\n",
      "VALIDATION -> Loss: 0.010142, MSE: 0.020317, RMSE: 0.142539, R²: 0.580672\n",
      "Iteration 23/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014112, MSE: 0.028223, RMSE: 0.167998, R²: 0.403846\n",
      "VALIDATION -> Loss: 0.009791, MSE: 0.019617, RMSE: 0.140059, R²: 0.595134\n",
      "Iteration 24/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014105, MSE: 0.028202, RMSE: 0.167935, R²: 0.404290\n",
      "VALIDATION -> Loss: 0.009848, MSE: 0.019727, RMSE: 0.140452, R²: 0.592860\n",
      "Iteration 25/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014022, MSE: 0.028042, RMSE: 0.167457, R²: 0.407683\n",
      "VALIDATION -> Loss: 0.009745, MSE: 0.019524, RMSE: 0.139727, R²: 0.597055\n",
      "Iteration 26/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013996, MSE: 0.027990, RMSE: 0.167302, R²: 0.408774\n",
      "VALIDATION -> Loss: 0.009608, MSE: 0.019249, RMSE: 0.138742, R²: 0.602711\n",
      "Iteration 27/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014000, MSE: 0.028004, RMSE: 0.167344, R²: 0.408478\n",
      "VALIDATION -> Loss: 0.009878, MSE: 0.019789, RMSE: 0.140675, R²: 0.591570\n",
      "Iteration 28/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014009, MSE: 0.028018, RMSE: 0.167385, R²: 0.408191\n",
      "VALIDATION -> Loss: 0.010243, MSE: 0.020521, RMSE: 0.143251, R²: 0.576473\n",
      "Iteration 29/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013995, MSE: 0.027990, RMSE: 0.167301, R²: 0.408783\n",
      "VALIDATION -> Loss: 0.009644, MSE: 0.019320, RMSE: 0.138996, R²: 0.601259\n",
      "Iteration 30/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013880, MSE: 0.027756, RMSE: 0.166603, R²: 0.413708\n",
      "VALIDATION -> Loss: 0.009507, MSE: 0.019047, RMSE: 0.138011, R²: 0.606889\n",
      "Iteration 31/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013887, MSE: 0.027777, RMSE: 0.166664, R²: 0.413279\n",
      "VALIDATION -> Loss: 0.009720, MSE: 0.019472, RMSE: 0.139543, R²: 0.598114\n",
      "Iteration 32/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013872, MSE: 0.027745, RMSE: 0.166567, R²: 0.413957\n",
      "VALIDATION -> Loss: 0.009985, MSE: 0.020004, RMSE: 0.141436, R²: 0.587135\n",
      "Iteration 33/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013873, MSE: 0.027751, RMSE: 0.166585, R²: 0.413830\n",
      "VALIDATION -> Loss: 0.009822, MSE: 0.019679, RMSE: 0.140284, R²: 0.593836\n",
      "Iteration 34/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013807, MSE: 0.027616, RMSE: 0.166181, R²: 0.416669\n",
      "VALIDATION -> Loss: 0.009489, MSE: 0.019009, RMSE: 0.137875, R²: 0.607667\n",
      "Iteration 35/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013795, MSE: 0.027589, RMSE: 0.166098, R²: 0.417253\n",
      "VALIDATION -> Loss: 0.009589, MSE: 0.019211, RMSE: 0.138605, R²: 0.603497\n",
      "Iteration 36/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013762, MSE: 0.027519, RMSE: 0.165888, R²: 0.418724\n",
      "VALIDATION -> Loss: 0.009673, MSE: 0.019380, RMSE: 0.139213, R²: 0.600012\n",
      "Iteration 37/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013786, MSE: 0.027571, RMSE: 0.166045, R²: 0.417625\n",
      "VALIDATION -> Loss: 0.009721, MSE: 0.019476, RMSE: 0.139556, R²: 0.598038\n",
      "Iteration 38/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013752, MSE: 0.027512, RMSE: 0.165867, R²: 0.418874\n",
      "VALIDATION -> Loss: 0.009811, MSE: 0.019656, RMSE: 0.140199, R²: 0.594327\n",
      "Iteration 39/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013728, MSE: 0.027441, RMSE: 0.165653, R²: 0.420370\n",
      "VALIDATION -> Loss: 0.009565, MSE: 0.019164, RMSE: 0.138436, R²: 0.604466\n",
      "Iteration 40/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013656, MSE: 0.027313, RMSE: 0.165267, R²: 0.423074\n",
      "VALIDATION -> Loss: 0.009634, MSE: 0.019301, RMSE: 0.138927, R²: 0.601653\n",
      "Iteration 41/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013711, MSE: 0.027410, RMSE: 0.165560, R²: 0.421021\n",
      "VALIDATION -> Loss: 0.009774, MSE: 0.019584, RMSE: 0.139944, R²: 0.595803\n",
      "Iteration 42/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013665, MSE: 0.027331, RMSE: 0.165321, R²: 0.422694\n",
      "VALIDATION -> Loss: 0.009575, MSE: 0.019188, RMSE: 0.138519, R²: 0.603988\n",
      "Iteration 43/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013661, MSE: 0.027317, RMSE: 0.165280, R²: 0.422981\n",
      "VALIDATION -> Loss: 0.009625, MSE: 0.019284, RMSE: 0.138869, R²: 0.601989\n",
      "Iteration 44/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013618, MSE: 0.027242, RMSE: 0.165052, R²: 0.424569\n",
      "VALIDATION -> Loss: 0.009620, MSE: 0.019273, RMSE: 0.138827, R²: 0.602225\n",
      "Iteration 45/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013592, MSE: 0.027184, RMSE: 0.164875, R²: 0.425808\n",
      "VALIDATION -> Loss: 0.009733, MSE: 0.019499, RMSE: 0.139641, R²: 0.597551\n",
      "Iteration 46/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013590, MSE: 0.027179, RMSE: 0.164859, R²: 0.425914\n",
      "VALIDATION -> Loss: 0.009468, MSE: 0.018970, RMSE: 0.137730, R²: 0.608487\n",
      "Iteration 47/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013574, MSE: 0.027143, RMSE: 0.164751, R²: 0.426670\n",
      "VALIDATION -> Loss: 0.009569, MSE: 0.019171, RMSE: 0.138461, R²: 0.604323\n",
      "Iteration 48/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013538, MSE: 0.027063, RMSE: 0.164508, R²: 0.428358\n",
      "VALIDATION -> Loss: 0.009698, MSE: 0.019428, RMSE: 0.139383, R²: 0.599035\n",
      "Iteration 49/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013533, MSE: 0.027065, RMSE: 0.164516, R²: 0.428305\n",
      "VALIDATION -> Loss: 0.009912, MSE: 0.019857, RMSE: 0.140913, R²: 0.590181\n",
      "Iteration 50/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013552, MSE: 0.027102, RMSE: 0.164627, R²: 0.427531\n",
      "VALIDATION -> Loss: 0.009507, MSE: 0.019048, RMSE: 0.138014, R²: 0.606875\n"
     ]
    }
   ],
   "source": [
    "#The model is trained and validated for 50 epochs\n",
    "for i in range(50): \n",
    "    print(f\"Iteration {i+1}/50 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4452\n",
      "MAE: 0.1217\n",
      "MSE: 0.0263\n",
      "RMSE: 0.1621\n",
      "Precisión: 0.6199\n",
      "Recall: 0.9309\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#And display the evaluation metrics of the model\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SECOND MODEL**\n",
    "# Adding to the first model the data from TMBD \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb = pd.read_csv(\"ml-latest-small/movie_info_tmdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "df = tmdb.merge(links, on=\"tmdbId\", how=\"left\")\n",
    "df = df.merge(ratings, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df2 = df.copy()\n",
    "df2 = df2.drop(columns=[\"title\"])\n",
    "df2[\"release_date\"] = pd.to_datetime(df2[\"release_date\"], errors=\"coerce\").dt.year\n",
    "df2 = pd.get_dummies(df2, columns=[\"original_language\"], dtype=float)\n",
    "columns_to_convert = [col for col in df2.columns if col != \"title\"]\n",
    "df2[columns_to_convert] = df2[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "numerical_col = [col for col in df2.columns if col not in [\"title\"]]\n",
    "#Only the year is taken from release_date, title is removed and original_language is dummyfied\n",
    "#The TMDB data, links.csv and ratings.csv are merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61074, 58) (61074, 1)\n",
      "(20358, 58) (20358, 1)\n",
      "(20358, 58) (20358, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\"])\n",
    "df_shuffle = df_shuffle.dropna()\n",
    "#Feature timestamp is removed and NAs are removed\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.6), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.6):int(len(df_shuffle) * 0.8), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.8):, :]\n",
    "#Data is split into train, validate and testing sets(60-20-20)\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns if col not in [\"rating\", \"timestamp\", \"title\"]]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "#Features transformed into numerical and  divided into input and output/target variable\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmdbId                False\n",
      "ratingTmdb            False\n",
      "release_date          False\n",
      "votes                 False\n",
      "budget                False\n",
      "                      ...  \n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "Length: 65, dtype: bool\n",
      "   tmdbId  ratingTmdb  release_date    votes      budget      revenue  \\\n",
      "0     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "1     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "2     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "3     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "4     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "\n",
      "   runtime  movieId  imdbId  userId  ...  original_language_sr  \\\n",
      "0     81.0        1  114709     1.0  ...                   0.0   \n",
      "1     81.0        1  114709     5.0  ...                   0.0   \n",
      "2     81.0        1  114709     7.0  ...                   0.0   \n",
      "3     81.0        1  114709    15.0  ...                   0.0   \n",
      "4     81.0        1  114709    17.0  ...                   0.0   \n",
      "\n",
      "   original_language_sv  original_language_ta  original_language_th  \\\n",
      "0                   0.0                   0.0                   0.0   \n",
      "1                   0.0                   0.0                   0.0   \n",
      "2                   0.0                   0.0                   0.0   \n",
      "3                   0.0                   0.0                   0.0   \n",
      "4                   0.0                   0.0                   0.0   \n",
      "\n",
      "   original_language_tn  original_language_tr  original_language_vi  \\\n",
      "0                   0.0                   0.0                   0.0   \n",
      "1                   0.0                   0.0                   0.0   \n",
      "2                   0.0                   0.0                   0.0   \n",
      "3                   0.0                   0.0                   0.0   \n",
      "4                   0.0                   0.0                   0.0   \n",
      "\n",
      "   original_language_wo  original_language_xx  original_language_zh  \n",
      "0                   0.0                   0.0                   0.0  \n",
      "1                   0.0                   0.0                   0.0  \n",
      "2                   0.0                   0.0                   0.0  \n",
      "3                   0.0                   0.0                   0.0  \n",
      "4                   0.0                   0.0                   0.0  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Here, I have created for each set (train, validation and testing) new 6 variables. For training and validation, the data related to the average of the \n",
    "#rating is taken from the set itself, but the columns of the testing set can not be created from the average of the rating of the training set (would be considered data leakage)\n",
    "#So the solution is using the average from the data of the training set\n",
    "#NAs are also removed\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_train).any())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61074, 65)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "#Only numerical columns are normalized\n",
    "\n",
    "\n",
    "numerical_col = df_train.select_dtypes(include=['number']).columns\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.transform(df_val[numerical_col])  \n",
    "df_test[numerical_col] = scaler.transform(df_test[numerical_col]) \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
    "#Tensors are transformed into TensorDatasets and then into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "#The neural network is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy())  \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.080063, MSE: 0.160725, RMSE: 0.400905, R²: -2.385594\n",
      "VALIDATION -> Loss: 0.018461, MSE: 0.036931, RMSE: 0.192174, R²: 0.216698\n",
      "Iteration 2/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.028573, MSE: 0.057176, RMSE: 0.239116, R²: -0.204393\n",
      "VALIDATION -> Loss: 0.013844, MSE: 0.027692, RMSE: 0.166410, R²: 0.412644\n",
      "Iteration 3/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.023259, MSE: 0.046514, RMSE: 0.215672, R²: 0.020196\n",
      "VALIDATION -> Loss: 0.012744, MSE: 0.025492, RMSE: 0.159662, R²: 0.459316\n",
      "Iteration 4/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.021594, MSE: 0.043196, RMSE: 0.207835, R²: 0.090104\n",
      "VALIDATION -> Loss: 0.012352, MSE: 0.024709, RMSE: 0.157191, R²: 0.475918\n",
      "Iteration 5/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020822, MSE: 0.041662, RMSE: 0.204113, R²: 0.122402\n",
      "VALIDATION -> Loss: 0.012488, MSE: 0.024980, RMSE: 0.158051, R²: 0.470171\n",
      "Iteration 6/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020135, MSE: 0.040276, RMSE: 0.200689, R²: 0.151599\n",
      "VALIDATION -> Loss: 0.012221, MSE: 0.024445, RMSE: 0.156351, R²: 0.481510\n",
      "Iteration 7/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019562, MSE: 0.039150, RMSE: 0.197863, R²: 0.175325\n",
      "VALIDATION -> Loss: 0.012318, MSE: 0.024641, RMSE: 0.156976, R²: 0.477353\n",
      "Iteration 8/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019253, MSE: 0.038510, RMSE: 0.196240, R²: 0.188805\n",
      "VALIDATION -> Loss: 0.011974, MSE: 0.023952, RMSE: 0.154764, R²: 0.491977\n",
      "Iteration 9/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018797, MSE: 0.037603, RMSE: 0.193914, R²: 0.207918\n",
      "VALIDATION -> Loss: 0.011788, MSE: 0.023581, RMSE: 0.153560, R²: 0.499850\n",
      "Iteration 10/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018255, MSE: 0.036515, RMSE: 0.191088, R²: 0.230832\n",
      "VALIDATION -> Loss: 0.011888, MSE: 0.023781, RMSE: 0.154211, R²: 0.495602\n",
      "Iteration 11/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018150, MSE: 0.036292, RMSE: 0.190506, R²: 0.235517\n",
      "VALIDATION -> Loss: 0.011798, MSE: 0.023601, RMSE: 0.153625, R²: 0.499428\n",
      "Iteration 12/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018059, MSE: 0.036136, RMSE: 0.190096, R²: 0.238803\n",
      "VALIDATION -> Loss: 0.012116, MSE: 0.024238, RMSE: 0.155685, R²: 0.485914\n",
      "Iteration 13/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017715, MSE: 0.035449, RMSE: 0.188280, R²: 0.253278\n",
      "VALIDATION -> Loss: 0.012337, MSE: 0.024681, RMSE: 0.157102, R²: 0.476515\n",
      "Iteration 14/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017499, MSE: 0.034990, RMSE: 0.187056, R²: 0.262953\n",
      "VALIDATION -> Loss: 0.011585, MSE: 0.023176, RMSE: 0.152235, R²: 0.508446\n",
      "Iteration 15/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017259, MSE: 0.034529, RMSE: 0.185821, R²: 0.272654\n",
      "VALIDATION -> Loss: 0.011492, MSE: 0.022991, RMSE: 0.151629, R²: 0.512354\n",
      "Iteration 16/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017013, MSE: 0.034034, RMSE: 0.184483, R²: 0.283087\n",
      "VALIDATION -> Loss: 0.011827, MSE: 0.023661, RMSE: 0.153820, R²: 0.498158\n",
      "Iteration 17/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016852, MSE: 0.033691, RMSE: 0.183552, R²: 0.290310\n",
      "VALIDATION -> Loss: 0.011721, MSE: 0.023447, RMSE: 0.153125, R²: 0.502682\n",
      "Iteration 18/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016811, MSE: 0.033605, RMSE: 0.183318, R²: 0.292115\n",
      "VALIDATION -> Loss: 0.011462, MSE: 0.022930, RMSE: 0.151426, R²: 0.513656\n",
      "Iteration 19/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016688, MSE: 0.033365, RMSE: 0.182660, R²: 0.297185\n",
      "VALIDATION -> Loss: 0.011400, MSE: 0.022807, RMSE: 0.151019, R²: 0.516271\n",
      "Iteration 20/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016673, MSE: 0.033336, RMSE: 0.182582, R²: 0.297786\n",
      "VALIDATION -> Loss: 0.011462, MSE: 0.022932, RMSE: 0.151433, R²: 0.513611\n",
      "Iteration 21/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016311, MSE: 0.032620, RMSE: 0.180609, R²: 0.312880\n",
      "VALIDATION -> Loss: 0.011298, MSE: 0.022603, RMSE: 0.150342, R²: 0.520593\n",
      "Iteration 22/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016244, MSE: 0.032497, RMSE: 0.180270, R²: 0.315460\n",
      "VALIDATION -> Loss: 0.011297, MSE: 0.022602, RMSE: 0.150339, R²: 0.520614\n",
      "Iteration 23/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016089, MSE: 0.032185, RMSE: 0.179402, R²: 0.322036\n",
      "VALIDATION -> Loss: 0.011294, MSE: 0.022595, RMSE: 0.150316, R²: 0.520760\n",
      "Iteration 24/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016040, MSE: 0.032074, RMSE: 0.179092, R²: 0.324378\n",
      "VALIDATION -> Loss: 0.011747, MSE: 0.023502, RMSE: 0.153305, R²: 0.501512\n",
      "Iteration 25/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015932, MSE: 0.031860, RMSE: 0.178493, R²: 0.328891\n",
      "VALIDATION -> Loss: 0.011293, MSE: 0.022592, RMSE: 0.150306, R²: 0.520822\n",
      "Iteration 26/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015775, MSE: 0.031503, RMSE: 0.177492, R²: 0.336395\n",
      "VALIDATION -> Loss: 0.011752, MSE: 0.023512, RMSE: 0.153338, R²: 0.501299\n",
      "Iteration 27/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015646, MSE: 0.031293, RMSE: 0.176898, R²: 0.340832\n",
      "VALIDATION -> Loss: 0.011492, MSE: 0.022994, RMSE: 0.151637, R²: 0.512300\n",
      "Iteration 28/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015544, MSE: 0.031072, RMSE: 0.176272, R²: 0.345485\n",
      "VALIDATION -> Loss: 0.011282, MSE: 0.022574, RMSE: 0.150247, R²: 0.521201\n",
      "Iteration 29/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015477, MSE: 0.030955, RMSE: 0.175940, R²: 0.347951\n",
      "VALIDATION -> Loss: 0.011389, MSE: 0.022788, RMSE: 0.150956, R²: 0.516671\n",
      "Iteration 30/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015332, MSE: 0.030659, RMSE: 0.175096, R²: 0.354187\n",
      "VALIDATION -> Loss: 0.011512, MSE: 0.023032, RMSE: 0.151764, R²: 0.511482\n",
      "Iteration 31/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015294, MSE: 0.030577, RMSE: 0.174864, R²: 0.355900\n",
      "VALIDATION -> Loss: 0.011289, MSE: 0.022587, RMSE: 0.150289, R²: 0.520936\n",
      "Iteration 32/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015204, MSE: 0.030404, RMSE: 0.174368, R²: 0.359551\n",
      "VALIDATION -> Loss: 0.011382, MSE: 0.022772, RMSE: 0.150904, R²: 0.517004\n",
      "Iteration 33/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015129, MSE: 0.030263, RMSE: 0.173964, R²: 0.362514\n",
      "VALIDATION -> Loss: 0.011336, MSE: 0.022680, RMSE: 0.150598, R²: 0.518964\n",
      "Iteration 34/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015084, MSE: 0.030157, RMSE: 0.173658, R²: 0.364750\n",
      "VALIDATION -> Loss: 0.011319, MSE: 0.022646, RMSE: 0.150486, R²: 0.519676\n",
      "Iteration 35/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014909, MSE: 0.029814, RMSE: 0.172668, R²: 0.371974\n",
      "VALIDATION -> Loss: 0.011312, MSE: 0.022634, RMSE: 0.150447, R²: 0.519927\n",
      "Iteration 36/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014968, MSE: 0.029930, RMSE: 0.173002, R²: 0.369547\n",
      "VALIDATION -> Loss: 0.011310, MSE: 0.022626, RMSE: 0.150420, R²: 0.520098\n",
      "Iteration 37/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014888, MSE: 0.029766, RMSE: 0.172529, R²: 0.372986\n",
      "VALIDATION -> Loss: 0.011287, MSE: 0.022582, RMSE: 0.150272, R²: 0.521039\n",
      "Iteration 38/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014754, MSE: 0.029514, RMSE: 0.171795, R²: 0.378308\n",
      "VALIDATION -> Loss: 0.011405, MSE: 0.022817, RMSE: 0.151053, R²: 0.516048\n",
      "Iteration 39/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014734, MSE: 0.029480, RMSE: 0.171697, R²: 0.379019\n",
      "VALIDATION -> Loss: 0.011242, MSE: 0.022491, RMSE: 0.149969, R²: 0.522968\n",
      "Iteration 40/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014631, MSE: 0.029256, RMSE: 0.171043, R²: 0.383739\n",
      "VALIDATION -> Loss: 0.011274, MSE: 0.022557, RMSE: 0.150189, R²: 0.521570\n",
      "Iteration 41/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014551, MSE: 0.029097, RMSE: 0.170580, R²: 0.387076\n",
      "VALIDATION -> Loss: 0.011363, MSE: 0.022735, RMSE: 0.150781, R²: 0.517791\n",
      "Iteration 42/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014542, MSE: 0.029079, RMSE: 0.170526, R²: 0.387462\n",
      "VALIDATION -> Loss: 0.011209, MSE: 0.022426, RMSE: 0.149753, R²: 0.524344\n",
      "Iteration 43/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014443, MSE: 0.028879, RMSE: 0.169939, R²: 0.391674\n",
      "VALIDATION -> Loss: 0.011221, MSE: 0.022450, RMSE: 0.149834, R²: 0.523827\n",
      "Iteration 44/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014297, MSE: 0.028613, RMSE: 0.169152, R²: 0.397290\n",
      "VALIDATION -> Loss: 0.011345, MSE: 0.022698, RMSE: 0.150659, R²: 0.518570\n",
      "Iteration 45/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014319, MSE: 0.028638, RMSE: 0.169228, R²: 0.396754\n",
      "VALIDATION -> Loss: 0.011191, MSE: 0.022389, RMSE: 0.149630, R²: 0.525123\n",
      "Iteration 46/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014264, MSE: 0.028523, RMSE: 0.168886, R²: 0.399183\n",
      "VALIDATION -> Loss: 0.011219, MSE: 0.022445, RMSE: 0.149817, R²: 0.523941\n",
      "Iteration 47/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014178, MSE: 0.028374, RMSE: 0.168445, R²: 0.402318\n",
      "VALIDATION -> Loss: 0.011298, MSE: 0.022604, RMSE: 0.150346, R²: 0.520568\n",
      "Iteration 48/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014115, MSE: 0.028240, RMSE: 0.168048, R²: 0.405132\n",
      "VALIDATION -> Loss: 0.011311, MSE: 0.022628, RMSE: 0.150427, R²: 0.520056\n",
      "Iteration 49/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014076, MSE: 0.028144, RMSE: 0.167761, R²: 0.407162\n",
      "VALIDATION -> Loss: 0.011439, MSE: 0.022887, RMSE: 0.151286, R²: 0.514558\n",
      "Iteration 50/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014062, MSE: 0.028129, RMSE: 0.167717, R²: 0.407476\n",
      "VALIDATION -> Loss: 0.011450, MSE: 0.022909, RMSE: 0.151358, R²: 0.514097\n",
      "Iteration 51/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014068, MSE: 0.028127, RMSE: 0.167710, R²: 0.407521\n",
      "VALIDATION -> Loss: 0.011224, MSE: 0.022456, RMSE: 0.149852, R²: 0.523714\n",
      "Iteration 52/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014013, MSE: 0.028018, RMSE: 0.167387, R²: 0.409803\n",
      "VALIDATION -> Loss: 0.011220, MSE: 0.022446, RMSE: 0.149821, R²: 0.523911\n",
      "Iteration 53/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013961, MSE: 0.027910, RMSE: 0.167064, R²: 0.412080\n",
      "VALIDATION -> Loss: 0.011278, MSE: 0.022562, RMSE: 0.150208, R²: 0.521450\n",
      "Iteration 54/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013945, MSE: 0.027879, RMSE: 0.166971, R²: 0.412733\n",
      "VALIDATION -> Loss: 0.011323, MSE: 0.022654, RMSE: 0.150513, R²: 0.519504\n",
      "Iteration 55/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013872, MSE: 0.027752, RMSE: 0.166590, R²: 0.415408\n",
      "VALIDATION -> Loss: 0.011240, MSE: 0.022487, RMSE: 0.149957, R²: 0.523046\n",
      "Iteration 56/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013765, MSE: 0.027531, RMSE: 0.165925, R²: 0.420070\n",
      "VALIDATION -> Loss: 0.011408, MSE: 0.022823, RMSE: 0.151072, R²: 0.515931\n",
      "Iteration 57/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013813, MSE: 0.027614, RMSE: 0.166174, R²: 0.418328\n",
      "VALIDATION -> Loss: 0.011260, MSE: 0.022528, RMSE: 0.150093, R²: 0.522182\n",
      "Iteration 58/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013678, MSE: 0.027366, RMSE: 0.165426, R²: 0.423550\n",
      "VALIDATION -> Loss: 0.011362, MSE: 0.022733, RMSE: 0.150776, R²: 0.517822\n",
      "Iteration 59/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013808, MSE: 0.027586, RMSE: 0.166092, R²: 0.418905\n",
      "VALIDATION -> Loss: 0.011567, MSE: 0.023142, RMSE: 0.152126, R²: 0.509152\n",
      "Iteration 60/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013764, MSE: 0.027541, RMSE: 0.165954, R²: 0.419866\n",
      "VALIDATION -> Loss: 0.011244, MSE: 0.022495, RMSE: 0.149983, R²: 0.522880\n",
      "Iteration 61/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013643, MSE: 0.027286, RMSE: 0.165186, R²: 0.425226\n",
      "VALIDATION -> Loss: 0.011503, MSE: 0.023014, RMSE: 0.151702, R²: 0.511881\n",
      "Iteration 62/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013642, MSE: 0.027285, RMSE: 0.165181, R²: 0.425261\n",
      "VALIDATION -> Loss: 0.011224, MSE: 0.022456, RMSE: 0.149853, R²: 0.523709\n",
      "Iteration 63/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013586, MSE: 0.027167, RMSE: 0.164826, R²: 0.427729\n",
      "VALIDATION -> Loss: 0.011293, MSE: 0.022596, RMSE: 0.150320, R²: 0.520736\n",
      "Iteration 64/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013580, MSE: 0.027176, RMSE: 0.164852, R²: 0.427548\n",
      "VALIDATION -> Loss: 0.011307, MSE: 0.022623, RMSE: 0.150408, R²: 0.520174\n",
      "Iteration 65/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013554, MSE: 0.027106, RMSE: 0.164637, R²: 0.429035\n",
      "VALIDATION -> Loss: 0.011306, MSE: 0.022622, RMSE: 0.150405, R²: 0.520192\n",
      "Iteration 66/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013557, MSE: 0.027114, RMSE: 0.164665, R²: 0.428847\n",
      "VALIDATION -> Loss: 0.011214, MSE: 0.022435, RMSE: 0.149784, R²: 0.524145\n",
      "Iteration 67/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013529, MSE: 0.027060, RMSE: 0.164500, R²: 0.429985\n",
      "VALIDATION -> Loss: 0.011474, MSE: 0.022958, RMSE: 0.151518, R²: 0.513066\n",
      "Iteration 68/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013540, MSE: 0.027089, RMSE: 0.164586, R²: 0.429389\n",
      "VALIDATION -> Loss: 0.011248, MSE: 0.022503, RMSE: 0.150011, R²: 0.522707\n",
      "Iteration 69/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013476, MSE: 0.026950, RMSE: 0.164165, R²: 0.432309\n",
      "VALIDATION -> Loss: 0.011458, MSE: 0.022923, RMSE: 0.151405, R²: 0.513793\n",
      "Iteration 70/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013472, MSE: 0.026942, RMSE: 0.164141, R²: 0.432471\n",
      "VALIDATION -> Loss: 0.011275, MSE: 0.022559, RMSE: 0.150196, R²: 0.521524\n",
      "Iteration 71/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013476, MSE: 0.026946, RMSE: 0.164152, R²: 0.432398\n",
      "VALIDATION -> Loss: 0.011224, MSE: 0.022455, RMSE: 0.149850, R²: 0.523728\n",
      "Iteration 72/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013452, MSE: 0.026907, RMSE: 0.164034, R²: 0.433213\n",
      "VALIDATION -> Loss: 0.011301, MSE: 0.022611, RMSE: 0.150368, R²: 0.520427\n",
      "Iteration 73/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013416, MSE: 0.026836, RMSE: 0.163818, R²: 0.434706\n",
      "VALIDATION -> Loss: 0.011383, MSE: 0.022775, RMSE: 0.150913, R²: 0.516948\n",
      "Iteration 74/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013434, MSE: 0.026868, RMSE: 0.163914, R²: 0.434043\n",
      "VALIDATION -> Loss: 0.011264, MSE: 0.022535, RMSE: 0.150118, R²: 0.522023\n",
      "Iteration 75/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013394, MSE: 0.026790, RMSE: 0.163676, R²: 0.435683\n",
      "VALIDATION -> Loss: 0.011352, MSE: 0.022713, RMSE: 0.150707, R²: 0.518262\n",
      "Iteration 76/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013422, MSE: 0.026838, RMSE: 0.163823, R²: 0.434668\n",
      "VALIDATION -> Loss: 0.011689, MSE: 0.023388, RMSE: 0.152933, R²: 0.503931\n",
      "Iteration 77/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013406, MSE: 0.026810, RMSE: 0.163738, R²: 0.435257\n",
      "VALIDATION -> Loss: 0.011274, MSE: 0.022556, RMSE: 0.150186, R²: 0.521592\n",
      "Iteration 78/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013364, MSE: 0.026699, RMSE: 0.163400, R²: 0.437587\n",
      "VALIDATION -> Loss: 0.011266, MSE: 0.022542, RMSE: 0.150139, R²: 0.521886\n",
      "Iteration 79/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013361, MSE: 0.026727, RMSE: 0.163483, R²: 0.437011\n",
      "VALIDATION -> Loss: 0.011217, MSE: 0.022441, RMSE: 0.149802, R²: 0.524034\n",
      "Iteration 80/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013362, MSE: 0.026712, RMSE: 0.163438, R²: 0.437322\n",
      "VALIDATION -> Loss: 0.011309, MSE: 0.022627, RMSE: 0.150423, R²: 0.520076\n",
      "Iteration 81/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013282, MSE: 0.026568, RMSE: 0.162996, R²: 0.440363\n",
      "VALIDATION -> Loss: 0.011356, MSE: 0.022723, RMSE: 0.150742, R²: 0.518043\n",
      "Iteration 82/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013354, MSE: 0.026705, RMSE: 0.163415, R²: 0.437480\n",
      "VALIDATION -> Loss: 0.011390, MSE: 0.022790, RMSE: 0.150962, R²: 0.516632\n",
      "Iteration 83/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013349, MSE: 0.026704, RMSE: 0.163412, R²: 0.437500\n",
      "VALIDATION -> Loss: 0.011361, MSE: 0.022729, RMSE: 0.150762, R²: 0.517912\n",
      "Iteration 84/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013424, MSE: 0.026839, RMSE: 0.163826, R²: 0.434650\n",
      "VALIDATION -> Loss: 0.011418, MSE: 0.022842, RMSE: 0.151136, R²: 0.515517\n",
      "Iteration 85/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013329, MSE: 0.026660, RMSE: 0.163279, R²: 0.438418\n",
      "VALIDATION -> Loss: 0.011251, MSE: 0.022508, RMSE: 0.150027, R²: 0.522600\n",
      "Iteration 86/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013243, MSE: 0.026505, RMSE: 0.162803, R²: 0.441690\n",
      "VALIDATION -> Loss: 0.011622, MSE: 0.023255, RMSE: 0.152496, R²: 0.506762\n",
      "Iteration 87/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013331, MSE: 0.026676, RMSE: 0.163327, R²: 0.438088\n",
      "VALIDATION -> Loss: 0.011328, MSE: 0.022664, RMSE: 0.150546, R²: 0.519292\n",
      "Iteration 88/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013295, MSE: 0.026604, RMSE: 0.163106, R²: 0.439608\n",
      "VALIDATION -> Loss: 0.011325, MSE: 0.022657, RMSE: 0.150524, R²: 0.519435\n",
      "Iteration 89/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013309, MSE: 0.026621, RMSE: 0.163160, R²: 0.439239\n",
      "VALIDATION -> Loss: 0.011326, MSE: 0.022657, RMSE: 0.150524, R²: 0.519435\n",
      "Iteration 90/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013257, MSE: 0.026517, RMSE: 0.162841, R²: 0.441430\n",
      "VALIDATION -> Loss: 0.011358, MSE: 0.022724, RMSE: 0.150745, R²: 0.518019\n",
      "Iteration 91/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013226, MSE: 0.026470, RMSE: 0.162695, R²: 0.442428\n",
      "VALIDATION -> Loss: 0.011279, MSE: 0.022566, RMSE: 0.150221, R²: 0.521368\n",
      "Iteration 92/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013267, MSE: 0.026506, RMSE: 0.162807, R²: 0.441662\n",
      "VALIDATION -> Loss: 0.011252, MSE: 0.022510, RMSE: 0.150033, R²: 0.522562\n",
      "Iteration 93/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013232, MSE: 0.026485, RMSE: 0.162743, R²: 0.442097\n",
      "VALIDATION -> Loss: 0.011343, MSE: 0.022693, RMSE: 0.150641, R²: 0.518688\n",
      "Iteration 94/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013188, MSE: 0.026352, RMSE: 0.162333, R²: 0.444903\n",
      "VALIDATION -> Loss: 0.011403, MSE: 0.022814, RMSE: 0.151042, R²: 0.516118\n",
      "Iteration 95/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013157, MSE: 0.026344, RMSE: 0.162308, R²: 0.445077\n",
      "VALIDATION -> Loss: 0.011442, MSE: 0.022891, RMSE: 0.151298, R²: 0.514482\n",
      "Iteration 96/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013250, MSE: 0.026488, RMSE: 0.162751, R²: 0.442042\n",
      "VALIDATION -> Loss: 0.011411, MSE: 0.022829, RMSE: 0.151093, R²: 0.515791\n",
      "Iteration 97/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013146, MSE: 0.026290, RMSE: 0.162141, R²: 0.446217\n",
      "VALIDATION -> Loss: 0.011417, MSE: 0.022841, RMSE: 0.151134, R²: 0.515533\n",
      "Iteration 98/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013166, MSE: 0.026338, RMSE: 0.162289, R²: 0.445209\n",
      "VALIDATION -> Loss: 0.011444, MSE: 0.022897, RMSE: 0.151316, R²: 0.514363\n",
      "Iteration 99/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013208, MSE: 0.026403, RMSE: 0.162490, R²: 0.443834\n",
      "VALIDATION -> Loss: 0.011344, MSE: 0.022697, RMSE: 0.150654, R²: 0.518603\n",
      "Iteration 100/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013185, MSE: 0.026371, RMSE: 0.162390, R²: 0.444517\n",
      "VALIDATION -> Loss: 0.011273, MSE: 0.022553, RMSE: 0.150176, R²: 0.521653\n"
     ]
    }
   ],
   "source": [
    "#The model is trained and validated for 50 epochs\n",
    "for i in range(100): \n",
    "    print(f\"Iteration {i+1}/100 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4699\n",
      "MAE: 0.1182\n",
      "MSE: 0.0252\n",
      "RMSE: 0.1586\n",
      "Precisión: 0.6185\n",
      "Recall: 0.9331\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#And display the evaluation metrics of the model\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THIRD MODEL**\n",
    "# Adding to the first model the data from IMDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(\"ml-latest-small/movie_info_imdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "imdb['imdbId'] = imdb['imdbId'].str.replace('tt', '', regex=False).astype(int)\n",
    "columns_to_drop = ['Actors', 'Awards', 'DVD', 'Director', 'Genre', 'Title', 'Type', 'Website', 'Year', 'Poster', 'Production', 'Rated', 'Plot', 'Writer', 'Response', 'Ratings']\n",
    "imdb = imdb.drop(columns=columns_to_drop)\n",
    "\n",
    "imdb = imdb.astype({\n",
    "    \"BoxOffice\": \"string\",  \n",
    "    \"Country\": \"string\",\n",
    "    \"Language\": \"string\",\n",
    "    \"Metascore\": \"float\",\n",
    "    \"Released\": \"string\",\n",
    "    \"Runtime\": \"string\",\n",
    "    \"imdbRating\": \"float\",\n",
    "    \"imdbVotes\": \"string\"\n",
    "})\n",
    "\n",
    "imdb['BoxOffice'] = imdb['BoxOffice'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "imdb['Released'] = imdb['Released'].str.extract(r'(\\d{4})').astype(float)\n",
    "imdb['Runtime'] = imdb['Runtime'].str.extract(r'(\\d+)').astype(float)\n",
    "imdb['imdbVotes'] = imdb['imdbVotes'].str.replace(',', '', regex=True).astype(float)\n",
    "\n",
    "imdb = imdb.merge(links, on=\"imdbId\", how=\"left\")\n",
    "imdb = imdb.merge(ratings, on=\"movieId\", how=\"left\")\n",
    "#The data from IMDB, links.csv and ratings.csv are merged\n",
    "\n",
    "imdb = imdb.dropna()\n",
    "#'imdbId', 'BoxOffice', 'Language', 'Metascore', 'Released', 'Runtime', 'imdbRating', 'imdbVotes' are the selected features from the IMDB data ( I have considered the others not so usefull)\n",
    "#and are filtered to transform all the data into numeric\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Language', 'Metascore', 'Released', 'Runtime',\n",
      "       'imdbRating', 'imdbVotes', 'movieId', 'tmdbId',\n",
      "       ...\n",
      "       'Country_United States, United Kingdom, Switzerland, Panama',\n",
      "       'Country_United States, Venezuela',\n",
      "       'Country_United States, Vietnam, United Kingdom, Canada, Denmark',\n",
      "       'Country_United States, West Germany', 'Country_West Germany',\n",
      "       'Country_West Germany, France',\n",
      "       'Country_West Germany, France, United Kingdom',\n",
      "       'Country_West Germany, Italy, France',\n",
      "       'Country_West Germany, United States',\n",
      "       'Country_Yugoslavia, United States'],\n",
      "      dtype='object', length=931)\n"
     ]
    }
   ],
   "source": [
    "df2 = imdb\n",
    "df2 = pd.get_dummies(df2, columns=[\"Country\"], dtype=float)\n",
    "\n",
    "print(df2.columns)\n",
    "df2[\"Language\"] = df2[\"Language\"].str.split(\",\")\n",
    "moviesExploded = df2.explode(\"Language\")\n",
    "moviesExploded[\"Language\"] = moviesExploded[\"Language\"].str.strip()\n",
    "movies_dummies = pd.get_dummies(moviesExploded[\"Language\"], dtype=int)\n",
    "movies_dummies = moviesExploded[[\"movieId\"]].join(movies_dummies).groupby(\"movieId\").max()\n",
    "df2 = df2.drop(columns=[\"Language\"]).merge(movies_dummies, on=\"movieId\")\n",
    "#Country and Language are dummyfied\n",
    "\n",
    "columns_to_convert = [col for col in df2.columns]\n",
    "df2[columns_to_convert] = df2[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "#All columns are transformed to numeric\n",
    "\n",
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1087) (54776, 1)\n",
      "(18259, 1087) (18259, 1)\n",
      "(18259, 1087) (18259, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\"])\n",
    "df_shuffle = df_shuffle.dropna()\n",
    "#Feature timestamp is removed and NAs are removed\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.6), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.6):int(len(df_shuffle) * 0.8), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.8):, :]\n",
    "#Data is splited into training, validation and test sets(60-20-20)\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "#Features transformed into numerical and  divided into input and output/target variable\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbId                False\n",
      "BoxOffice             False\n",
      "Metascore             False\n",
      "Released              False\n",
      "Runtime               False\n",
      "                      ...  \n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "Length: 1093, dtype: bool\n",
      "   imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  imdbVotes  \\\n",
      "0  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "1  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "2  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "3  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "4  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "\n",
      "   movieId  tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  Washoe  Welsh  \\\n",
      "0      1.0   862.0     1.0  ...         0     0           0       0      0   \n",
      "1      1.0   862.0     5.0  ...         0     0           0       0      0   \n",
      "2      1.0   862.0     7.0  ...         0     0           0       0      0   \n",
      "3      1.0   862.0    15.0  ...         0     0           0       0      0   \n",
      "4      1.0   862.0    17.0  ...         0     0           0       0      0   \n",
      "\n",
      "   Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0      0      0        0       0     0  \n",
      "1      0      0        0       0     0  \n",
      "2      0      0        0       0     0  \n",
      "3      0      0        0       0     0  \n",
      "4      0      0        0       0     0  \n",
      "\n",
      "[5 rows x 1088 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Here, I have created for each set (train, validation and testing) new 6 variables. For training and validation, the data related to the average of the \n",
    "#rating is taken from the set itself, but the columns of the testing set can not be created from the average of the rating of the training set (would be considered data leakage)\n",
    "#So the solution is using the average from the data of the training set\n",
    "#NAs are also removed\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_train).any())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1093)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_col = df_train.select_dtypes(include=['number']).columns\n",
    "#Only numerical columns are normalized\n",
    "\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.transform(df_val[numerical_col])  \n",
    "df_test[numerical_col] = scaler.transform(df_test[numerical_col]) \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
    "#Tensors are transformed into TensorDatasets and then into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1092, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "#The neural network is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy())  \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.084616, MSE: 0.170415, RMSE: 0.412814, R²: -2.624800\n",
      "VALIDATION -> Loss: 0.019125, MSE: 0.038267, RMSE: 0.195618, R²: 0.188671\n",
      "Iteration 2/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.036733, MSE: 0.073477, RMSE: 0.271066, R²: -0.562882\n",
      "VALIDATION -> Loss: 0.016121, MSE: 0.032258, RMSE: 0.179604, R²: 0.316075\n",
      "Iteration 3/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.033069, MSE: 0.066172, RMSE: 0.257240, R²: -0.407513\n",
      "VALIDATION -> Loss: 0.015273, MSE: 0.030563, RMSE: 0.174821, R²: 0.352012\n",
      "Iteration 4/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.031577, MSE: 0.063169, RMSE: 0.251334, R²: -0.343627\n",
      "VALIDATION -> Loss: 0.014726, MSE: 0.029469, RMSE: 0.171666, R²: 0.375195\n",
      "Iteration 5/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.030384, MSE: 0.060795, RMSE: 0.246567, R²: -0.293138\n",
      "VALIDATION -> Loss: 0.014195, MSE: 0.028405, RMSE: 0.168538, R²: 0.397757\n",
      "Iteration 6/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.029275, MSE: 0.058590, RMSE: 0.242055, R²: -0.246240\n",
      "VALIDATION -> Loss: 0.014765, MSE: 0.029546, RMSE: 0.171890, R²: 0.373562\n",
      "Iteration 7/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.028257, MSE: 0.056459, RMSE: 0.237610, R²: -0.200894\n",
      "VALIDATION -> Loss: 0.014295, MSE: 0.028602, RMSE: 0.169122, R²: 0.393572\n",
      "Iteration 8/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.027596, MSE: 0.055256, RMSE: 0.235066, R²: -0.175312\n",
      "VALIDATION -> Loss: 0.013263, MSE: 0.026537, RMSE: 0.162902, R²: 0.437359\n",
      "Iteration 9/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.026809, MSE: 0.053647, RMSE: 0.231619, R²: -0.141102\n",
      "VALIDATION -> Loss: 0.014396, MSE: 0.028806, RMSE: 0.169723, R²: 0.389259\n",
      "Iteration 10/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.026256, MSE: 0.052572, RMSE: 0.229285, R²: -0.118217\n",
      "VALIDATION -> Loss: 0.013367, MSE: 0.026748, RMSE: 0.163549, R²: 0.432879\n",
      "Iteration 11/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.025260, MSE: 0.050558, RMSE: 0.224851, R²: -0.075389\n",
      "VALIDATION -> Loss: 0.013511, MSE: 0.027035, RMSE: 0.164423, R²: 0.426807\n",
      "Iteration 12/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.024236, MSE: 0.048460, RMSE: 0.220136, R²: -0.030758\n",
      "VALIDATION -> Loss: 0.014634, MSE: 0.029283, RMSE: 0.171122, R²: 0.379143\n",
      "Iteration 13/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.023590, MSE: 0.047194, RMSE: 0.217241, R²: -0.003830\n",
      "VALIDATION -> Loss: 0.014678, MSE: 0.029368, RMSE: 0.171371, R²: 0.377338\n",
      "Iteration 14/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.022916, MSE: 0.045844, RMSE: 0.214111, R²: 0.024889\n",
      "VALIDATION -> Loss: 0.013308, MSE: 0.026628, RMSE: 0.163182, R²: 0.435425\n",
      "Iteration 15/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.022167, MSE: 0.044378, RMSE: 0.210661, R²: 0.056058\n",
      "VALIDATION -> Loss: 0.012946, MSE: 0.025899, RMSE: 0.160933, R²: 0.450882\n",
      "Iteration 16/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.021784, MSE: 0.043581, RMSE: 0.208760, R²: 0.073019\n",
      "VALIDATION -> Loss: 0.013480, MSE: 0.026969, RMSE: 0.164222, R²: 0.428204\n",
      "Iteration 17/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.021205, MSE: 0.042373, RMSE: 0.205847, R²: 0.098707\n",
      "VALIDATION -> Loss: 0.012982, MSE: 0.025970, RMSE: 0.161153, R²: 0.449377\n",
      "Iteration 18/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020631, MSE: 0.041275, RMSE: 0.203163, R²: 0.122057\n",
      "VALIDATION -> Loss: 0.013292, MSE: 0.026598, RMSE: 0.163090, R²: 0.436060\n",
      "Iteration 19/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020001, MSE: 0.040015, RMSE: 0.200038, R²: 0.148865\n",
      "VALIDATION -> Loss: 0.013665, MSE: 0.027340, RMSE: 0.165347, R²: 0.420342\n",
      "Iteration 20/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019508, MSE: 0.039042, RMSE: 0.197591, R²: 0.169559\n",
      "VALIDATION -> Loss: 0.013155, MSE: 0.026319, RMSE: 0.162230, R²: 0.441992\n",
      "Iteration 21/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019232, MSE: 0.038453, RMSE: 0.196094, R²: 0.182096\n",
      "VALIDATION -> Loss: 0.013799, MSE: 0.027606, RMSE: 0.166151, R²: 0.414693\n",
      "Iteration 22/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018659, MSE: 0.037334, RMSE: 0.193219, R²: 0.205899\n",
      "VALIDATION -> Loss: 0.013134, MSE: 0.026276, RMSE: 0.162097, R²: 0.442904\n",
      "Iteration 23/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018282, MSE: 0.036580, RMSE: 0.191260, R²: 0.221924\n",
      "VALIDATION -> Loss: 0.012789, MSE: 0.025588, RMSE: 0.159963, R²: 0.457480\n",
      "Iteration 24/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017879, MSE: 0.035794, RMSE: 0.189193, R²: 0.238650\n",
      "VALIDATION -> Loss: 0.012887, MSE: 0.025777, RMSE: 0.160553, R²: 0.453472\n",
      "Iteration 25/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017714, MSE: 0.035448, RMSE: 0.188276, R²: 0.246013\n",
      "VALIDATION -> Loss: 0.012792, MSE: 0.025587, RMSE: 0.159961, R²: 0.457493\n",
      "Iteration 26/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017324, MSE: 0.034654, RMSE: 0.186156, R²: 0.262894\n",
      "VALIDATION -> Loss: 0.013040, MSE: 0.026085, RMSE: 0.161508, R²: 0.446950\n",
      "Iteration 27/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016979, MSE: 0.033980, RMSE: 0.184336, R²: 0.277239\n",
      "VALIDATION -> Loss: 0.014061, MSE: 0.028131, RMSE: 0.167724, R²: 0.403559\n",
      "Iteration 28/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016800, MSE: 0.033596, RMSE: 0.183293, R²: 0.285391\n",
      "VALIDATION -> Loss: 0.012872, MSE: 0.025749, RMSE: 0.160464, R²: 0.454078\n",
      "Iteration 29/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016391, MSE: 0.032794, RMSE: 0.181090, R²: 0.302466\n",
      "VALIDATION -> Loss: 0.013046, MSE: 0.026096, RMSE: 0.161542, R²: 0.446718\n",
      "Iteration 30/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016131, MSE: 0.032269, RMSE: 0.179637, R²: 0.313617\n",
      "VALIDATION -> Loss: 0.012974, MSE: 0.025954, RMSE: 0.161104, R²: 0.449714\n",
      "Iteration 31/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015850, MSE: 0.031710, RMSE: 0.178073, R²: 0.325520\n",
      "VALIDATION -> Loss: 0.012704, MSE: 0.025412, RMSE: 0.159411, R²: 0.461217\n",
      "Iteration 32/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015680, MSE: 0.031336, RMSE: 0.177019, R²: 0.333479\n",
      "VALIDATION -> Loss: 0.012815, MSE: 0.025633, RMSE: 0.160102, R²: 0.456537\n",
      "Iteration 33/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015456, MSE: 0.030930, RMSE: 0.175869, R²: 0.342108\n",
      "VALIDATION -> Loss: 0.013853, MSE: 0.027712, RMSE: 0.166469, R²: 0.412453\n",
      "Iteration 34/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015209, MSE: 0.030427, RMSE: 0.174432, R²: 0.352814\n",
      "VALIDATION -> Loss: 0.012910, MSE: 0.025826, RMSE: 0.160704, R²: 0.452437\n",
      "Iteration 35/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015055, MSE: 0.030122, RMSE: 0.173557, R²: 0.359290\n",
      "VALIDATION -> Loss: 0.012893, MSE: 0.025786, RMSE: 0.160581, R²: 0.453281\n",
      "Iteration 36/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014952, MSE: 0.029873, RMSE: 0.172839, R²: 0.364582\n",
      "VALIDATION -> Loss: 0.013414, MSE: 0.026833, RMSE: 0.163809, R²: 0.431076\n",
      "Iteration 37/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014781, MSE: 0.029528, RMSE: 0.171838, R²: 0.371919\n",
      "VALIDATION -> Loss: 0.012807, MSE: 0.025617, RMSE: 0.160053, R²: 0.456867\n",
      "Iteration 38/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014602, MSE: 0.029186, RMSE: 0.170838, R²: 0.379208\n",
      "VALIDATION -> Loss: 0.013001, MSE: 0.026009, RMSE: 0.161272, R²: 0.448566\n",
      "Iteration 39/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014500, MSE: 0.028999, RMSE: 0.170290, R²: 0.383188\n",
      "VALIDATION -> Loss: 0.012733, MSE: 0.025471, RMSE: 0.159596, R²: 0.459968\n",
      "Iteration 40/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014353, MSE: 0.028707, RMSE: 0.169432, R²: 0.389390\n",
      "VALIDATION -> Loss: 0.012958, MSE: 0.025919, RMSE: 0.160995, R²: 0.450456\n",
      "Iteration 41/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014290, MSE: 0.028588, RMSE: 0.169080, R²: 0.391920\n",
      "VALIDATION -> Loss: 0.012854, MSE: 0.025712, RMSE: 0.160350, R²: 0.454849\n",
      "Iteration 42/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014220, MSE: 0.028415, RMSE: 0.168567, R²: 0.395608\n",
      "VALIDATION -> Loss: 0.012703, MSE: 0.025411, RMSE: 0.159408, R²: 0.461237\n",
      "Iteration 43/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014083, MSE: 0.028164, RMSE: 0.167821, R²: 0.400943\n",
      "VALIDATION -> Loss: 0.013710, MSE: 0.027422, RMSE: 0.165596, R²: 0.418598\n",
      "Iteration 44/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013957, MSE: 0.027945, RMSE: 0.167168, R²: 0.405598\n",
      "VALIDATION -> Loss: 0.012703, MSE: 0.025410, RMSE: 0.159404, R²: 0.461266\n",
      "Iteration 45/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013864, MSE: 0.027752, RMSE: 0.166589, R²: 0.409709\n",
      "VALIDATION -> Loss: 0.012797, MSE: 0.025600, RMSE: 0.160000, R²: 0.457229\n",
      "Iteration 46/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013815, MSE: 0.027649, RMSE: 0.166279, R²: 0.411899\n",
      "VALIDATION -> Loss: 0.012777, MSE: 0.025554, RMSE: 0.159855, R²: 0.458214\n",
      "Iteration 47/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013675, MSE: 0.027378, RMSE: 0.165462, R²: 0.417670\n",
      "VALIDATION -> Loss: 0.012973, MSE: 0.025950, RMSE: 0.161089, R²: 0.449813\n",
      "Iteration 48/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013569, MSE: 0.027141, RMSE: 0.164747, R²: 0.422691\n",
      "VALIDATION -> Loss: 0.012773, MSE: 0.025553, RMSE: 0.159854, R²: 0.458217\n",
      "Iteration 49/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013593, MSE: 0.027193, RMSE: 0.164903, R²: 0.421595\n",
      "VALIDATION -> Loss: 0.013536, MSE: 0.027075, RMSE: 0.164545, R²: 0.425954\n",
      "Iteration 50/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013490, MSE: 0.026993, RMSE: 0.164296, R²: 0.425846\n",
      "VALIDATION -> Loss: 0.012910, MSE: 0.025825, RMSE: 0.160701, R²: 0.452459\n"
     ]
    }
   ],
   "source": [
    "#The model is trained and validated for 50 epochs\n",
    "for i in range(50): \n",
    "    print(f\"Iteration {i+1}/50 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4805\n",
      "MAE: 0.1186\n",
      "MSE: 0.0244\n",
      "RMSE: 0.1563\n",
      "Precisión: 0.6205\n",
      "Recall: 0.9401\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#And display the evaluation metrics of the model\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FOURTH MODEL**\n",
    "# Adding to the first model the data from IMDB and TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Country', 'Language', 'Metascore', 'Released',\n",
      "       'Runtime', 'imdbRating', 'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       'rating', 'timestamp'],\n",
      "      dtype='object')\n",
      "<bound method NDFrame.describe of          imdbId    BoxOffice                               Country  \\\n",
      "0        114709  223225679.0                         United States   \n",
      "1        114709  223225679.0                         United States   \n",
      "2        114709  223225679.0                         United States   \n",
      "3        114709  223225679.0                         United States   \n",
      "4        114709  223225679.0                         United States   \n",
      "...         ...          ...                                   ...   \n",
      "100799  4912910  220159104.0  United States, China, France, Norway   \n",
      "100800  4912910  220159104.0  United States, China, France, Norway   \n",
      "100801  7690670   20545116.0                         United States   \n",
      "100803  7349662   49275340.0                  United States, China   \n",
      "100804  4073790   12695691.0                         United States   \n",
      "\n",
      "                           Language  Metascore  Released  Runtime  imdbRating  \\\n",
      "0                           English       96.0    1995.0     81.0         8.3   \n",
      "1                           English       96.0    1995.0     81.0         8.3   \n",
      "2                           English       96.0    1995.0     81.0         8.3   \n",
      "3                           English       96.0    1995.0     81.0         8.3   \n",
      "4                           English       96.0    1995.0     81.0         8.3   \n",
      "...                             ...        ...       ...      ...         ...   \n",
      "100799              English, French       87.0    2018.0    147.0         7.7   \n",
      "100800              English, French       87.0    2018.0    147.0         7.7   \n",
      "100801  English, Spanish, Cantonese       52.0    2018.0    116.0         5.3   \n",
      "100803                      English       83.0    2018.0    135.0         7.5   \n",
      "100804                      English       39.0    2018.0    104.0         5.7   \n",
      "\n",
      "        imdbVotes   movieId    tmdbId  userId  rating     timestamp  \n",
      "0       1112586.0       1.0     862.0     1.0     4.0  9.649827e+08  \n",
      "1       1112586.0       1.0     862.0     5.0     4.0  8.474350e+08  \n",
      "2       1112586.0       1.0     862.0     7.0     4.0  1.106636e+09  \n",
      "3       1112586.0       1.0     862.0    15.0     2.0  1.510578e+09  \n",
      "4       1112586.0       1.0     862.0    17.0     4.0  1.305696e+09  \n",
      "...           ...       ...       ...     ...     ...           ...  \n",
      "100799   391467.0  189333.0  353081.0   184.0     4.0  1.537110e+09  \n",
      "100800   391467.0  189333.0  353081.0   248.0     3.0  1.534602e+09  \n",
      "100801     7799.0  189381.0  500475.0   318.0     2.0  1.536098e+09  \n",
      "100803   300208.0  189713.0  487558.0   462.0     2.0  1.536467e+09  \n",
      "100804    38486.0  190183.0  445651.0    50.0     3.0  1.533302e+09  \n",
      "\n",
      "[91294 rows x 14 columns]>\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv(\"ml-latest-small/movie_info_imdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "imdb['imdbId'] = imdb['imdbId'].str.replace('tt', '', regex=False).astype(int)\n",
    "columns_to_drop = ['Actors', 'Awards', 'DVD', 'Director', 'Genre', 'Title', 'Type', 'Website', 'Year', 'Poster', 'Production', 'Rated', 'Plot', 'Writer', 'Response', 'Ratings']\n",
    "imdb = imdb.drop(columns=columns_to_drop)\n",
    "\n",
    "imdb = imdb.astype({\n",
    "    \"BoxOffice\": \"string\",  \n",
    "    \"Country\": \"string\",\n",
    "    \"Language\": \"string\",\n",
    "    \"Metascore\": \"float\",\n",
    "    \"Released\": \"string\",\n",
    "    \"Runtime\": \"string\",\n",
    "    \"imdbRating\": \"float\",\n",
    "    \"imdbVotes\": \"string\"\n",
    "})\n",
    "\n",
    "imdb['BoxOffice'] = imdb['BoxOffice'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "imdb['Released'] = imdb['Released'].str.extract(r'(\\d{4})').astype(float)\n",
    "imdb['Runtime'] = imdb['Runtime'].str.extract(r'(\\d+)').astype(float)\n",
    "imdb['imdbVotes'] = imdb['imdbVotes'].str.replace(',', '', regex=True).astype(float)\n",
    "\n",
    "\n",
    "tmdb = pd.read_csv(\"ml-latest-small/movie_info_tmdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "tmdb = tmdb.drop(columns=[\"title\", \"original_language\"])\n",
    "tmdb[\"release_date\"] = pd.to_datetime(tmdb[\"release_date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "imdb = imdb.merge(links, on=\"imdbId\", how=\"left\")\n",
    "imdb = imdb.merge(ratings, on=\"movieId\", how=\"left\")\n",
    "df2 = imdb.merge(tmdb, on=\"tmdbId\", how=\"left\")\n",
    "\n",
    "imdb = imdb.dropna()\n",
    "print(imdb.columns)\n",
    "\n",
    "print(imdb.describe)\n",
    "#All the steps from previous models are performed and data from TMDB, IMDB, links and ratings are merged all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating',\n",
      "       'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       ...\n",
      "       'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa',\n",
      "       'Yiddish', 'Yoruba', 'Zulu'],\n",
      "      dtype='object', length=1088)\n",
      "<bound method NDFrame.head of         imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  \\\n",
      "0       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "1       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "2       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "3       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "4       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "...        ...          ...        ...       ...      ...         ...   \n",
      "91289  4912910  220159104.0       87.0    2018.0    147.0         7.7   \n",
      "91290  4912910  220159104.0       87.0    2018.0    147.0         7.7   \n",
      "91291  7690670   20545116.0       52.0    2018.0    116.0         5.3   \n",
      "91292  7349662   49275340.0       83.0    2018.0    135.0         7.5   \n",
      "91293  4073790   12695691.0       39.0    2018.0    104.0         5.7   \n",
      "\n",
      "       imdbVotes   movieId    tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  \\\n",
      "0      1112586.0       1.0     862.0     1.0  ...         0     0           0   \n",
      "1      1112586.0       1.0     862.0     5.0  ...         0     0           0   \n",
      "2      1112586.0       1.0     862.0     7.0  ...         0     0           0   \n",
      "3      1112586.0       1.0     862.0    15.0  ...         0     0           0   \n",
      "4      1112586.0       1.0     862.0    17.0  ...         0     0           0   \n",
      "...          ...       ...       ...     ...  ...       ...   ...         ...   \n",
      "91289   391467.0  189333.0  353081.0   184.0  ...         0     0           0   \n",
      "91290   391467.0  189333.0  353081.0   248.0  ...         0     0           0   \n",
      "91291     7799.0  189381.0  500475.0   318.0  ...         0     0           0   \n",
      "91292   300208.0  189713.0  487558.0   462.0  ...         0     0           0   \n",
      "91293    38486.0  190183.0  445651.0    50.0  ...         0     0           0   \n",
      "\n",
      "       Washoe  Welsh  Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0           0      0      0      0        0       0     0  \n",
      "1           0      0      0      0        0       0     0  \n",
      "2           0      0      0      0        0       0     0  \n",
      "3           0      0      0      0        0       0     0  \n",
      "4           0      0      0      0        0       0     0  \n",
      "...       ...    ...    ...    ...      ...     ...   ...  \n",
      "91289       0      0      0      0        0       0     0  \n",
      "91290       0      0      0      0        0       0     0  \n",
      "91291       0      0      0      0        0       0     0  \n",
      "91292       0      0      0      0        0       0     0  \n",
      "91293       0      0      0      0        0       0     0  \n",
      "\n",
      "[91294 rows x 1088 columns]>\n"
     ]
    }
   ],
   "source": [
    "df2 = imdb\n",
    "\n",
    "df2 = pd.get_dummies(df2, columns=[\"Country\"], dtype=float)\n",
    "\n",
    "df2[\"Language\"] = df2[\"Language\"].str.split(\",\")\n",
    "moviesExploded = df2.explode(\"Language\")\n",
    "moviesExploded[\"Language\"] = moviesExploded[\"Language\"].str.strip()\n",
    "movies_dummies = pd.get_dummies(moviesExploded[\"Language\"], dtype=int)\n",
    "movies_dummies = moviesExploded[[\"movieId\"]].join(movies_dummies).groupby(\"movieId\").max()\n",
    "df2 = df2.drop(columns=[\"Language\"]).merge(movies_dummies, on=\"movieId\")\n",
    "\n",
    "columns_to_convert = [col for col in df2.columns]\n",
    "df2[columns_to_convert] = df2[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df2 = df2.dropna()\n",
    "\n",
    "print(df2.columns)\n",
    "print(df2.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1087) (54776, 1)\n",
      "(18259, 1087) (18259, 1)\n",
      "(18259, 1087) (18259, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\"])\n",
    "df_shuffle = df_shuffle.dropna()\n",
    "#Feature timestamp is removed and NAs are removed\n",
    "\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.6), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.6):int(len(df_shuffle) * 0.8), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.8):, :]\n",
    "#Data is splited into training, validation and test sets(60-20-20)\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "#Features transformed into numerical and  divided into input and output/target variable\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating',\n",
      "       'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       ...\n",
      "       'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa',\n",
      "       'Yiddish', 'Yoruba', 'Zulu'],\n",
      "      dtype='object', length=1088)\n",
      "imdbId                False\n",
      "BoxOffice             False\n",
      "Metascore             False\n",
      "Released              False\n",
      "Runtime               False\n",
      "                      ...  \n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "Length: 1093, dtype: bool\n",
      "   imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  imdbVotes  \\\n",
      "0  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "1  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "2  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "3  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "4  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "\n",
      "   movieId  tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  Washoe  Welsh  \\\n",
      "0      1.0   862.0     1.0  ...         0     0           0       0      0   \n",
      "1      1.0   862.0     5.0  ...         0     0           0       0      0   \n",
      "2      1.0   862.0     7.0  ...         0     0           0       0      0   \n",
      "3      1.0   862.0    15.0  ...         0     0           0       0      0   \n",
      "4      1.0   862.0    17.0  ...         0     0           0       0      0   \n",
      "\n",
      "   Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0      0      0        0       0     0  \n",
      "1      0      0        0       0     0  \n",
      "2      0      0        0       0     0  \n",
      "3      0      0        0       0     0  \n",
      "4      0      0        0       0     0  \n",
      "\n",
      "[5 rows x 1088 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Here, I have created for each set (train, validation and testing) new 6 variables. For training and validation, the data related to the average of the \n",
    "#rating is taken from the set itself, but the columns of the testing set can not be created from the average of the rating of the training set (would be considered data leakage)\n",
    "#So the solution is using the average from the data of the training set\n",
    "#NAs are also removed\n",
    "print(df2.columns)\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_train).any())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1093)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_col = df_train.select_dtypes(include=['number']).columns\n",
    "#Only numerical columns are normalized\n",
    "\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.transform(df_val[numerical_col])  \n",
    "df_test[numerical_col] = scaler.transform(df_test[numerical_col]) \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
    "#Tensors are transformed into TensorDatasets and then into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1092, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "#The neural network is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy())  \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.043108, MSE: 0.086530, RMSE: 0.294160, R²: -0.840525\n",
      "VALIDATION -> Loss: 0.018207, MSE: 0.036432, RMSE: 0.190873, R²: 0.227559\n",
      "Iteration 2/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.023532, MSE: 0.047078, RMSE: 0.216974, R²: -0.001357\n",
      "VALIDATION -> Loss: 0.015604, MSE: 0.031225, RMSE: 0.176707, R²: 0.337962\n",
      "Iteration 3/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020512, MSE: 0.041026, RMSE: 0.202549, R²: 0.127362\n",
      "VALIDATION -> Loss: 0.014633, MSE: 0.029281, RMSE: 0.171118, R²: 0.379179\n",
      "Iteration 4/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019468, MSE: 0.038945, RMSE: 0.197344, R²: 0.171634\n",
      "VALIDATION -> Loss: 0.014468, MSE: 0.028953, RMSE: 0.170155, R²: 0.386144\n",
      "Iteration 5/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018755, MSE: 0.037507, RMSE: 0.193668, R²: 0.202207\n",
      "VALIDATION -> Loss: 0.013411, MSE: 0.026832, RMSE: 0.163805, R²: 0.431103\n",
      "Iteration 6/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018397, MSE: 0.036803, RMSE: 0.191841, R²: 0.217188\n",
      "VALIDATION -> Loss: 0.013415, MSE: 0.026844, RMSE: 0.163843, R²: 0.430844\n",
      "Iteration 7/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018032, MSE: 0.036078, RMSE: 0.189942, R²: 0.232612\n",
      "VALIDATION -> Loss: 0.013459, MSE: 0.026930, RMSE: 0.164103, R²: 0.429033\n",
      "Iteration 8/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017535, MSE: 0.035079, RMSE: 0.187294, R²: 0.253852\n",
      "VALIDATION -> Loss: 0.013135, MSE: 0.026283, RMSE: 0.162121, R²: 0.442741\n",
      "Iteration 9/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017376, MSE: 0.034788, RMSE: 0.186515, R²: 0.260049\n",
      "VALIDATION -> Loss: 0.013748, MSE: 0.027509, RMSE: 0.165859, R²: 0.416751\n",
      "Iteration 10/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016823, MSE: 0.033621, RMSE: 0.183360, R²: 0.284870\n",
      "VALIDATION -> Loss: 0.013168, MSE: 0.026342, RMSE: 0.162302, R²: 0.441498\n",
      "Iteration 11/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016637, MSE: 0.033264, RMSE: 0.182384, R²: 0.292465\n",
      "VALIDATION -> Loss: 0.013018, MSE: 0.026048, RMSE: 0.161393, R²: 0.447737\n",
      "Iteration 12/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016425, MSE: 0.032921, RMSE: 0.181440, R²: 0.299766\n",
      "VALIDATION -> Loss: 0.012995, MSE: 0.025998, RMSE: 0.161240, R²: 0.448780\n",
      "Iteration 13/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016290, MSE: 0.032583, RMSE: 0.180508, R²: 0.306947\n",
      "VALIDATION -> Loss: 0.013031, MSE: 0.026066, RMSE: 0.161450, R²: 0.447343\n",
      "Iteration 14/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016109, MSE: 0.032179, RMSE: 0.179385, R²: 0.315541\n",
      "VALIDATION -> Loss: 0.013557, MSE: 0.027124, RMSE: 0.164695, R²: 0.424907\n",
      "Iteration 15/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015830, MSE: 0.031696, RMSE: 0.178035, R²: 0.325806\n",
      "VALIDATION -> Loss: 0.012864, MSE: 0.025734, RMSE: 0.160419, R²: 0.454381\n",
      "Iteration 16/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015732, MSE: 0.031474, RMSE: 0.177408, R²: 0.330546\n",
      "VALIDATION -> Loss: 0.013010, MSE: 0.026031, RMSE: 0.161341, R²: 0.448093\n",
      "Iteration 17/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015589, MSE: 0.031199, RMSE: 0.176631, R²: 0.336395\n",
      "VALIDATION -> Loss: 0.013155, MSE: 0.026316, RMSE: 0.162222, R²: 0.442049\n",
      "Iteration 18/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015277, MSE: 0.030545, RMSE: 0.174770, R²: 0.350303\n",
      "VALIDATION -> Loss: 0.013375, MSE: 0.026756, RMSE: 0.163573, R²: 0.432714\n",
      "Iteration 19/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015248, MSE: 0.030441, RMSE: 0.174474, R²: 0.352505\n",
      "VALIDATION -> Loss: 0.013104, MSE: 0.026216, RMSE: 0.161913, R²: 0.444168\n",
      "Iteration 20/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015049, MSE: 0.030084, RMSE: 0.173448, R²: 0.360100\n",
      "VALIDATION -> Loss: 0.012734, MSE: 0.025475, RMSE: 0.159608, R²: 0.459886\n",
      "Iteration 21/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014936, MSE: 0.029893, RMSE: 0.172896, R²: 0.364161\n",
      "VALIDATION -> Loss: 0.013144, MSE: 0.026292, RMSE: 0.162148, R²: 0.442556\n",
      "Iteration 22/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014868, MSE: 0.029699, RMSE: 0.172335, R²: 0.368282\n",
      "VALIDATION -> Loss: 0.012911, MSE: 0.025827, RMSE: 0.160708, R²: 0.452416\n",
      "Iteration 23/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014704, MSE: 0.029439, RMSE: 0.171579, R²: 0.373815\n",
      "VALIDATION -> Loss: 0.012912, MSE: 0.025833, RMSE: 0.160725, R²: 0.452296\n",
      "Iteration 24/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014538, MSE: 0.029085, RMSE: 0.170543, R²: 0.381353\n",
      "VALIDATION -> Loss: 0.012681, MSE: 0.025373, RMSE: 0.159287, R²: 0.462051\n",
      "Iteration 25/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014385, MSE: 0.028780, RMSE: 0.169647, R²: 0.387837\n",
      "VALIDATION -> Loss: 0.012831, MSE: 0.025672, RMSE: 0.160224, R²: 0.455708\n",
      "Iteration 26/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014430, MSE: 0.028850, RMSE: 0.169853, R²: 0.386346\n",
      "VALIDATION -> Loss: 0.012773, MSE: 0.025548, RMSE: 0.159837, R²: 0.458331\n",
      "Iteration 27/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014261, MSE: 0.028500, RMSE: 0.168818, R²: 0.393802\n",
      "VALIDATION -> Loss: 0.012749, MSE: 0.025505, RMSE: 0.159703, R²: 0.459242\n",
      "Iteration 28/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014162, MSE: 0.028303, RMSE: 0.168234, R²: 0.397988\n",
      "VALIDATION -> Loss: 0.012703, MSE: 0.025412, RMSE: 0.159410, R²: 0.461222\n",
      "Iteration 29/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014126, MSE: 0.028243, RMSE: 0.168057, R²: 0.399257\n",
      "VALIDATION -> Loss: 0.013001, MSE: 0.026008, RMSE: 0.161271, R²: 0.448572\n",
      "Iteration 30/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014013, MSE: 0.028019, RMSE: 0.167389, R²: 0.404024\n",
      "VALIDATION -> Loss: 0.013079, MSE: 0.026164, RMSE: 0.161752, R²: 0.445276\n",
      "Iteration 31/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014007, MSE: 0.028025, RMSE: 0.167408, R²: 0.403889\n",
      "VALIDATION -> Loss: 0.012799, MSE: 0.025608, RMSE: 0.160025, R²: 0.457055\n",
      "Iteration 32/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013859, MSE: 0.027722, RMSE: 0.166499, R²: 0.410342\n",
      "VALIDATION -> Loss: 0.012690, MSE: 0.025387, RMSE: 0.159333, R²: 0.461746\n",
      "Iteration 33/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013831, MSE: 0.027655, RMSE: 0.166297, R²: 0.411772\n",
      "VALIDATION -> Loss: 0.012664, MSE: 0.025335, RMSE: 0.159171, R²: 0.462840\n",
      "Iteration 34/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013755, MSE: 0.027469, RMSE: 0.165738, R²: 0.415721\n",
      "VALIDATION -> Loss: 0.012767, MSE: 0.025543, RMSE: 0.159823, R²: 0.458429\n",
      "Iteration 35/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013721, MSE: 0.027429, RMSE: 0.165617, R²: 0.416576\n",
      "VALIDATION -> Loss: 0.012674, MSE: 0.025353, RMSE: 0.159227, R²: 0.462460\n",
      "Iteration 36/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013770, MSE: 0.027551, RMSE: 0.165986, R²: 0.413972\n",
      "VALIDATION -> Loss: 0.012699, MSE: 0.025404, RMSE: 0.159387, R²: 0.461377\n",
      "Iteration 37/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013672, MSE: 0.027350, RMSE: 0.165377, R²: 0.418263\n",
      "VALIDATION -> Loss: 0.013029, MSE: 0.026068, RMSE: 0.161457, R²: 0.447296\n",
      "Iteration 38/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013684, MSE: 0.027354, RMSE: 0.165391, R²: 0.418167\n",
      "VALIDATION -> Loss: 0.013081, MSE: 0.026169, RMSE: 0.161770, R²: 0.445155\n",
      "Iteration 39/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013626, MSE: 0.027205, RMSE: 0.164941, R²: 0.421331\n",
      "VALIDATION -> Loss: 0.012959, MSE: 0.025929, RMSE: 0.161023, R²: 0.450262\n",
      "Iteration 40/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013434, MSE: 0.026874, RMSE: 0.163932, R²: 0.428389\n",
      "VALIDATION -> Loss: 0.012806, MSE: 0.025622, RMSE: 0.160070, R²: 0.456752\n",
      "Iteration 41/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013413, MSE: 0.026842, RMSE: 0.163836, R²: 0.429053\n",
      "VALIDATION -> Loss: 0.013118, MSE: 0.026248, RMSE: 0.162011, R²: 0.443496\n",
      "Iteration 42/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013342, MSE: 0.026661, RMSE: 0.163282, R²: 0.432911\n",
      "VALIDATION -> Loss: 0.013106, MSE: 0.026221, RMSE: 0.161928, R²: 0.444067\n",
      "Iteration 43/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013287, MSE: 0.026557, RMSE: 0.162962, R²: 0.435130\n",
      "VALIDATION -> Loss: 0.012856, MSE: 0.025722, RMSE: 0.160382, R²: 0.454632\n",
      "Iteration 44/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013252, MSE: 0.026490, RMSE: 0.162757, R²: 0.436553\n",
      "VALIDATION -> Loss: 0.012769, MSE: 0.025547, RMSE: 0.159834, R²: 0.458354\n",
      "Iteration 45/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013190, MSE: 0.026407, RMSE: 0.162503, R²: 0.438311\n",
      "VALIDATION -> Loss: 0.012724, MSE: 0.025456, RMSE: 0.159551, R²: 0.460273\n",
      "Iteration 46/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013163, MSE: 0.026335, RMSE: 0.162281, R²: 0.439840\n",
      "VALIDATION -> Loss: 0.012751, MSE: 0.025511, RMSE: 0.159721, R²: 0.459120\n",
      "Iteration 47/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013144, MSE: 0.026313, RMSE: 0.162214, R²: 0.440307\n",
      "VALIDATION -> Loss: 0.013213, MSE: 0.026434, RMSE: 0.162584, R²: 0.439555\n",
      "Iteration 48/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013259, MSE: 0.026523, RMSE: 0.162860, R²: 0.435837\n",
      "VALIDATION -> Loss: 0.013008, MSE: 0.026019, RMSE: 0.161303, R²: 0.448354\n",
      "Iteration 49/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013040, MSE: 0.026078, RMSE: 0.161488, R²: 0.445304\n",
      "VALIDATION -> Loss: 0.012906, MSE: 0.025826, RMSE: 0.160704, R²: 0.452438\n",
      "Iteration 50/30 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013037, MSE: 0.026080, RMSE: 0.161494, R²: 0.445260\n",
      "VALIDATION -> Loss: 0.013487, MSE: 0.026985, RMSE: 0.164270, R²: 0.427868\n"
     ]
    }
   ],
   "source": [
    "#The model is trained and validated for 50 epochs\n",
    "for i in range(50): \n",
    "    print(f\"Iteration {i+1}/50 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4629\n",
      "MAE: 0.1223\n",
      "MSE: 0.0253\n",
      "RMSE: 0.1589\n",
      "Precisión: 0.6534\n",
      "Recall: 0.9150\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#And display the evaluation metrics of the model\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FIFTH MODEL**\n",
    "# Like the fourth one but using the 1M ratings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Country', 'Language', 'Metascore', 'Released',\n",
      "       'Runtime', 'imdbRating', 'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       'rating', 'timestamp'],\n",
      "      dtype='object')\n",
      "<bound method NDFrame.describe of          imdbId    BoxOffice                               Country  \\\n",
      "0        114709  223225679.0                         United States   \n",
      "1        114709  223225679.0                         United States   \n",
      "2        114709  223225679.0                         United States   \n",
      "3        114709  223225679.0                         United States   \n",
      "4        114709  223225679.0                         United States   \n",
      "...         ...          ...                                   ...   \n",
      "100799  4912910  220159104.0  United States, China, France, Norway   \n",
      "100800  4912910  220159104.0  United States, China, France, Norway   \n",
      "100801  7690670   20545116.0                         United States   \n",
      "100803  7349662   49275340.0                  United States, China   \n",
      "100804  4073790   12695691.0                         United States   \n",
      "\n",
      "                           Language  Metascore  Released  Runtime  imdbRating  \\\n",
      "0                           English       96.0    1995.0     81.0         8.3   \n",
      "1                           English       96.0    1995.0     81.0         8.3   \n",
      "2                           English       96.0    1995.0     81.0         8.3   \n",
      "3                           English       96.0    1995.0     81.0         8.3   \n",
      "4                           English       96.0    1995.0     81.0         8.3   \n",
      "...                             ...        ...       ...      ...         ...   \n",
      "100799              English, French       87.0    2018.0    147.0         7.7   \n",
      "100800              English, French       87.0    2018.0    147.0         7.7   \n",
      "100801  English, Spanish, Cantonese       52.0    2018.0    116.0         5.3   \n",
      "100803                      English       83.0    2018.0    135.0         7.5   \n",
      "100804                      English       39.0    2018.0    104.0         5.7   \n",
      "\n",
      "        imdbVotes   movieId    tmdbId  userId  rating     timestamp  \n",
      "0       1112586.0       1.0     862.0     1.0     4.0  9.649827e+08  \n",
      "1       1112586.0       1.0     862.0     5.0     4.0  8.474350e+08  \n",
      "2       1112586.0       1.0     862.0     7.0     4.0  1.106636e+09  \n",
      "3       1112586.0       1.0     862.0    15.0     2.0  1.510578e+09  \n",
      "4       1112586.0       1.0     862.0    17.0     4.0  1.305696e+09  \n",
      "...           ...       ...       ...     ...     ...           ...  \n",
      "100799   391467.0  189333.0  353081.0   184.0     4.0  1.537110e+09  \n",
      "100800   391467.0  189333.0  353081.0   248.0     3.0  1.534602e+09  \n",
      "100801     7799.0  189381.0  500475.0   318.0     2.0  1.536098e+09  \n",
      "100803   300208.0  189713.0  487558.0   462.0     2.0  1.536467e+09  \n",
      "100804    38486.0  190183.0  445651.0    50.0     3.0  1.533302e+09  \n",
      "\n",
      "[91294 rows x 14 columns]>\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv(\"ml-latest-small/movie_info_imdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "#This is the chang with respect to the previous one. Data is not from the 100k dataset but from the 1M\n",
    "imdb['imdbId'] = imdb['imdbId'].str.replace('tt', '', regex=False).astype(int)\n",
    "columns_to_drop = ['Actors', 'Awards', 'DVD', 'Director', 'Genre', 'Title', 'Type', 'Website', 'Year', 'Poster', 'Production', 'Rated', 'Plot', 'Writer', 'Response', 'Ratings']\n",
    "imdb = imdb.drop(columns=columns_to_drop)\n",
    "\n",
    "imdb = imdb.astype({\n",
    "    \"BoxOffice\": \"string\", \n",
    "    \"Country\": \"string\",\n",
    "    \"Language\": \"string\",\n",
    "    \"Metascore\": \"float\",\n",
    "    \"Released\": \"string\",\n",
    "    \"Runtime\": \"string\",\n",
    "    \"imdbRating\": \"float\",\n",
    "    \"imdbVotes\": \"string\"\n",
    "})\n",
    "\n",
    "imdb['BoxOffice'] = imdb['BoxOffice'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "imdb['Released'] = imdb['Released'].str.extract(r'(\\d{4})').astype(float)\n",
    "imdb['Runtime'] = imdb['Runtime'].str.extract(r'(\\d+)').astype(float)\n",
    "imdb['imdbVotes'] = imdb['imdbVotes'].str.replace(',', '', regex=True).astype(float)\n",
    "\n",
    "tmdb = pd.read_csv(\"ml-latest-small/movie_info_tmdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "tmdb = tmdb.drop(columns=[\"title\", \"original_language\"])\n",
    "tmdb[\"release_date\"] = pd.to_datetime(tmdb[\"release_date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "imdb = imdb.merge(links, on=\"imdbId\", how=\"left\")\n",
    "imdb = imdb.merge(ratings, on=\"movieId\", how=\"left\")\n",
    "df2 = imdb.merge(tmdb, on=\"tmdbId\", how=\"left\")\n",
    "\n",
    "imdb = imdb.dropna()\n",
    "print(imdb.columns)\n",
    "print(imdb.describe)\n",
    "#All the steps from previous models are performed and data from TMDB, IMDB, links and ratings are merged all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating',\n",
      "       'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       ...\n",
      "       'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa',\n",
      "       'Yiddish', 'Yoruba', 'Zulu'],\n",
      "      dtype='object', length=1088)\n",
      "<bound method NDFrame.head of         imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  \\\n",
      "0       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "1       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "2       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "3       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "4       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "...        ...          ...        ...       ...      ...         ...   \n",
      "91289  4912910  220159104.0       87.0    2018.0    147.0         7.7   \n",
      "91290  4912910  220159104.0       87.0    2018.0    147.0         7.7   \n",
      "91291  7690670   20545116.0       52.0    2018.0    116.0         5.3   \n",
      "91292  7349662   49275340.0       83.0    2018.0    135.0         7.5   \n",
      "91293  4073790   12695691.0       39.0    2018.0    104.0         5.7   \n",
      "\n",
      "       imdbVotes   movieId    tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  \\\n",
      "0      1112586.0       1.0     862.0     1.0  ...         0     0           0   \n",
      "1      1112586.0       1.0     862.0     5.0  ...         0     0           0   \n",
      "2      1112586.0       1.0     862.0     7.0  ...         0     0           0   \n",
      "3      1112586.0       1.0     862.0    15.0  ...         0     0           0   \n",
      "4      1112586.0       1.0     862.0    17.0  ...         0     0           0   \n",
      "...          ...       ...       ...     ...  ...       ...   ...         ...   \n",
      "91289   391467.0  189333.0  353081.0   184.0  ...         0     0           0   \n",
      "91290   391467.0  189333.0  353081.0   248.0  ...         0     0           0   \n",
      "91291     7799.0  189381.0  500475.0   318.0  ...         0     0           0   \n",
      "91292   300208.0  189713.0  487558.0   462.0  ...         0     0           0   \n",
      "91293    38486.0  190183.0  445651.0    50.0  ...         0     0           0   \n",
      "\n",
      "       Washoe  Welsh  Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0           0      0      0      0        0       0     0  \n",
      "1           0      0      0      0        0       0     0  \n",
      "2           0      0      0      0        0       0     0  \n",
      "3           0      0      0      0        0       0     0  \n",
      "4           0      0      0      0        0       0     0  \n",
      "...       ...    ...    ...    ...      ...     ...   ...  \n",
      "91289       0      0      0      0        0       0     0  \n",
      "91290       0      0      0      0        0       0     0  \n",
      "91291       0      0      0      0        0       0     0  \n",
      "91292       0      0      0      0        0       0     0  \n",
      "91293       0      0      0      0        0       0     0  \n",
      "\n",
      "[91294 rows x 1088 columns]>\n"
     ]
    }
   ],
   "source": [
    "df2 = imdb\n",
    "df2 = pd.get_dummies(df2, columns=[\"Country\"], dtype=float)\n",
    "\n",
    "df2[\"Language\"] = df2[\"Language\"].str.split(\",\")\n",
    "moviesExploded = df2.explode(\"Language\")\n",
    "moviesExploded[\"Language\"] = moviesExploded[\"Language\"].str.strip()\n",
    "movies_dummies = pd.get_dummies(moviesExploded[\"Language\"], dtype=int)\n",
    "movies_dummies = moviesExploded[[\"movieId\"]].join(movies_dummies).groupby(\"movieId\").max()\n",
    "df2 = df2.drop(columns=[\"Language\"]).merge(movies_dummies, on=\"movieId\")\n",
    "\n",
    "\n",
    "columns_to_convert = [col for col in df2.columns]\n",
    "df2[columns_to_convert] = df2[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df2 = df2.dropna()\n",
    "\n",
    "print(df2.columns)\n",
    "print(df2.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1086) (54776, 1)\n",
      "(18259, 1086) (18259, 1)\n",
      "(18259, 1086) (18259, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\"])\n",
    "df_shuffle = df_shuffle.dropna()\n",
    "#Feature timestamp is removed and NAs are removed\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.6), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.6):int(len(df_shuffle) * 0.8), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.8):, :]\n",
    "#Data is splited into training, validation and test sets(60-20-20)\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns if col != \"rating\"]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "#Features transformed into numerical and  divided into input and output/target variable\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating',\n",
      "       'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       ...\n",
      "       'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa',\n",
      "       'Yiddish', 'Yoruba', 'Zulu'],\n",
      "      dtype='object', length=1088)\n",
      "imdbId                False\n",
      "BoxOffice             False\n",
      "Metascore             False\n",
      "Released              False\n",
      "Runtime               False\n",
      "                      ...  \n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "Length: 1093, dtype: bool\n",
      "   imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  imdbVotes  \\\n",
      "0  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "1  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "2  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "3  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "4  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "\n",
      "   movieId  tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  Washoe  Welsh  \\\n",
      "0      1.0   862.0     1.0  ...         0     0           0       0      0   \n",
      "1      1.0   862.0     5.0  ...         0     0           0       0      0   \n",
      "2      1.0   862.0     7.0  ...         0     0           0       0      0   \n",
      "3      1.0   862.0    15.0  ...         0     0           0       0      0   \n",
      "4      1.0   862.0    17.0  ...         0     0           0       0      0   \n",
      "\n",
      "   Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0      0      0        0       0     0  \n",
      "1      0      0        0       0     0  \n",
      "2      0      0        0       0     0  \n",
      "3      0      0        0       0     0  \n",
      "4      0      0        0       0     0  \n",
      "\n",
      "[5 rows x 1088 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Here, I have created for each set (train, validation and testing) new 6 variables. For training and validation, the data related to the average of the \n",
    "#rating is taken from the set itself, but the columns of the testing set can not be created from the average of the rating of the training set (would be considered data leakage)\n",
    "#So the solution is using the average from the data of the training set\n",
    "#NAs are also removed\n",
    "print(df2.columns)\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_train).any())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1093)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_col = df_train.select_dtypes(include=['number']).columns\n",
    "#Only numerical columns are normalized\n",
    "\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.transform(df_val[numerical_col])  # Usar transform en validación\n",
    "df_test[numerical_col] = scaler.transform(df_test[numerical_col]) \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating', 'imdbVotes', 'movieId', 'tmdbId', 'userId', 'Country_Afghanistan, Ireland, Japan, Netherlands, Iran', 'Country_Argentina', 'Country_Argentina, Spain', 'Country_Argentina, Spain, France, United Kingdom', 'Country_Argentina, Spain, Germany', 'Country_Argentina, United States', 'Country_Aruba, Hong Kong, United States', 'Country_Australia', 'Country_Australia, Canada', 'Country_Australia, China, Germany, United States', 'Country_Australia, France', 'Country_Australia, Germany', 'Country_Australia, Germany, United States', 'Country_Australia, South Africa', 'Country_Australia, United Kingdom', 'Country_Australia, United Kingdom, France', 'Country_Australia, United Kingdom, United States', 'Country_Australia, United Kingdom, United States, France', 'Country_Australia, United States', 'Country_Australia, United States, South Korea, Taiwan, United Kingdom, France, Canada, Thailand, Denmark', 'Country_Australia, United States, United Kingdom', 'Country_Austria', 'Country_Austria, Belgium, France, Germany', 'Country_Austria, Germany', 'Country_Austria, Germany, France', 'Country_Bahamas, United States', 'Country_Belgium', 'Country_Belgium, France', 'Country_Belgium, France, Italy', 'Country_Belgium, France, Luxembourg', 'Country_Belgium, France, Luxembourg, Tunisia', 'Country_Belgium, Germany, Canada, France, United Kingdom, Luxembourg, United States', 'Country_Belgium, Luxembourg, France', 'Country_Belgium, Netherlands, France', 'Country_Belgium, United States', 'Country_Bhutan, Australia', 'Country_Brazil', 'Country_Brazil, Argentina', 'Country_Brazil, Argentina, Italy', 'Country_Brazil, Canada, Japan, United Kingdom, Italy', 'Country_Brazil, France', 'Country_Brazil, France, Italy, United States', 'Country_Brazil, United States', 'Country_Brazil, Uruguay, Netherlands', 'Country_Canada', 'Country_Canada, China, United Kingdom', 'Country_Canada, France', 'Country_Canada, France, Japan', 'Country_Canada, France, United States', 'Country_Canada, France, United States, Japan', 'Country_Canada, Germany, France, United Kingdom, United States', 'Country_Canada, Germany, France, United States', 'Country_Canada, Germany, South Africa, United Kingdom, United States', 'Country_Canada, Germany, United States', 'Country_Canada, Ireland', 'Country_Canada, Italy, United States, United Kingdom, Austria', 'Country_Canada, New Zealand, United States', 'Country_Canada, South Africa', 'Country_Canada, South Africa, United States', 'Country_Canada, South Korea, United States', 'Country_Canada, Spain, France', 'Country_Canada, Spain, Japan', 'Country_Canada, United Arab Emirates, United States', 'Country_Canada, United Kingdom', 'Country_Canada, United Kingdom, Australia, United States', 'Country_Canada, United Kingdom, Iceland, United States, Australia', 'Country_Canada, United Kingdom, Japan, Netherlands', 'Country_Canada, United Kingdom, United States', 'Country_Canada, United Kingdom, United States, India', 'Country_Canada, United States', 'Country_Canada, United States, Australia', 'Country_Canada, United States, France, Germany, United Kingdom', 'Country_Canada, United States, Germany', 'Country_Canada, United States, Germany, France', 'Country_Canada, United States, Hungary, United Kingdom', 'Country_Canada, United States, India', 'Country_Canada, United States, Russia', 'Country_Canada, United States, Sweden', 'Country_Canada, United States, Taiwan, South Korea', 'Country_Canada, United States, United Kingdom', 'Country_Chile', 'Country_Chile, France', 'Country_Chile, Israel, United States', 'Country_Chile, United Kingdom, France, Japan', 'Country_China', 'Country_China, Canada, United States, Hong Kong', 'Country_China, France', 'Country_China, France, Hong Kong, United States', 'Country_China, France, Japan', 'Country_China, Hong Kong', 'Country_China, Hong Kong, Japan, Taiwan, South Korea, United States', 'Country_China, Hong Kong, United States', 'Country_China, Italy, United Kingdom, France', 'Country_China, South Korea', 'Country_China, Taiwan, Hong Kong, United States', 'Country_China, United States', 'Country_China, United States, Canada, Japan', 'Country_Colombia, Ecuador, United States', 'Country_Colombia, Mexico, Venezuela, Argentina, Spain, Netherlands', 'Country_Czech Republic', 'Country_Czech Republic, France, Germany, United States', 'Country_Czech Republic, France, United States', 'Country_Czech Republic, Slovakia', 'Country_Czech Republic, Slovakia, Austria', 'Country_Czech Republic, United Kingdom, France, Italy, United States', 'Country_Czech Republic, United Kingdom, France, United States', 'Country_Czech Republic, United Kingdom, Germany, Canada, United States', 'Country_Czech Republic, United Kingdom, Germany, Denmark, Italy, France, United States', 'Country_Czech Republic, United Kingdom, Germany, United States', 'Country_Czech Republic, United Kingdom, Germany, United States, Switzerland, Ireland', 'Country_Czech Republic, United Kingdom, United States', 'Country_Czech Republic, United States', 'Country_Denmark', 'Country_Denmark, Finland, Sweden, Italy', 'Country_Denmark, France, Germany, United Kingdom', 'Country_Denmark, France, United States, Sweden, Belgium, Thailand', 'Country_Denmark, Germany', 'Country_Denmark, Germany, Belgium, United Kingdom, France, Sweden', 'Country_Denmark, Germany, Belgium, United Kingdom, France, Sweden, United States', 'Country_Denmark, Germany, France, Sweden, Italy, Poland', 'Country_Denmark, Germany, Netherlands, Italy, United States, United Kingdom, France, Sweden, Finland, Iceland, Norway', 'Country_Denmark, Germany, Sweden', 'Country_Denmark, Netherlands, Sweden, Germany, United Kingdom, France, Finland, Norway, Italy', 'Country_Denmark, Sweden', 'Country_Denmark, Sweden, Belgium, France', 'Country_Denmark, Sweden, Belgium, Germany', 'Country_Denmark, Sweden, France, Germany', 'Country_Denmark, Sweden, France, Germany, Switzerland', 'Country_Denmark, Sweden, France, Italy, Germany, Iceland', 'Country_Denmark, Sweden, France, Netherlands, Norway, Iceland, United Kingdom, Finland, Italy, Belgium, Germany, Switzerland, United States', 'Country_Denmark, Sweden, Netherlands, France, Germany, United Kingdom, Italy, United States', 'Country_Denmark, Sweden, Norway, United Kingdom, United States, Germany, Netherlands, Israel, Spain, Belgium, Canada', 'Country_Denmark, Sweden, United Kingdom, Norway', 'Country_Denmark, United Kingdom', 'Country_Denmark, United Kingdom, South Africa, Sweden, Belgium', 'Country_Denmark, United Kingdom, Sweden, France', 'Country_Denmark, United Kingdom, Sweden, Norway', 'Country_Estonia, Georgia', 'Country_Federal Republic of Yugoslavia, France, Germany, Bulgaria, Czech Republic, Hungary', 'Country_Finland, Germany, France', 'Country_Finland, Germany, France, Italy, Sweden', 'Country_Finland, Norway, France, Sweden', 'Country_Finland, United States, Canada', 'Country_France', 'Country_France, Algeria', 'Country_France, Australia', 'Country_France, Australia, United States', 'Country_France, Australia, United States, Germany', 'Country_France, Austria, Germany', 'Country_France, Austria, Germany, Italy', 'Country_France, Belgium', 'Country_France, Belgium, Canada, United Kingdom, Latvia, South Korea', 'Country_France, Belgium, Germany, Haiti', 'Country_France, Belgium, Luxembourg, United States', 'Country_France, Belgium, Singapore', 'Country_France, Belgium, Spain', 'Country_France, Belgium, United States', 'Country_France, Bosnia and Herzegovina, Italy, Belgium, United Kingdom, Slovenia', 'Country_France, Canada', 'Country_France, Canada, Italy', 'Country_France, Canada, Italy, United States, China', 'Country_France, Canada, United States', 'Country_France, Canada, United States, Jordan', 'Country_France, Canada, United States, Spain', 'Country_France, China', 'Country_France, China, Belgium, Germany, United Arab Emirates, United States, Canada, Qatar', 'Country_France, Czechoslovakia', 'Country_France, Germany', 'Country_France, Germany, Belgium', 'Country_France, Germany, Federal Republic of Yugoslavia, Austria, Greece', 'Country_France, Germany, Italy, Canada, Japan', 'Country_France, Germany, Poland, Spain', 'Country_France, Germany, Romania', 'Country_France, Germany, Spain, Belgium', 'Country_France, Germany, Switzerland, Spain, Italy', 'Country_France, Germany, Taiwan, Canada', 'Country_France, Germany, United Kingdom, Belgium, Romania, Japan', 'Country_France, Germany, United Kingdom, Belgium, United States', 'Country_France, Germany, United Kingdom, Ireland, United States', 'Country_France, Germany, United States', 'Country_France, Germany, United States, Japan', 'Country_France, Ireland, United Kingdom, United States', 'Country_France, Italy', 'Country_France, Italy, Belgium', 'Country_France, Italy, Belgium, Iran', 'Country_France, Italy, Brazil', 'Country_France, Italy, Germany, United States', 'Country_France, Italy, Japan, United States', 'Country_France, Italy, Luxembourg, United States', 'Country_France, Italy, Romania, Belgium', 'Country_France, Italy, Spain', 'Country_France, Italy, United States', 'Country_France, Japan', 'Country_France, Japan, South Korea, Germany', 'Country_France, Japan, United States', 'Country_France, Liechtenstein, Switzerland, Germany, United States', 'Country_France, Luxembourg, Belgium', 'Country_France, Luxembourg, United Kingdom, Belgium, United States', 'Country_France, Mexico, United States', 'Country_France, Monaco, United States', 'Country_France, Netherlands, Israel, Belgium, Romania', 'Country_France, Poland', 'Country_France, Poland, Germany, United Kingdom, United States', 'Country_France, Poland, Norway', 'Country_France, Poland, Switzerland', 'Country_France, Poland, United States', 'Country_France, Portugal, Spain', 'Country_France, Romania', 'Country_France, Spain', 'Country_France, Spain, United States', 'Country_France, Sweden', 'Country_France, Switzerland', 'Country_France, Switzerland, Belgium, Luxembourg', 'Country_France, Switzerland, Côte d&#x27;Ivoire, Burkina Faso, Italy', 'Country_France, Switzerland, Germany', 'Country_France, Switzerland, Italy, United Kingdom', 'Country_France, Switzerland, Poland', 'Country_France, Switzerland, United Kingdom, Nepal', 'Country_France, Switzerland, United States', 'Country_France, Taiwan, China', 'Country_France, United Kingdom', 'Country_France, United Kingdom, Belgium', 'Country_France, United Kingdom, Belgium, Canada, United States', 'Country_France, United Kingdom, Czech Republic', 'Country_France, United Kingdom, Germany, Austria', 'Country_France, United Kingdom, Germany, Japan', 'Country_France, United Kingdom, South Africa, United States', 'Country_France, United Kingdom, South Korea', 'Country_France, United Kingdom, United States', 'Country_France, United States', 'Country_France, United States, Belgium', 'Country_France, United States, Belgium, Switzerland', 'Country_France, United States, Italy', 'Country_France, United States, Turkey, United Kingdom', 'Country_France, United States, United Kingdom', 'Country_France, United States, United Kingdom, Turkey', 'Country_France, United States, United Kingdom, Ukraine', 'Country_France, Vietnam', 'Country_France, West Germany', 'Country_France, West Germany, Cameroon', 'Country_France, West Germany, Italy', 'Country_Germany', 'Country_Germany, Australia, United States', 'Country_Germany, Austria', 'Country_Germany, Austria, France, Italy, Canada', 'Country_Germany, Austria, Italy', 'Country_Germany, Austria, Monaco, Romania, France, Switzerland, Belgium', 'Country_Germany, Belgium, United States', 'Country_Germany, Canada', 'Country_Germany, Canada, Australia, United States', 'Country_Germany, Canada, United Kingdom, South Africa, United States', 'Country_Germany, Canada, United States', 'Country_Germany, Finland, United Kingdom, Canada, France', 'Country_Germany, France', 'Country_Germany, France, Australia', 'Country_Germany, France, Czech Republic, Morocco, United States', 'Country_Germany, France, Poland', 'Country_Germany, France, Spain, United States, Belgium', 'Country_Germany, Hungary', 'Country_Germany, Italy, Austria, Switzerland', 'Country_Germany, Japan, Mexico, Canada, United States', 'Country_Germany, Japan, United States', 'Country_Germany, Luxembourg, United Kingdom, United States', 'Country_Germany, Luxembourg, United Kingdom, United States, Argentina, France', 'Country_Germany, Mongolia', 'Country_Germany, Netherlands, United States', 'Country_Germany, Saudi Arabia, United States', 'Country_Germany, Turkey, Italy', 'Country_Germany, United Kingdom', 'Country_Germany, United Kingdom, France, Canada, United States', 'Country_Germany, United Kingdom, France, Finland', 'Country_Germany, United Kingdom, France, Japan, United States', 'Country_Germany, United Kingdom, Israel', 'Country_Germany, United Kingdom, United States', 'Country_Germany, United Kingdom, United States, Italy', 'Country_Germany, United States', 'Country_Germany, United States, Australia', 'Country_Germany, United States, Canada', 'Country_Germany, United States, Czech Republic, Italy', 'Country_Germany, United States, France', 'Country_Germany, United States, Taiwan, United Kingdom, Canada, South Korea, Japan', 'Country_Germany, United States, United Kingdom', 'Country_Germany, United States, United Kingdom, Canada', 'Country_Germany, United States, United Kingdom, France, Cuba', 'Country_Hong Kong', 'Country_Hong Kong, Canada, United States, China', 'Country_Hong Kong, China', 'Country_Hong Kong, China, France, Italy, Germany, Netherlands', 'Country_Hong Kong, China, Netherlands', 'Country_Hong Kong, China, United Kingdom, United States', 'Country_Hong Kong, China, United States', 'Country_Hong Kong, France', 'Country_Hong Kong, France, China', 'Country_Hong Kong, Japan, South Korea', 'Country_Hong Kong, Singapore, Thailand, United Kingdom, Netherlands', 'Country_Hong Kong, South Korea, Japan', 'Country_Hong Kong, Taiwan', 'Country_Hong Kong, Taiwan, China, Japan', 'Country_Hong Kong, United States', 'Country_Hong Kong, United States, Japan', 'Country_Hungary', 'Country_Hungary, Canada, United Kingdom', 'Country_Hungary, Germany, Canada, Austria, France, United Kingdom, United States', 'Country_Hungary, United Kingdom', 'Country_Hungary, United States', 'Country_Iceland, Denmark, France, Norway, Germany', 'Country_Iceland, United States', 'Country_India', 'Country_India, United Kingdom, Australia, United States', 'Country_India, United States', 'Country_India, United States, France, United Kingdom', 'Country_India, United States, Italy, Germany, France, United Kingdom', 'Country_Indonesia, France, United States', 'Country_Indonesia, United States', 'Country_Iran', 'Country_Iran, France', 'Country_Iran, France, Australia', 'Country_Iran, France, Iraq', 'Country_Iran, Italy, Canada, Switzerland', 'Country_Iran, Italy, Switzerland', 'Country_Ireland, Belgium, France, Luxembourg', 'Country_Ireland, Canada, United Kingdom, United States', 'Country_Ireland, Denmark, Belgium, France, Luxembourg', 'Country_Ireland, Netherlands, United Kingdom, Germany, France, Finland', 'Country_Ireland, Netherlands, United Kingdom, Germany, United States', 'Country_Ireland, Switzerland, United Kingdom, United States', 'Country_Ireland, United Kingdom', 'Country_Ireland, United Kingdom, Canada, United States', 'Country_Ireland, United Kingdom, France, Canada', 'Country_Ireland, United Kingdom, Germany, Italy, Spain, France, Switzerland', 'Country_Ireland, United Kingdom, Germany, United States', 'Country_Ireland, United Kingdom, Greece, France, Netherlands, United States, Belgium', 'Country_Ireland, United Kingdom, United States', 'Country_Ireland, United Kingdom, United States, France', 'Country_Ireland, United States', 'Country_Ireland, United States, United Kingdom', 'Country_Israel', 'Country_Israel, France', 'Country_Israel, France, Germany, United States, Finland, Switzerland, Belgium, Australia', 'Country_Israel, United States, France', 'Country_Italy', 'Country_Italy, Algeria', 'Country_Italy, Belgium', 'Country_Italy, Canada', 'Country_Italy, France', 'Country_Italy, France, Belgium', 'Country_Italy, France, Germany', 'Country_Italy, France, Ireland', 'Country_Italy, France, Spain', 'Country_Italy, France, Spain, United Kingdom, Romania', 'Country_Italy, France, United Kingdom', 'Country_Italy, France, West Germany', 'Country_Italy, Japan, United States', 'Country_Italy, Spain, United Kingdom', 'Country_Italy, Spain, West Germany', 'Country_Italy, Spain, West Germany, United States', 'Country_Italy, Switzerland', 'Country_Italy, Turkey, Portugal, United Kingdom', 'Country_Italy, United Kingdom', 'Country_Italy, United Kingdom, United States', 'Country_Italy, United States', 'Country_Japan', 'Country_Japan, France', 'Country_Japan, Hong Kong, United States', 'Country_Japan, South Korea', 'Country_Japan, United Kingdom', 'Country_Japan, United States', 'Country_Japan, United States, Australia, Hong Kong', 'Country_Japan, United States, France', 'Country_Luxembourg, United States', 'Country_Mexico', 'Country_Mexico, Argentina, Spain', 'Country_Mexico, Canada, Puerto Rico, United States', 'Country_Mexico, France', 'Country_Mexico, Spain', 'Country_Mexico, United States', 'Country_Monaco, United States, Morocco', 'Country_Netherlands', 'Country_Netherlands, Germany, United Kingdom, Belgium', 'Country_Netherlands, Italy, Canada', 'Country_Netherlands, United Kingdom, Finland, United States', 'Country_Netherlands, United Kingdom, France', 'Country_Netherlands, United Kingdom, France, Luxembourg', 'Country_Netherlands, United Kingdom, Germany, United States', 'Country_New Zealand', 'Country_New Zealand, Australia, France', 'Country_New Zealand, Germany', 'Country_New Zealand, Japan, United States', 'Country_New Zealand, South Korea', 'Country_New Zealand, United States', 'Country_New Zealand, United States, United Kingdom', 'Country_Norway', 'Country_Norway, Iceland, United States, United Kingdom', 'Country_Norway, Sweden', 'Country_Norway, Sweden, Denmark, Germany', 'Country_Occupied Palestinian Territory, Netherlands, Israel, Germany, France', 'Country_Poland, United Kingdom, United States, Switzerland, Netherlands', 'Country_Romania, Netherlands', 'Country_Russia', 'Country_Russia, Germany, Kazakhstan', 'Country_Russia, Ukraine, Bulgaria, France, Spain', 'Country_Russia, United States', 'Country_Russia, United States, China', 'Country_Saudi Arabia, Netherlands, Germany, Jordan, United Arab Emirates, United States', 'Country_Serbia and Montenegro, United States', 'Country_Singapore, United States, Poland, Thailand', 'Country_South Africa, Botswana', 'Country_South Africa, Botswana, United States', 'Country_South Africa, United States', 'Country_South Africa, United States, New Zealand, Canada', 'Country_South Korea', 'Country_South Korea, Czech Republic', 'Country_South Korea, Germany', 'Country_South Korea, Japan', 'Country_South Korea, United States', 'Country_South Korea, United States, Canada, Netherlands, Switzerland, China', 'Country_Soviet Union', 'Country_Spain', 'Country_Spain, Belgium', 'Country_Spain, Canada', 'Country_Spain, France', 'Country_Spain, France, Colombia', 'Country_Spain, France, Italy', 'Country_Spain, France, United Kingdom', 'Country_Spain, France, United States', 'Country_Spain, France, United States, Germany', 'Country_Spain, Ireland', 'Country_Spain, Italy', 'Country_Spain, Malta, Bulgaria, United States', 'Country_Spain, Mexico', 'Country_Spain, Mexico, France', 'Country_Spain, Mexico, France, Belgium', 'Country_Spain, Thailand, United States', 'Country_Spain, United Kingdom', 'Country_Spain, United Kingdom, Bulgaria', 'Country_Spain, United Kingdom, France, United States', 'Country_Spain, United Kingdom, United States, Canada, Netherlands', 'Country_Spain, United States', 'Country_Spain, United States, France', 'Country_Sweden', 'Country_Sweden, Denmark', 'Country_Sweden, Denmark, Germany', 'Country_Sweden, Denmark, Germany, Norway, Spain', 'Country_Sweden, Denmark, Italy', 'Country_Sweden, France, United Kingdom', 'Country_Sweden, France, West Germany', 'Country_Sweden, Germany, Denmark', 'Country_Sweden, Germany, France, Denmark, United States', 'Country_Sweden, Germany, Norway, France, Denmark', 'Country_Sweden, Norway, Denmark, France, Germany', 'Country_Sweden, Norway, France', 'Country_Sweden, South Africa, United States', 'Country_Sweden, United Kingdom, Finland', 'Country_Sweden, United States', 'Country_Switzerland, Canada, United States', 'Country_Switzerland, France, Poland', 'Country_Switzerland, France, Spain, Monaco, United Arab Emirates, United States', 'Country_Switzerland, Germany, United Kingdom', 'Country_Switzerland, United Kingdom, France, Italy', 'Country_Switzerland, United Kingdom, United States, Mexico', 'Country_Switzerland, United States', 'Country_Taiwan, France', 'Country_Taiwan, Japan', 'Country_Taiwan, United States', 'Country_Taiwan, United States, Hong Kong, China', 'Country_Thailand', 'Country_Thailand, France', 'Country_Thailand, Japan, Netherlands', 'Country_Thailand, United States, Hong Kong, France, Australia', 'Country_Tunisia, France', 'Country_United Arab Emirates, United States', 'Country_United Arab Emirates, United States, France', 'Country_United Kingdom', 'Country_United Kingdom, Australia', 'Country_United Kingdom, Australia, France, United States', 'Country_United Kingdom, Australia, Germany', 'Country_United Kingdom, Australia, New Zealand, United States', 'Country_United Kingdom, Australia, United States', 'Country_United Kingdom, Australia, United States, Jordan, Morocco', 'Country_United Kingdom, Austria, United States', 'Country_United Kingdom, Belgium', 'Country_United Kingdom, Canada', 'Country_United Kingdom, Canada, Italy', 'Country_United Kingdom, Canada, Romania, United States', 'Country_United Kingdom, Canada, United States', 'Country_United Kingdom, Canada, United States, Australia', 'Country_United Kingdom, China', 'Country_United Kingdom, China, Japan, United States, Mexico', 'Country_United Kingdom, China, United States', 'Country_United Kingdom, Czech Republic, Germany, United States', 'Country_United Kingdom, Czech Republic, Mexico, United States', 'Country_United Kingdom, Denmark, France, Germany, Japan, United States', 'Country_United Kingdom, Denmark, Indonesia, Norway', 'Country_United Kingdom, Denmark, United States, Cayman Islands', 'Country_United Kingdom, France', 'Country_United Kingdom, France, Argentina, United States', 'Country_United Kingdom, France, Australia, Japan, United States', 'Country_United Kingdom, France, Australia, United States', 'Country_United Kingdom, France, Belgium', 'Country_United Kingdom, France, Belgium, Italy', 'Country_United Kingdom, France, Canada', 'Country_United Kingdom, France, Canada, United States', 'Country_United Kingdom, France, Denmark, Canada, United States', 'Country_United Kingdom, France, Egypt, Japan, Mexico, United States, Iran', 'Country_United Kingdom, France, Germany, Canada, United States', 'Country_United Kingdom, France, Germany, Italy, Spain, Japan, United States', 'Country_United Kingdom, France, Germany, Japan, United States', 'Country_United Kingdom, France, Germany, Spain, Italy, Switzerland', 'Country_United Kingdom, France, Germany, Spain, United States', 'Country_United Kingdom, France, Germany, Switzerland, Luxembourg, United States', 'Country_United Kingdom, France, Germany, United States', 'Country_United Kingdom, France, Ireland, United States', 'Country_United Kingdom, France, Italy', 'Country_United Kingdom, France, Italy, Germany, Canada, United States', 'Country_United Kingdom, France, Italy, Germany, United States', 'Country_United Kingdom, France, Italy, Japan, United States', 'Country_United Kingdom, France, Italy, Pakistan, United States', 'Country_United Kingdom, France, Italy, United States', 'Country_United Kingdom, France, Japan', 'Country_United Kingdom, France, Japan, United States', 'Country_United Kingdom, France, Mexico, United States', 'Country_United Kingdom, France, Monaco, United States', 'Country_United Kingdom, France, Slovakia, Tunisia, Italy, Bulgaria, United States', 'Country_United Kingdom, France, South Africa', 'Country_United Kingdom, France, Spain', 'Country_United Kingdom, France, Spain, United States, Japan', 'Country_United Kingdom, France, United States', 'Country_United Kingdom, France, United States, Canada', 'Country_United Kingdom, France, United States, Czech Republic', 'Country_United Kingdom, France, United States, Germany', 'Country_United Kingdom, France, United States, India', 'Country_United Kingdom, Germany', 'Country_United Kingdom, Germany, Canada', 'Country_United Kingdom, Germany, Canada, United States', 'Country_United Kingdom, Germany, France, Italy', 'Country_United Kingdom, Germany, France, United States', 'Country_United Kingdom, Germany, Greece, France', 'Country_United Kingdom, Germany, Hungary', 'Country_United Kingdom, Germany, Ireland', 'Country_United Kingdom, Germany, Ireland, United States', 'Country_United Kingdom, Germany, Japan, Canada, United States, Australia', 'Country_United Kingdom, Germany, Japan, United States', 'Country_United Kingdom, Germany, Kenya, France, United States, Switzerland', 'Country_United Kingdom, Germany, Luxembourg', 'Country_United Kingdom, Germany, Luxembourg, United States, Canada', 'Country_United Kingdom, Germany, Romania, United States', 'Country_United Kingdom, Germany, South Africa, United States', 'Country_United Kingdom, Germany, Spain', 'Country_United Kingdom, Germany, Spain, Lithuania', 'Country_United Kingdom, Germany, Spain, Morocco, United States, Italy, France', 'Country_United Kingdom, Germany, Spain, United States', 'Country_United Kingdom, Germany, Sweden, Belgium, United States', 'Country_United Kingdom, Germany, United States', 'Country_United Kingdom, Germany, United States, Canada', 'Country_United Kingdom, Germany, United States, France', 'Country_United Kingdom, Germany, United States, Hong Kong, China, Thailand, Canada', 'Country_United Kingdom, Germany, United States, Vietnam, France', 'Country_United Kingdom, Hungary', 'Country_United Kingdom, Hungary, United States', 'Country_United Kingdom, India', 'Country_United Kingdom, India, United States', 'Country_United Kingdom, Ireland', 'Country_United Kingdom, Ireland, Belgium, United States', 'Country_United Kingdom, Ireland, France', 'Country_United Kingdom, Ireland, France, Germany', 'Country_United Kingdom, Ireland, France, United States', 'Country_United Kingdom, Ireland, Japan, United States', 'Country_United Kingdom, Ireland, United States', 'Country_United Kingdom, Isle of Man, United States', 'Country_United Kingdom, Italy', 'Country_United Kingdom, Italy, France, United States', 'Country_United Kingdom, Italy, Germany', 'Country_United Kingdom, Italy, Luxembourg, United States', 'Country_United Kingdom, Italy, United States', 'Country_United Kingdom, Japan', 'Country_United Kingdom, Japan, United States', 'Country_United Kingdom, Luxembourg, France, Belgium, United States', 'Country_United Kingdom, Luxembourg, Spain', 'Country_United Kingdom, Luxembourg, United States', 'Country_United Kingdom, Mexico', 'Country_United Kingdom, Netherlands, France, United States', 'Country_United Kingdom, Netherlands, Luxembourg, Germany', 'Country_United Kingdom, Netherlands, Luxembourg, United States, France, Germany', 'Country_United Kingdom, New Zealand', 'Country_United Kingdom, New Zealand, United States', 'Country_United Kingdom, Norway', 'Country_United Kingdom, Romania, France, United States', 'Country_United Kingdom, Russia, Italy, France, Netherlands', 'Country_United Kingdom, Russia, United States', 'Country_United Kingdom, South Africa', 'Country_United Kingdom, South Africa, United States, India', 'Country_United Kingdom, Spain', 'Country_United Kingdom, Spain, United States', 'Country_United Kingdom, Sweden, Denmark, Ireland', 'Country_United Kingdom, Switzerland, United States', 'Country_United Kingdom, United Arab Emirates, United States', 'Country_United Kingdom, United States', 'Country_United Kingdom, United States, Australia', 'Country_United Kingdom, United States, Canada', 'Country_United Kingdom, United States, Canada, France, Australia', 'Country_United Kingdom, United States, China', 'Country_United Kingdom, United States, Denmark, Canada, Italy', 'Country_United Kingdom, United States, France', 'Country_United Kingdom, United States, France, Argentina, Peru, Chile, Germany', 'Country_United Kingdom, United States, France, Germany, Japan', 'Country_United Kingdom, United States, France, Italy', 'Country_United Kingdom, United States, Germany', 'Country_United Kingdom, United States, Germany, Denmark, Belgium, Japan', 'Country_United Kingdom, United States, Germany, Netherlands', 'Country_United Kingdom, United States, Iceland', 'Country_United Kingdom, United States, India, France', 'Country_United Kingdom, United States, Isle of Man', 'Country_United Kingdom, United States, Italy', 'Country_United Kingdom, United States, Japan', 'Country_United Kingdom, United States, Japan, China, Croatia', 'Country_United Kingdom, United States, Japan, France', 'Country_United Kingdom, United States, Mexico', 'Country_United Kingdom, United States, Morocco, Italy, Austria, Canada, Mexico', 'Country_United Kingdom, United States, Netherlands', 'Country_United Kingdom, United States, Netherlands, Italy', 'Country_United Kingdom, United States, New Zealand, France, Canada, India', 'Country_United Kingdom, United States, Serbia', 'Country_United Kingdom, United States, Singapore, Hong Kong, United Arab Emirates, India', 'Country_United Kingdom, United States, Spain', 'Country_United Kingdom, United States, Sweden', 'Country_United Kingdom, United States, Turkey, China', 'Country_United Kingdom, United States, West Germany', 'Country_United Kingdom, West Germany, Netherlands', 'Country_United Kingdom, West Germany, United States', 'Country_United States', 'Country_United States, Algeria, New Zealand', 'Country_United States, Argentina', 'Country_United States, Argentina, Spain, Italy', 'Country_United States, Australia', 'Country_United States, Australia, Canada', 'Country_United States, Australia, Canada, Mexico, India, United Kingdom', 'Country_United States, Australia, China', 'Country_United States, Australia, China, Hong Kong', 'Country_United States, Australia, France', 'Country_United States, Australia, France, Canada, Germany, Brazil, United Kingdom, China', 'Country_United States, Australia, France, Canada, Spain', 'Country_United States, Australia, Hong Kong', 'Country_United States, Australia, Hong Kong, Switzerland, China', 'Country_United States, Australia, Mexico', 'Country_United States, Australia, Philippines, Italy, Denmark, United Kingdom', 'Country_United States, Australia, South Korea', 'Country_United States, Australia, Spain', 'Country_United States, Australia, Switzerland, Canada', 'Country_United States, Australia, United Kingdom', 'Country_United States, Australia, United Kingdom, Canada', 'Country_United States, Austria', 'Country_United States, Austria, Greenland', 'Country_United States, Bahrain', 'Country_United States, Belgium, Canada', 'Country_United States, Belgium, Denmark, France', 'Country_United States, Brazil', 'Country_United States, Brazil, Canada', 'Country_United States, Brazil, Peru', 'Country_United States, Bulgaria', 'Country_United States, Bulgaria, Germany, India', 'Country_United States, Bulgaria, Spain, Germany', 'Country_United States, Bulgaria, United Kingdom', 'Country_United States, Canada', 'Country_United States, Canada, Australia', 'Country_United States, Canada, Brazil', 'Country_United States, Canada, Bulgaria', 'Country_United States, Canada, China', 'Country_United States, Canada, France', 'Country_United States, Canada, France, Germany', 'Country_United States, Canada, Germany', 'Country_United States, Canada, India', 'Country_United States, Canada, Indonesia', 'Country_United States, Canada, Italy, United Kingdom', 'Country_United States, Canada, Japan', 'Country_United States, Canada, Japan, France', 'Country_United States, Canada, Luxembourg, United Kingdom, Russia', 'Country_United States, Canada, Mexico', 'Country_United States, Canada, Netherlands', 'Country_United States, Canada, Netherlands, Japan, United Kingdom', 'Country_United States, Canada, Russia', 'Country_United States, Canada, Spain', 'Country_United States, Canada, United Kingdom', 'Country_United States, Canada, United Kingdom, Germany', 'Country_United States, Canada, Vietnam', 'Country_United States, Chile, United Kingdom', 'Country_United States, China', 'Country_United States, China, Brazil, Germany', 'Country_United States, China, Canada', 'Country_United States, China, France, Norway', 'Country_United States, China, Hong Kong', 'Country_United States, China, Japan', 'Country_United States, China, Japan, Canada, United Arab Emirates', 'Country_United States, China, Spain, Japan, Iceland, Germany, Italy', 'Country_United States, China, United Kingdom, Austria, Morocco, Hong Kong', 'Country_United States, China, Vietnam, Canada, Germany, United Kingdom, Singapore, India, Australia', 'Country_United States, Croatia, Bosnia and Herzegovina, Mexico', 'Country_United States, Czech Republic', 'Country_United States, Czech Republic, Canada', 'Country_United States, Czech Republic, France', 'Country_United States, Czech Republic, Thailand', 'Country_United States, Czech Republic, United Kingdom', 'Country_United States, Denmark', 'Country_United States, Denmark, Australia', 'Country_United States, Denmark, Australia, Norway', 'Country_United States, France', 'Country_United States, France, Canada', 'Country_United States, France, Canada, Belgium', 'Country_United States, France, Canada, Spain', 'Country_United States, France, China, United Kingdom', 'Country_United States, France, Germany', 'Country_United States, France, Germany, Bulgaria', 'Country_United States, France, Japan', 'Country_United States, France, Luxembourg, Germany, Canada, United Kingdom', 'Country_United States, France, Morocco', 'Country_United States, France, United Kingdom', 'Country_United States, France, United Kingdom, Austria, Germany, Italy', 'Country_United States, France, United Kingdom, Hong Kong, Taiwan, Malta, Spain, Canada', 'Country_United States, Germany', 'Country_United States, Germany, Australia', 'Country_United States, Germany, Australia, France, Pakistan', 'Country_United States, Germany, Belgium', 'Country_United States, Germany, Brazil, Italy', 'Country_United States, Germany, Canada', 'Country_United States, Germany, Canada, France', 'Country_United States, Germany, Canada, Japan', 'Country_United States, Germany, China', 'Country_United States, Germany, China, Canada, United Kingdom, Hong Kong', 'Country_United States, Germany, China, Italy', 'Country_United States, Germany, Czech Republic', 'Country_United States, Germany, Czech Republic, United Kingdom', 'Country_United States, Germany, France', 'Country_United States, Germany, France, Spain', 'Country_United States, Germany, France, United Kingdom', 'Country_United States, Germany, Hong Kong, Singapore, China, United Kingdom, Spain', 'Country_United States, Germany, Hungary', 'Country_United States, Germany, India, Russia', 'Country_United States, Germany, Japan', 'Country_United States, Germany, Japan, France, United Kingdom', 'Country_United States, Germany, Japan, United Kingdom, Hong Kong', 'Country_United States, Germany, Netherlands', 'Country_United States, Germany, Paraguay, Uruguay', 'Country_United States, Germany, Poland', 'Country_United States, Germany, Puerto Rico', 'Country_United States, Germany, Russia', 'Country_United States, Germany, Switzerland', 'Country_United States, Germany, Taiwan, France, Philippines, South Korea, Canada, United Kingdom', 'Country_United States, Germany, United Arab Emirates', 'Country_United States, Germany, United Kingdom', 'Country_United States, Germany, United Kingdom, Canada', 'Country_United States, Germany, United Kingdom, France, Turkey', 'Country_United States, Germany, United Kingdom, Japan', 'Country_United States, Greece', 'Country_United States, Hong Kong', 'Country_United States, Hong Kong, Bulgaria, Netherlands, Canada, United Kingdom, France', 'Country_United States, Hong Kong, Cambodia, United Kingdom', 'Country_United States, Hong Kong, Canada', 'Country_United States, Hong Kong, China', 'Country_United States, Hong Kong, Japan, Canada, New Zealand', 'Country_United States, Hong Kong, Japan, China', 'Country_United States, Hong Kong, Japan, United Arab Emirates', 'Country_United States, Hong Kong, Nepal', 'Country_United States, Hong Kong, Taiwan, Argentina', 'Country_United States, Hungary', 'Country_United States, Iceland', 'Country_United States, India', 'Country_United States, India, Canada', 'Country_United States, India, Canada, Italy, United Kingdom, Zambia', 'Country_United States, India, Germany', 'Country_United States, India, Hong Kong, China, Canada', 'Country_United States, India, Jordan', 'Country_United States, India, Singapore, Canada, United Kingdom, Japan, Australia', 'Country_United States, India, United Kingdom', 'Country_United States, India, United Kingdom, Australia', 'Country_United States, India, United Kingdom, Mexico, Japan, Canada', 'Country_United States, Indonesia', 'Country_United States, Iraq, Canada', 'Country_United States, Ireland', 'Country_United States, Ireland, United Kingdom', 'Country_United States, Israel', 'Country_United States, Italy', 'Country_United States, Italy, Canada, Japan, United Kingdom', 'Country_United States, Italy, Hong Kong, Canada', 'Country_United States, Italy, Romania', 'Country_United States, Italy, Spain', 'Country_United States, Italy, Switzerland, Thailand, United Kingdom', 'Country_United States, Italy, United Kingdom', 'Country_United States, Italy, United Kingdom, Germany, France', 'Country_United States, Japan', 'Country_United States, Japan, Australia, New Zealand', 'Country_United States, Japan, Brazil', 'Country_United States, Japan, Colombia, Canada', 'Country_United States, Japan, France', 'Country_United States, Japan, Germany, Chile, Canada, United Kingdom', 'Country_United States, Japan, India', 'Country_United States, Japan, Mexico', 'Country_United States, Japan, South Korea', 'Country_United States, Japan, South Korea, Canada, United Arab Emirates, Philippines', 'Country_United States, Japan, Spain, Hong Kong', 'Country_United States, Japan, United Kingdom', 'Country_United States, Japan, United Kingdom, Canada', 'Country_United States, Jordan, United Kingdom', 'Country_United States, Luxembourg', 'Country_United States, Malta', 'Country_United States, Malta, France, United Kingdom', 'Country_United States, Mexico', 'Country_United States, Mexico, Australia, Canada', 'Country_United States, Mexico, Canada', 'Country_United States, Mexico, Canada, Germany, United Kingdom', 'Country_United States, Mexico, France, Japan', 'Country_United States, Mexico, France, Panama', 'Country_United States, Mexico, Germany', 'Country_United States, Mexico, Hong Kong', 'Country_United States, Mexico, Spain', 'Country_United States, Mexico, United Arab Emirates', 'Country_United States, Mexico, United Kingdom', 'Country_United States, Morocco', 'Country_United States, Morocco, United Kingdom, Jordan', 'Country_United States, Nepal', 'Country_United States, Netherlands', 'Country_United States, Netherlands Antilles, Italy', 'Country_United States, Netherlands, Italy', 'Country_United States, Netherlands, United Kingdom, Denmark', 'Country_United States, New Zealand', 'Country_United States, New Zealand, Australia', 'Country_United States, New Zealand, Germany', 'Country_United States, New Zealand, Japan', 'Country_United States, Norway', 'Country_United States, Peru', 'Country_United States, Philippines', 'Country_United States, Poland', 'Country_United States, Poland, Slovenia, Czech Republic, United Kingdom, Australia, Germany, New Zealand', 'Country_United States, Poland, United Arab Emirates, India', 'Country_United States, Romania', 'Country_United States, Romania, France, Italy, Germany', 'Country_United States, Romania, Germany, France', 'Country_United States, Russia', 'Country_United States, Russia, United Kingdom', 'Country_United States, Singapore, Australia, United Kingdom, Czech Republic', 'Country_United States, South Africa, India', 'Country_United States, South Africa, New Zealand, Germany', 'Country_United States, South Korea', 'Country_United States, South Korea, India, Hong Kong, United Kingdom, Canada', 'Country_United States, South Korea, Japan, Canada', 'Country_United States, Spain', 'Country_United States, Spain, France, Belgium, Portugal, United Kingdom', 'Country_United States, Spain, France, Canada, United Kingdom, China, Italy, Germany', 'Country_United States, Spain, Hungary', 'Country_United States, Spain, United Kingdom', 'Country_United States, Spain, United Kingdom, France', 'Country_United States, Sweden, Hungary', 'Country_United States, Switzerland', 'Country_United States, Taiwan', 'Country_United States, Taiwan, Japan, United Kingdom', 'Country_United States, Taiwan, Philippines, China', 'Country_United States, Taiwan, United Kingdom, Canada, India, France', 'Country_United States, Thailand, United Kingdom, India, Canada', 'Country_United States, United Arab Emirates', 'Country_United States, United Arab Emirates, India', 'Country_United States, United Arab Emirates, Russia, Canada, Czech Republic, India', 'Country_United States, United Arab Emirates, United Kingdom', 'Country_United States, United Kingdom', 'Country_United States, United Kingdom, Australia', 'Country_United States, United Kingdom, Australia, India', 'Country_United States, United Kingdom, Belgium, Canada', 'Country_United States, United Kingdom, Bulgaria', 'Country_United States, United Kingdom, Canada', 'Country_United States, United Kingdom, Canada, Australia, Germany', 'Country_United States, United Kingdom, Canada, China', 'Country_United States, United Kingdom, Canada, France', 'Country_United States, United Kingdom, Canada, Japan', 'Country_United States, United Kingdom, Czech Republic', 'Country_United States, United Kingdom, Czech Republic, Germany, Bahamas', 'Country_United States, United Kingdom, Czech Republic, Italy', 'Country_United States, United Kingdom, Denmark, France, Canada, Australia', 'Country_United States, United Kingdom, France', 'Country_United States, United Kingdom, France, Canada', 'Country_United States, United Kingdom, France, China, Canada', 'Country_United States, United Kingdom, France, Germany', 'Country_United States, United Kingdom, France, Germany, Japan', 'Country_United States, United Kingdom, Germany', 'Country_United States, United Kingdom, Germany, Canada, Singapore', 'Country_United States, United Kingdom, Germany, Hungary', 'Country_United States, United Kingdom, Germany, Japan', 'Country_United States, United Kingdom, Germany, Netherlands, France, Italy, Morocco, Thailand', 'Country_United States, United Kingdom, Germany, Sweden', 'Country_United States, United Kingdom, Hungary', 'Country_United States, United Kingdom, Hungary, Israel', 'Country_United States, United Kingdom, Hungary, Jordan', 'Country_United States, United Kingdom, India', 'Country_United States, United Kingdom, Ireland', 'Country_United States, United Kingdom, Italy', 'Country_United States, United Kingdom, Italy, Japan, France', 'Country_United States, United Kingdom, Italy, Taiwan, Mexico, Japan', 'Country_United States, United Kingdom, Japan', 'Country_United States, United Kingdom, Japan, Germany', 'Country_United States, United Kingdom, Japan, Hungary', 'Country_United States, United Kingdom, Malta, Morocco', 'Country_United States, United Kingdom, Mexico', 'Country_United States, United Kingdom, Monaco', 'Country_United States, United Kingdom, New Zealand', 'Country_United States, United Kingdom, New Zealand, India', 'Country_United States, United Kingdom, Qatar', 'Country_United States, United Kingdom, Romania', 'Country_United States, United Kingdom, Slovakia', 'Country_United States, United Kingdom, South Africa, Italy', 'Country_United States, United Kingdom, South Korea', 'Country_United States, United Kingdom, Spain', 'Country_United States, United Kingdom, Spain, South Korea', 'Country_United States, United Kingdom, Switzerland, Panama', 'Country_United States, Venezuela', 'Country_United States, Vietnam, United Kingdom, Canada, Denmark', 'Country_United States, West Germany', 'Country_West Germany', 'Country_West Germany, France', 'Country_West Germany, France, United Kingdom', 'Country_West Germany, Italy, France', 'Country_West Germany, United States', 'Country_Yugoslavia, United States', 'Aboriginal', 'Acholi', 'Afrikaans', 'Akan', 'Albanian', 'Algonquin', 'American Sign', 'Amharic', 'Ancient (to 1453)', 'Apache languages', 'Arabic', 'Aramaic', 'Armenian', 'Assyrian Neo-Aramaic', 'Azerbaijani', 'Bambara', 'Belarusian', 'Bengali', 'Berber languages', 'Bosnian', 'Brazilian Sign', 'British Sign', 'Bulgarian', 'Burmese', 'Cantonese', 'Catalan', 'Central Khmer', 'Chechen', 'Chinese', 'Cornish', 'Corsican', 'Cree', 'Croatian', 'Czech', 'Danish', 'Dari', 'Dutch', 'Egyptian (Ancient)', 'English', 'Esperanto', 'Estonian', 'Filipino', 'Finnish', 'Flemish', 'French', 'French Sign', 'Gaelic', 'Galician', 'Georgian', 'German', 'Greek', 'Greenlandic', 'Guarani', 'Haitian', 'Hakka', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hmong', 'Hokkien', 'Hopi', 'Hungarian', 'Ibo', 'Icelandic', 'Indonesian', 'Inuktitut', 'Irish Gaelic', 'Italian', 'Japanese', 'Japanese Sign', 'Kalmyk-Oirat', 'Kikuyu', 'Kinyarwanda', 'Kirundi', 'Klingon', 'Korean', 'Korean Sign', 'Kurdish', 'Ladino', 'Lao', 'Latin', 'Latvian', 'Lingala', 'Luxembourgish', 'Malay', 'Mandarin', 'Mandingo', 'Maori', 'Mapudungun', 'Maya', 'Mende', 'Middle English', 'Min Nan', 'Mohawk', 'Mongolian', 'Nahuatl', 'Nama', 'Navajo', 'Neapolitan', 'Nepali', 'None', 'Norse', 'North American Indian', 'Norwegian', 'Nyanja', 'Old', 'Old English', 'Papiamento', 'Pashtu', 'Pawnee', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Quechua', 'Quenya', 'Romanian', 'Romany', 'Russian', 'Ryukyuan', 'Saami', 'Sanskrit', 'Scots', 'Serbian', 'Serbo-Croatian', 'Shanghainese', 'Sicilian', 'Sign', 'Sindarin', 'Sinhala', 'Sioux', 'Slovak', 'Somali', 'Sotho', 'Spanish', 'Spanish Sign', 'Swahili', 'Swedish', 'Swiss German', 'Syriac', 'Tagalog', 'Tamil', 'Thai', 'Tibetan', 'Tok Pisin', 'Turkish', 'Ukrainian', 'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa', 'Yiddish', 'Yoruba', 'Zulu', 'avg_movie_rating', 'count_movie_rating', 'std_movie_rating', 'avg_user_rating', 'count_user_rating', 'std_user_rating']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "print(feature_cols)\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
    "#Tensors are transformed into TensorDatasets and then into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1092, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
    "#The neural network is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy())  \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.043108, MSE: 0.086530, RMSE: 0.294160, R²: -0.840525\n",
      "VALIDATION -> Loss: 0.018207, MSE: 0.036432, RMSE: 0.190873, R²: 0.227559\n",
      "Iteration 2/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.023532, MSE: 0.047078, RMSE: 0.216974, R²: -0.001357\n",
      "VALIDATION -> Loss: 0.015604, MSE: 0.031225, RMSE: 0.176707, R²: 0.337962\n",
      "Iteration 3/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020512, MSE: 0.041026, RMSE: 0.202549, R²: 0.127362\n",
      "VALIDATION -> Loss: 0.014633, MSE: 0.029281, RMSE: 0.171118, R²: 0.379179\n",
      "Iteration 4/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019468, MSE: 0.038945, RMSE: 0.197344, R²: 0.171634\n",
      "VALIDATION -> Loss: 0.014468, MSE: 0.028953, RMSE: 0.170155, R²: 0.386144\n",
      "Iteration 5/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018755, MSE: 0.037507, RMSE: 0.193668, R²: 0.202207\n",
      "VALIDATION -> Loss: 0.013411, MSE: 0.026832, RMSE: 0.163805, R²: 0.431103\n",
      "Iteration 6/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018397, MSE: 0.036803, RMSE: 0.191841, R²: 0.217188\n",
      "VALIDATION -> Loss: 0.013415, MSE: 0.026844, RMSE: 0.163843, R²: 0.430844\n",
      "Iteration 7/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018032, MSE: 0.036078, RMSE: 0.189942, R²: 0.232612\n",
      "VALIDATION -> Loss: 0.013459, MSE: 0.026930, RMSE: 0.164103, R²: 0.429033\n",
      "Iteration 8/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017535, MSE: 0.035079, RMSE: 0.187294, R²: 0.253852\n",
      "VALIDATION -> Loss: 0.013135, MSE: 0.026283, RMSE: 0.162121, R²: 0.442741\n",
      "Iteration 9/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017376, MSE: 0.034788, RMSE: 0.186515, R²: 0.260049\n",
      "VALIDATION -> Loss: 0.013748, MSE: 0.027509, RMSE: 0.165859, R²: 0.416751\n",
      "Iteration 10/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016823, MSE: 0.033621, RMSE: 0.183360, R²: 0.284870\n",
      "VALIDATION -> Loss: 0.013168, MSE: 0.026342, RMSE: 0.162302, R²: 0.441498\n",
      "Iteration 11/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016637, MSE: 0.033264, RMSE: 0.182384, R²: 0.292465\n",
      "VALIDATION -> Loss: 0.013018, MSE: 0.026048, RMSE: 0.161393, R²: 0.447737\n",
      "Iteration 12/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016425, MSE: 0.032921, RMSE: 0.181440, R²: 0.299766\n",
      "VALIDATION -> Loss: 0.012995, MSE: 0.025998, RMSE: 0.161240, R²: 0.448780\n",
      "Iteration 13/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016290, MSE: 0.032583, RMSE: 0.180508, R²: 0.306947\n",
      "VALIDATION -> Loss: 0.013031, MSE: 0.026066, RMSE: 0.161450, R²: 0.447343\n",
      "Iteration 14/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016109, MSE: 0.032179, RMSE: 0.179385, R²: 0.315541\n",
      "VALIDATION -> Loss: 0.013557, MSE: 0.027124, RMSE: 0.164695, R²: 0.424907\n",
      "Iteration 15/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015830, MSE: 0.031696, RMSE: 0.178035, R²: 0.325806\n",
      "VALIDATION -> Loss: 0.012864, MSE: 0.025734, RMSE: 0.160419, R²: 0.454381\n",
      "Iteration 16/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015732, MSE: 0.031474, RMSE: 0.177408, R²: 0.330546\n",
      "VALIDATION -> Loss: 0.013010, MSE: 0.026031, RMSE: 0.161341, R²: 0.448093\n",
      "Iteration 17/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015589, MSE: 0.031199, RMSE: 0.176631, R²: 0.336395\n",
      "VALIDATION -> Loss: 0.013155, MSE: 0.026316, RMSE: 0.162222, R²: 0.442049\n",
      "Iteration 18/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015277, MSE: 0.030545, RMSE: 0.174770, R²: 0.350303\n",
      "VALIDATION -> Loss: 0.013375, MSE: 0.026756, RMSE: 0.163573, R²: 0.432714\n",
      "Iteration 19/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015248, MSE: 0.030441, RMSE: 0.174474, R²: 0.352505\n",
      "VALIDATION -> Loss: 0.013104, MSE: 0.026216, RMSE: 0.161913, R²: 0.444168\n",
      "Iteration 20/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015049, MSE: 0.030084, RMSE: 0.173448, R²: 0.360100\n",
      "VALIDATION -> Loss: 0.012734, MSE: 0.025475, RMSE: 0.159608, R²: 0.459886\n",
      "Iteration 21/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014936, MSE: 0.029893, RMSE: 0.172896, R²: 0.364161\n",
      "VALIDATION -> Loss: 0.013144, MSE: 0.026292, RMSE: 0.162148, R²: 0.442556\n",
      "Iteration 22/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014868, MSE: 0.029699, RMSE: 0.172335, R²: 0.368282\n",
      "VALIDATION -> Loss: 0.012911, MSE: 0.025827, RMSE: 0.160708, R²: 0.452416\n",
      "Iteration 23/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014704, MSE: 0.029439, RMSE: 0.171579, R²: 0.373815\n",
      "VALIDATION -> Loss: 0.012912, MSE: 0.025833, RMSE: 0.160725, R²: 0.452296\n",
      "Iteration 24/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014538, MSE: 0.029085, RMSE: 0.170543, R²: 0.381353\n",
      "VALIDATION -> Loss: 0.012681, MSE: 0.025373, RMSE: 0.159287, R²: 0.462051\n",
      "Iteration 25/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014385, MSE: 0.028780, RMSE: 0.169647, R²: 0.387837\n",
      "VALIDATION -> Loss: 0.012831, MSE: 0.025672, RMSE: 0.160224, R²: 0.455708\n",
      "Iteration 26/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014430, MSE: 0.028850, RMSE: 0.169853, R²: 0.386346\n",
      "VALIDATION -> Loss: 0.012773, MSE: 0.025548, RMSE: 0.159837, R²: 0.458331\n",
      "Iteration 27/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014261, MSE: 0.028500, RMSE: 0.168818, R²: 0.393802\n",
      "VALIDATION -> Loss: 0.012749, MSE: 0.025505, RMSE: 0.159703, R²: 0.459242\n",
      "Iteration 28/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014162, MSE: 0.028303, RMSE: 0.168234, R²: 0.397988\n",
      "VALIDATION -> Loss: 0.012703, MSE: 0.025412, RMSE: 0.159410, R²: 0.461222\n",
      "Iteration 29/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014126, MSE: 0.028243, RMSE: 0.168057, R²: 0.399257\n",
      "VALIDATION -> Loss: 0.013001, MSE: 0.026008, RMSE: 0.161271, R²: 0.448572\n",
      "Iteration 30/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014013, MSE: 0.028019, RMSE: 0.167389, R²: 0.404024\n",
      "VALIDATION -> Loss: 0.013079, MSE: 0.026164, RMSE: 0.161752, R²: 0.445276\n",
      "Iteration 31/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014007, MSE: 0.028025, RMSE: 0.167408, R²: 0.403889\n",
      "VALIDATION -> Loss: 0.012799, MSE: 0.025608, RMSE: 0.160025, R²: 0.457055\n",
      "Iteration 32/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013859, MSE: 0.027722, RMSE: 0.166499, R²: 0.410342\n",
      "VALIDATION -> Loss: 0.012690, MSE: 0.025387, RMSE: 0.159333, R²: 0.461746\n",
      "Iteration 33/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013831, MSE: 0.027655, RMSE: 0.166297, R²: 0.411772\n",
      "VALIDATION -> Loss: 0.012664, MSE: 0.025335, RMSE: 0.159171, R²: 0.462840\n",
      "Iteration 34/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013755, MSE: 0.027469, RMSE: 0.165738, R²: 0.415721\n",
      "VALIDATION -> Loss: 0.012767, MSE: 0.025543, RMSE: 0.159823, R²: 0.458429\n",
      "Iteration 35/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013721, MSE: 0.027429, RMSE: 0.165617, R²: 0.416576\n",
      "VALIDATION -> Loss: 0.012674, MSE: 0.025353, RMSE: 0.159227, R²: 0.462460\n",
      "Iteration 36/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013770, MSE: 0.027551, RMSE: 0.165986, R²: 0.413972\n",
      "VALIDATION -> Loss: 0.012699, MSE: 0.025404, RMSE: 0.159387, R²: 0.461377\n",
      "Iteration 37/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013672, MSE: 0.027350, RMSE: 0.165377, R²: 0.418263\n",
      "VALIDATION -> Loss: 0.013029, MSE: 0.026068, RMSE: 0.161457, R²: 0.447296\n",
      "Iteration 38/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013684, MSE: 0.027354, RMSE: 0.165391, R²: 0.418167\n",
      "VALIDATION -> Loss: 0.013081, MSE: 0.026169, RMSE: 0.161770, R²: 0.445155\n",
      "Iteration 39/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013626, MSE: 0.027205, RMSE: 0.164941, R²: 0.421331\n",
      "VALIDATION -> Loss: 0.012959, MSE: 0.025929, RMSE: 0.161023, R²: 0.450262\n",
      "Iteration 40/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013434, MSE: 0.026874, RMSE: 0.163932, R²: 0.428389\n",
      "VALIDATION -> Loss: 0.012806, MSE: 0.025622, RMSE: 0.160070, R²: 0.456752\n",
      "Iteration 41/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013413, MSE: 0.026842, RMSE: 0.163836, R²: 0.429053\n",
      "VALIDATION -> Loss: 0.013118, MSE: 0.026248, RMSE: 0.162011, R²: 0.443496\n",
      "Iteration 42/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013342, MSE: 0.026661, RMSE: 0.163282, R²: 0.432911\n",
      "VALIDATION -> Loss: 0.013106, MSE: 0.026221, RMSE: 0.161928, R²: 0.444067\n",
      "Iteration 43/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013287, MSE: 0.026557, RMSE: 0.162962, R²: 0.435130\n",
      "VALIDATION -> Loss: 0.012856, MSE: 0.025722, RMSE: 0.160382, R²: 0.454632\n",
      "Iteration 44/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013252, MSE: 0.026490, RMSE: 0.162757, R²: 0.436553\n",
      "VALIDATION -> Loss: 0.012769, MSE: 0.025547, RMSE: 0.159834, R²: 0.458354\n",
      "Iteration 45/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013190, MSE: 0.026407, RMSE: 0.162503, R²: 0.438311\n",
      "VALIDATION -> Loss: 0.012724, MSE: 0.025456, RMSE: 0.159551, R²: 0.460273\n",
      "Iteration 46/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013163, MSE: 0.026335, RMSE: 0.162281, R²: 0.439840\n",
      "VALIDATION -> Loss: 0.012751, MSE: 0.025511, RMSE: 0.159721, R²: 0.459120\n",
      "Iteration 47/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013144, MSE: 0.026313, RMSE: 0.162214, R²: 0.440307\n",
      "VALIDATION -> Loss: 0.013213, MSE: 0.026434, RMSE: 0.162584, R²: 0.439555\n",
      "Iteration 48/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013259, MSE: 0.026523, RMSE: 0.162860, R²: 0.435837\n",
      "VALIDATION -> Loss: 0.013008, MSE: 0.026019, RMSE: 0.161303, R²: 0.448354\n",
      "Iteration 49/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013040, MSE: 0.026078, RMSE: 0.161488, R²: 0.445304\n",
      "VALIDATION -> Loss: 0.012906, MSE: 0.025826, RMSE: 0.160704, R²: 0.452438\n",
      "Iteration 50/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013037, MSE: 0.026080, RMSE: 0.161494, R²: 0.445260\n",
      "VALIDATION -> Loss: 0.013487, MSE: 0.026985, RMSE: 0.164270, R²: 0.427868\n",
      "Iteration 51/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013008, MSE: 0.026032, RMSE: 0.161346, R²: 0.446281\n",
      "VALIDATION -> Loss: 0.012866, MSE: 0.025744, RMSE: 0.160450, R²: 0.454169\n",
      "Iteration 52/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012993, MSE: 0.025992, RMSE: 0.161219, R²: 0.447149\n",
      "VALIDATION -> Loss: 0.012885, MSE: 0.025781, RMSE: 0.160564, R²: 0.453393\n",
      "Iteration 53/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012956, MSE: 0.025926, RMSE: 0.161017, R²: 0.448534\n",
      "VALIDATION -> Loss: 0.012999, MSE: 0.026008, RMSE: 0.161269, R²: 0.448583\n",
      "Iteration 54/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012955, MSE: 0.025950, RMSE: 0.161091, R²: 0.448026\n",
      "VALIDATION -> Loss: 0.013042, MSE: 0.026092, RMSE: 0.161530, R²: 0.446795\n",
      "Iteration 55/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012838, MSE: 0.025669, RMSE: 0.160215, R²: 0.454016\n",
      "VALIDATION -> Loss: 0.012973, MSE: 0.025959, RMSE: 0.161117, R²: 0.449621\n",
      "Iteration 56/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012848, MSE: 0.025686, RMSE: 0.160268, R²: 0.453654\n",
      "VALIDATION -> Loss: 0.013162, MSE: 0.026339, RMSE: 0.162294, R²: 0.441551\n",
      "Iteration 57/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012804, MSE: 0.025590, RMSE: 0.159968, R²: 0.455697\n",
      "VALIDATION -> Loss: 0.013189, MSE: 0.026393, RMSE: 0.162460, R²: 0.440406\n",
      "Iteration 58/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012731, MSE: 0.025484, RMSE: 0.159636, R²: 0.457954\n",
      "VALIDATION -> Loss: 0.012940, MSE: 0.025893, RMSE: 0.160913, R²: 0.451015\n",
      "Iteration 59/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012720, MSE: 0.025461, RMSE: 0.159565, R²: 0.458438\n",
      "VALIDATION -> Loss: 0.012972, MSE: 0.025949, RMSE: 0.161088, R²: 0.449819\n",
      "Iteration 60/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012698, MSE: 0.025407, RMSE: 0.159395, R²: 0.459591\n",
      "VALIDATION -> Loss: 0.013076, MSE: 0.026161, RMSE: 0.161742, R²: 0.445342\n",
      "Iteration 61/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012736, MSE: 0.025482, RMSE: 0.159632, R²: 0.457979\n",
      "VALIDATION -> Loss: 0.012930, MSE: 0.025871, RMSE: 0.160845, R²: 0.451480\n",
      "Iteration 62/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012765, MSE: 0.025550, RMSE: 0.159842, R²: 0.456552\n",
      "VALIDATION -> Loss: 0.012883, MSE: 0.025775, RMSE: 0.160545, R²: 0.453525\n",
      "Iteration 63/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012652, MSE: 0.025308, RMSE: 0.159085, R²: 0.461688\n",
      "VALIDATION -> Loss: 0.012937, MSE: 0.025882, RMSE: 0.160878, R²: 0.451257\n",
      "Iteration 64/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012558, MSE: 0.025127, RMSE: 0.158515, R²: 0.465541\n",
      "VALIDATION -> Loss: 0.013010, MSE: 0.026032, RMSE: 0.161345, R²: 0.448064\n",
      "Iteration 65/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012580, MSE: 0.025184, RMSE: 0.158696, R²: 0.464318\n",
      "VALIDATION -> Loss: 0.013300, MSE: 0.026612, RMSE: 0.163131, R²: 0.435777\n",
      "Iteration 66/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012685, MSE: 0.025339, RMSE: 0.159181, R²: 0.461037\n",
      "VALIDATION -> Loss: 0.013280, MSE: 0.026566, RMSE: 0.162992, R²: 0.436736\n",
      "Iteration 67/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012585, MSE: 0.025167, RMSE: 0.158642, R²: 0.464681\n",
      "VALIDATION -> Loss: 0.013118, MSE: 0.026245, RMSE: 0.162003, R²: 0.443550\n",
      "Iteration 68/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012438, MSE: 0.024883, RMSE: 0.157745, R²: 0.470721\n",
      "VALIDATION -> Loss: 0.013308, MSE: 0.026629, RMSE: 0.163184, R²: 0.435414\n",
      "Iteration 69/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012572, MSE: 0.025132, RMSE: 0.158530, R²: 0.465438\n",
      "VALIDATION -> Loss: 0.013126, MSE: 0.026261, RMSE: 0.162052, R²: 0.443217\n",
      "Iteration 70/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012459, MSE: 0.024932, RMSE: 0.157898, R²: 0.469693\n",
      "VALIDATION -> Loss: 0.012974, MSE: 0.025958, RMSE: 0.161116, R²: 0.449631\n",
      "Iteration 71/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012469, MSE: 0.024897, RMSE: 0.157789, R²: 0.470422\n",
      "VALIDATION -> Loss: 0.013429, MSE: 0.026866, RMSE: 0.163907, R²: 0.430396\n",
      "Iteration 72/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012375, MSE: 0.024751, RMSE: 0.157325, R²: 0.473534\n",
      "VALIDATION -> Loss: 0.013253, MSE: 0.026514, RMSE: 0.162832, R²: 0.437847\n",
      "Iteration 73/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012327, MSE: 0.024655, RMSE: 0.157020, R²: 0.475570\n",
      "VALIDATION -> Loss: 0.013569, MSE: 0.027143, RMSE: 0.164751, R²: 0.424512\n",
      "Iteration 74/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012329, MSE: 0.024661, RMSE: 0.157038, R²: 0.475451\n",
      "VALIDATION -> Loss: 0.013214, MSE: 0.026438, RMSE: 0.162598, R²: 0.439460\n",
      "Iteration 75/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012312, MSE: 0.024622, RMSE: 0.156914, R²: 0.476284\n",
      "VALIDATION -> Loss: 0.013564, MSE: 0.027140, RMSE: 0.164743, R²: 0.424572\n",
      "Iteration 76/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012339, MSE: 0.024675, RMSE: 0.157082, R²: 0.475158\n",
      "VALIDATION -> Loss: 0.013281, MSE: 0.026576, RMSE: 0.163023, R²: 0.436525\n",
      "Iteration 77/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012249, MSE: 0.024522, RMSE: 0.156595, R²: 0.478410\n",
      "VALIDATION -> Loss: 0.013350, MSE: 0.026715, RMSE: 0.163446, R²: 0.433595\n",
      "Iteration 78/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012189, MSE: 0.024396, RMSE: 0.156191, R²: 0.481097\n",
      "VALIDATION -> Loss: 0.013407, MSE: 0.026814, RMSE: 0.163748, R²: 0.431498\n",
      "Iteration 79/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012219, MSE: 0.024427, RMSE: 0.156292, R²: 0.480426\n",
      "VALIDATION -> Loss: 0.013356, MSE: 0.026725, RMSE: 0.163477, R²: 0.433382\n",
      "Iteration 80/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012138, MSE: 0.024254, RMSE: 0.155738, R²: 0.484099\n",
      "VALIDATION -> Loss: 0.013508, MSE: 0.027019, RMSE: 0.164376, R²: 0.427132\n",
      "Iteration 81/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012125, MSE: 0.024248, RMSE: 0.155717, R²: 0.484239\n",
      "VALIDATION -> Loss: 0.013629, MSE: 0.027266, RMSE: 0.165123, R²: 0.421915\n",
      "Iteration 82/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012081, MSE: 0.024150, RMSE: 0.155402, R²: 0.486325\n",
      "VALIDATION -> Loss: 0.013584, MSE: 0.027177, RMSE: 0.164855, R²: 0.423787\n",
      "Iteration 83/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012030, MSE: 0.024105, RMSE: 0.155258, R²: 0.487277\n",
      "VALIDATION -> Loss: 0.013486, MSE: 0.026973, RMSE: 0.164233, R²: 0.428128\n",
      "Iteration 84/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012037, MSE: 0.024101, RMSE: 0.155246, R²: 0.487356\n",
      "VALIDATION -> Loss: 0.013223, MSE: 0.026447, RMSE: 0.162627, R²: 0.439261\n",
      "Iteration 85/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012028, MSE: 0.024030, RMSE: 0.155015, R²: 0.488877\n",
      "VALIDATION -> Loss: 0.013700, MSE: 0.027403, RMSE: 0.165537, R²: 0.419009\n",
      "Iteration 86/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012027, MSE: 0.024019, RMSE: 0.154980, R²: 0.489110\n",
      "VALIDATION -> Loss: 0.013582, MSE: 0.027170, RMSE: 0.164834, R²: 0.423933\n",
      "Iteration 87/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011911, MSE: 0.023850, RMSE: 0.154433, R²: 0.492708\n",
      "VALIDATION -> Loss: 0.013758, MSE: 0.027519, RMSE: 0.165888, R²: 0.416543\n",
      "Iteration 88/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011955, MSE: 0.023945, RMSE: 0.154742, R²: 0.490676\n",
      "VALIDATION -> Loss: 0.014008, MSE: 0.028018, RMSE: 0.167385, R²: 0.405965\n",
      "Iteration 89/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011915, MSE: 0.023823, RMSE: 0.154348, R²: 0.493269\n",
      "VALIDATION -> Loss: 0.013474, MSE: 0.026956, RMSE: 0.164182, R²: 0.428482\n",
      "Iteration 90/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011872, MSE: 0.023736, RMSE: 0.154066, R²: 0.495118\n",
      "VALIDATION -> Loss: 0.013437, MSE: 0.026890, RMSE: 0.163983, R²: 0.429868\n",
      "Iteration 91/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011874, MSE: 0.023708, RMSE: 0.153975, R²: 0.495715\n",
      "VALIDATION -> Loss: 0.013612, MSE: 0.027240, RMSE: 0.165044, R²: 0.422465\n",
      "Iteration 92/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011972, MSE: 0.023966, RMSE: 0.154809, R²: 0.490237\n",
      "VALIDATION -> Loss: 0.013532, MSE: 0.027078, RMSE: 0.164554, R²: 0.425889\n",
      "Iteration 93/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011803, MSE: 0.023600, RMSE: 0.153622, R²: 0.498025\n",
      "VALIDATION -> Loss: 0.013635, MSE: 0.027270, RMSE: 0.165137, R²: 0.421817\n",
      "Iteration 94/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011870, MSE: 0.023745, RMSE: 0.154094, R²: 0.494935\n",
      "VALIDATION -> Loss: 0.013384, MSE: 0.026770, RMSE: 0.163615, R²: 0.432422\n",
      "Iteration 95/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011714, MSE: 0.023416, RMSE: 0.153024, R²: 0.501924\n",
      "VALIDATION -> Loss: 0.013530, MSE: 0.027065, RMSE: 0.164513, R²: 0.426177\n",
      "Iteration 96/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011721, MSE: 0.023409, RMSE: 0.153000, R²: 0.502084\n",
      "VALIDATION -> Loss: 0.013681, MSE: 0.027365, RMSE: 0.165424, R²: 0.419802\n",
      "Iteration 97/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011800, MSE: 0.023607, RMSE: 0.153647, R²: 0.497862\n",
      "VALIDATION -> Loss: 0.013415, MSE: 0.026834, RMSE: 0.163811, R²: 0.431065\n",
      "Iteration 98/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011702, MSE: 0.023409, RMSE: 0.152999, R²: 0.502087\n",
      "VALIDATION -> Loss: 0.013572, MSE: 0.027142, RMSE: 0.164748, R²: 0.424538\n",
      "Iteration 99/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011748, MSE: 0.023465, RMSE: 0.153183, R²: 0.500892\n",
      "VALIDATION -> Loss: 0.013735, MSE: 0.027472, RMSE: 0.165748, R²: 0.417532\n",
      "Iteration 100/100 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.011727, MSE: 0.023469, RMSE: 0.153196, R²: 0.500805\n",
      "VALIDATION -> Loss: 0.013677, MSE: 0.027354, RMSE: 0.165390, R²: 0.420046\n"
     ]
    }
   ],
   "source": [
    "#The model is trained and validated for 100 epochs\n",
    "for i in range(100): \n",
    "    print(f\"Iteration {i+1}/100 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.5611\n",
      "MAE: 0.1054\n",
      "MSE: 0.0206\n",
      "RMSE: 0.1436\n",
      "Precisión: 0.6502\n",
      "Recall: 0.9453\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#And display the evaluation metrics of the model\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The films recommended for that user are: \n",
      "    movieId                                              title\n",
      "0    1596.0                                Career Girls (1997)\n",
      "1    2151.0                  Gods Must Be Crazy II, The (1989)\n",
      "2    2436.0                          Tea with Mussolini (1999)\n",
      "3    3241.0                           Cup, The (Phörpa) (1999)\n",
      "4    3266.0  Man Bites Dog (C'est arrivé près de chez vous)...\n",
      "5    3855.0  Affair of Love, An (Liaison pornographique, Un...\n",
      "6    4021.0                          Before Night Falls (2000)\n",
      "7    5135.0                             Monsoon Wedding (2001)\n",
      "8  110130.0                                Nut Job, The (2014)\n",
      "9  122912.0             Avengers: Infinity War - Part I (2018)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8927/3274941804.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  unseen_movies_df[\"predicted_rating\"] = predicted_ratings.cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "#This is the function I have created to, by giving a userId in the arguments, return the recommended movies for that user\n",
    "def recommend_movies(user_id, model, df2, top_n=10, device=\"cuda\"):\n",
    "\n",
    "    avg_movie_rating = df2.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "    count_movie_rating = df2.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "    std_movie_rating = df2.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "    avg_user_rating = df2.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "    count_user_rating = df2.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "    std_user_rating = df2.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")  \n",
    "\n",
    "    movies_seen = df2[df2[\"userId\"] == user_id][\"movieId\"].unique()\n",
    "    movies_seen = df2[df2[\"userId\"] == user_id][\"movieId\"].unique()\n",
    "    unseen_movies_df = df2[~df2[\"movieId\"].isin(movies_seen)]\n",
    "    unseen_movies_df = unseen_movies_df.drop_duplicates(subset=\"movieId\")   \n",
    "\n",
    "    unseen_movies_df = unseen_movies_df.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "    unseen_movies_df = unseen_movies_df.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "    unseen_movies_df = unseen_movies_df.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "    unseen_movies_df = unseen_movies_df.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "    unseen_movies_df = unseen_movies_df.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "    unseen_movies_df = unseen_movies_df.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "    df_copy = unseen_movies_df.copy()\n",
    "\n",
    "    feature_cols = [col for col in unseen_movies_df.columns if col not in [\"rating\", \"timestamp\"]]\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    unseen_movies_df[feature_cols] = scaler.fit_transform(unseen_movies_df[feature_cols])\n",
    "\n",
    "    x_unseen = torch.tensor(unseen_movies_df[feature_cols].to_numpy(dtype=np.float32)).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_ratings = model(x_unseen).squeeze() \n",
    "\n",
    "    unseen_movies_df[\"predicted_rating\"] = predicted_ratings.cpu().numpy()\n",
    "\n",
    "    top_unseen = unseen_movies_df.sort_values(by=\"predicted_rating\", ascending=False).head(10)\n",
    "\n",
    "    df_copy['movieIdNormalized'] = scaler.fit_transform(df_copy[['movieId']])\n",
    "\n",
    "    filtered_df = df_copy[df_copy['movieIdNormalized'].isin(top_unseen['movieId'])]\n",
    "    merged_df = filtered_df.merge(movies, on='movieId', how='left')\n",
    "    result = merged_df[['movieId', 'title']]\n",
    "    print(result)\n",
    "\n",
    "print(\"The films recommended for that user are: \")\n",
    "print(recommend_movies( 1, model, df2, top_n=10, device=\"cuda\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
