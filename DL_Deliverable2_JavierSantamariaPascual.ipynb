{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this Jupyter Notebook, 1 will develop a movie recommendation system that leverages deep learning techniques to predict user preferences based on their past ratings.\n",
    "\n",
    "I will cover the following steps:\n",
    "\n",
    "1. **Download and preprocess the MovieLens dataset**\n",
    "\n",
    "2. **Split the dataset into training, validation, and testing sets**\n",
    "\n",
    "3. **Implement a neural network architecture for the recommendation system**\n",
    "    \n",
    "4. **Train and evaluate the model using different metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first of all, I will load the data in the different variables links, movies, ratings and tags from the .csv.\n",
    "\n",
    "I will also display the first 5 rows of each one with the head() function and some metrics with the describe() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset: \n",
      "              movieId\n",
      "count    9742.000000\n",
      "mean    42200.353623\n",
      "std     52160.494854\n",
      "min         1.000000\n",
      "25%      3248.250000\n",
      "50%      7300.000000\n",
      "75%     76232.000000\n",
      "max    193609.000000\n",
      "---------------------------------------------\n",
      "First 5 rows: \n",
      "    movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#MOVIES DATASET\n",
    "movies = pd.read_csv(\"ml-latest-small/movies.csv\")\n",
    "print(\"Summary of the dataset: \\n\", movies.describe())\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"First 5 rows: \\n\", movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset: \n",
      "               userId        movieId         rating     timestamp\n",
      "count  100836.000000  100836.000000  100836.000000  1.008360e+05\n",
      "mean      326.127564   19435.295718       3.501557  1.205946e+09\n",
      "std       182.618491   35530.987199       1.042529  2.162610e+08\n",
      "min         1.000000       1.000000       0.500000  8.281246e+08\n",
      "25%       177.000000    1199.000000       3.000000  1.019124e+09\n",
      "50%       325.000000    2991.000000       3.500000  1.186087e+09\n",
      "75%       477.000000    8122.000000       4.000000  1.435994e+09\n",
      "max       610.000000  193609.000000       5.000000  1.537799e+09\n",
      "---------------------------------------------\n",
      "First 5 rows: \n",
      "    userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n"
     ]
    }
   ],
   "source": [
    "#RATINGS DATASET\n",
    "ratings = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "print(\"Summary of the dataset: \\n\",ratings.describe())\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"First 5 rows: \\n\", ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset: \n",
      "             userId        movieId     timestamp\n",
      "count  3683.000000    3683.000000  3.683000e+03\n",
      "mean    431.149335   27252.013576  1.320032e+09\n",
      "std     158.472553   43490.558803  1.721025e+08\n",
      "min       2.000000       1.000000  1.137179e+09\n",
      "25%     424.000000    1262.500000  1.137521e+09\n",
      "50%     474.000000    4454.000000  1.269833e+09\n",
      "75%     477.000000   39263.000000  1.498457e+09\n",
      "max     610.000000  193565.000000  1.537099e+09\n",
      "---------------------------------------------\n",
      "First 5 rows: \n",
      "    userId  movieId              tag   timestamp\n",
      "0       2    60756            funny  1445714994\n",
      "1       2    60756  Highly quotable  1445714996\n",
      "2       2    60756     will ferrell  1445714992\n",
      "3       2    89774     Boxing story  1445715207\n",
      "4       2    89774              MMA  1445715200\n"
     ]
    }
   ],
   "source": [
    "#TAGS DATASET\n",
    "tags = pd.read_csv(\"ml-latest-small/tags.csv\")\n",
    "print(\"Summary of the dataset: \\n\",tags.describe())\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"First 5 rows: \\n\", tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset: \n",
      "              movieId        imdbId         tmdbId\n",
      "count    9742.000000  9.742000e+03    9734.000000\n",
      "mean    42200.353623  6.771839e+05   55162.123793\n",
      "std     52160.494854  1.107228e+06   93653.481487\n",
      "min         1.000000  4.170000e+02       2.000000\n",
      "25%      3248.250000  9.518075e+04    9665.500000\n",
      "50%      7300.000000  1.672605e+05   16529.000000\n",
      "75%     76232.000000  8.055685e+05   44205.750000\n",
      "max    193609.000000  8.391976e+06  525662.000000\n",
      "---------------------------------------------\n",
      "First 5 rows: \n",
      "    movieId  imdbId   tmdbId\n",
      "0        1  114709    862.0\n",
      "1        2  113497   8844.0\n",
      "2        3  113228  15602.0\n",
      "3        4  114885  31357.0\n",
      "4        5  113041  11862.0\n"
     ]
    }
   ],
   "source": [
    "#LINKS DATASET\n",
    "links = pd.read_csv(\"ml-latest-small/links.csv\")\n",
    "print(\"Summary of the dataset: \\n\",links.describe())\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"First 5 rows: \\n\", links.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FIRST MODEL**\n",
    "# Using only original data (genres and ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title  (no genres listed)  Action  \\\n",
      "0        1                    Toy Story (1995)                   0       0   \n",
      "1        2                      Jumanji (1995)                   0       0   \n",
      "2        3             Grumpier Old Men (1995)                   0       0   \n",
      "3        4            Waiting to Exhale (1995)                   0       0   \n",
      "4        5  Father of the Bride Part II (1995)                   0       0   \n",
      "\n",
      "   Adventure  Animation  Children  Comedy  Crime  Documentary  ...  Film-Noir  \\\n",
      "0          1          1         1       1      0            0  ...          0   \n",
      "1          1          0         1       0      0            0  ...          0   \n",
      "2          0          0         0       1      0            0  ...          0   \n",
      "3          0          0         0       1      0            0  ...          0   \n",
      "4          0          0         0       1      0            0  ...          0   \n",
      "\n",
      "   Horror  IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
      "0       0     0        0        0        0       0         0    0        0  \n",
      "1       0     0        0        0        0       0         0    0        0  \n",
      "2       0     0        0        0        1       0         0    0        0  \n",
      "3       0     0        0        0        1       0         0    0        0  \n",
      "4       0     0        0        0        0       0         0    0        0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "   userId  movieId  rating  timestamp\n",
      "0       1        1       4  964982703\n",
      "1       1        3       4  964981247\n",
      "2       1        6       4  964982224\n",
      "3       1       47       5  964983815\n",
      "4       1       50       5  964982931\n",
      "   movieId  imdbId  tmdbId\n",
      "0        1  114709     862\n",
      "1        2  113497    8844\n",
      "2        3  113228   15602\n",
      "3        4  114885   31357\n",
      "4        5  113041   11862\n",
      "   userId  movieId              tag   timestamp\n",
      "0       2    60756            funny  1445714994\n",
      "1       2    60756  Highly quotable  1445714996\n",
      "2       2    60756     will ferrell  1445714992\n",
      "3       2    89774     Boxing story  1445715207\n",
      "4       2    89774              MMA  1445715200\n"
     ]
    }
   ],
   "source": [
    "movies[\"genres\"] = movies[\"genres\"].fillna(\"\").astype(str)\n",
    "movies[\"genres\"] = movies[\"genres\"].str.split('|')\n",
    "moviesExploded = movies.explode(\"genres\")\n",
    "movies_dummies = pd.get_dummies(moviesExploded[\"genres\"], prefix=\"\", prefix_sep=\"\", dtype=int)\n",
    "movies_dummies = moviesExploded[[\"movieId\"]].join(movies_dummies).groupby(\"movieId\").max()\n",
    "movies_final = movies.drop(columns=[\"genres\"]).merge(movies_dummies, on=\"movieId\")\n",
    "print(movies_final.head())\n",
    "\n",
    "ratings[\"rating\"] = ratings[\"rating\"].astype(int)\n",
    "print(ratings.head())\n",
    "\n",
    "links[\"tmdbId\"] = links[\"tmdbId\"].fillna(-1).astype(int)\n",
    "print(links.head())\n",
    "\n",
    "print(tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80668, 22) (80668, 1)\n",
      "(10084, 22) (10084, 1)\n",
      "(10084, 22) (10084, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df = ratings.merge(movies_final, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df2 = df.copy()\n",
    "numerical_col = [col for col in df2.columns if col not in [\"title\"]]\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\", \"title\"])\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.8), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.8):int(len(df_shuffle) * 0.9), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.9):, :]\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns if col not in [\"rating\", \"timestamp\", \"title\"]]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId                False\n",
      "movieId               False\n",
      "rating                False\n",
      "(no genres listed)    False\n",
      "Action                False\n",
      "Adventure             False\n",
      "Animation             False\n",
      "Children              False\n",
      "Comedy                False\n",
      "Crime                 False\n",
      "Documentary           False\n",
      "Drama                 False\n",
      "Fantasy               False\n",
      "Film-Noir             False\n",
      "Horror                False\n",
      "IMAX                  False\n",
      "Musical               False\n",
      "Mystery               False\n",
      "Romance               False\n",
      "Sci-Fi                False\n",
      "Thriller              False\n",
      "War                   False\n",
      "Western               False\n",
      "avg_movie_rating      False\n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "dtype: bool\n",
      "   userId  movieId  rating  timestamp                        title  \\\n",
      "0       1        1       4  964982703             Toy Story (1995)   \n",
      "1       1        3       4  964981247      Grumpier Old Men (1995)   \n",
      "2       1        6       4  964982224                  Heat (1995)   \n",
      "3       1       47       5  964983815  Seven (a.k.a. Se7en) (1995)   \n",
      "4       1       50       5  964982931   Usual Suspects, The (1995)   \n",
      "\n",
      "   (no genres listed)  Action  Adventure  Animation  Children  ...  Film-Noir  \\\n",
      "0                   0       0          1          1         1  ...          0   \n",
      "1                   0       0          0          0         0  ...          0   \n",
      "2                   0       1          0          0         0  ...          0   \n",
      "3                   0       0          0          0         0  ...          0   \n",
      "4                   0       0          0          0         0  ...          0   \n",
      "\n",
      "   Horror  IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
      "0       0     0        0        0        0       0         0    0        0  \n",
      "1       0     0        0        0        1       0         0    0        0  \n",
      "2       0     0        0        0        0       0         1    0        0  \n",
      "3       0     0        0        1        0       0         1    0        0  \n",
      "4       0     0        0        1        0       0         1    0        0  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_test).any())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = [\"movieId\", \"rating\", \"userId\", \"std_movie_rating\",  \"avg_movie_rating\",  \"count_movie_rating\", \"avg_user_rating\",  \"count_user_rating\",  \"std_user_rating\"]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.fit_transform(df_val[numerical_col])\n",
    "df_test[numerical_col] = scaler.fit_transform(df_test[numerical_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy()) \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.052538, MSE: 0.105261, RMSE: 0.324439, R²: -1.223391\n",
      "VALIDATION -> Loss: 0.017285, MSE: 0.034591, RMSE: 0.185986, R²: 0.286081\n",
      "Iteration 2/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019013, MSE: 0.038034, RMSE: 0.195022, R²: 0.196624\n",
      "VALIDATION -> Loss: 0.013096, MSE: 0.026218, RMSE: 0.161921, R²: 0.458879\n",
      "Iteration 3/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017171, MSE: 0.034358, RMSE: 0.185360, R²: 0.274259\n",
      "VALIDATION -> Loss: 0.011494, MSE: 0.023015, RMSE: 0.151708, R²: 0.524987\n",
      "Iteration 4/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016355, MSE: 0.032714, RMSE: 0.180870, R²: 0.308993\n",
      "VALIDATION -> Loss: 0.010864, MSE: 0.021754, RMSE: 0.147491, R²: 0.551029\n",
      "Iteration 5/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015886, MSE: 0.031773, RMSE: 0.178251, R²: 0.328862\n",
      "VALIDATION -> Loss: 0.010596, MSE: 0.021222, RMSE: 0.145676, R²: 0.562010\n",
      "Iteration 6/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015548, MSE: 0.031096, RMSE: 0.176340, R²: 0.343169\n",
      "VALIDATION -> Loss: 0.010442, MSE: 0.020912, RMSE: 0.144609, R²: 0.568403\n",
      "Iteration 7/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015337, MSE: 0.030674, RMSE: 0.175141, R²: 0.352075\n",
      "VALIDATION -> Loss: 0.010358, MSE: 0.020744, RMSE: 0.144026, R²: 0.571876\n",
      "Iteration 8/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015173, MSE: 0.030349, RMSE: 0.174210, R²: 0.358942\n",
      "VALIDATION -> Loss: 0.010314, MSE: 0.020658, RMSE: 0.143731, R²: 0.573631\n",
      "Iteration 9/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015048, MSE: 0.030093, RMSE: 0.173473, R²: 0.364360\n",
      "VALIDATION -> Loss: 0.010406, MSE: 0.020839, RMSE: 0.144358, R²: 0.569899\n",
      "Iteration 10/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014929, MSE: 0.029865, RMSE: 0.172814, R²: 0.369176\n",
      "VALIDATION -> Loss: 0.010265, MSE: 0.020561, RMSE: 0.143391, R²: 0.575643\n",
      "Iteration 11/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014815, MSE: 0.029634, RMSE: 0.172144, R²: 0.374057\n",
      "VALIDATION -> Loss: 0.010322, MSE: 0.020674, RMSE: 0.143784, R²: 0.573314\n",
      "Iteration 12/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014727, MSE: 0.029457, RMSE: 0.171630, R²: 0.377794\n",
      "VALIDATION -> Loss: 0.010578, MSE: 0.021188, RMSE: 0.145562, R²: 0.562694\n",
      "Iteration 13/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014683, MSE: 0.029363, RMSE: 0.171358, R²: 0.379765\n",
      "VALIDATION -> Loss: 0.010044, MSE: 0.020119, RMSE: 0.141842, R²: 0.584762\n",
      "Iteration 14/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014627, MSE: 0.029253, RMSE: 0.171035, R²: 0.382096\n",
      "VALIDATION -> Loss: 0.010101, MSE: 0.020231, RMSE: 0.142237, R²: 0.582443\n",
      "Iteration 15/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014599, MSE: 0.029204, RMSE: 0.170891, R²: 0.383136\n",
      "VALIDATION -> Loss: 0.009957, MSE: 0.019946, RMSE: 0.141232, R²: 0.588329\n",
      "Iteration 16/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014469, MSE: 0.028934, RMSE: 0.170101, R²: 0.388829\n",
      "VALIDATION -> Loss: 0.010505, MSE: 0.021044, RMSE: 0.145066, R²: 0.565669\n",
      "Iteration 17/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014392, MSE: 0.028791, RMSE: 0.169678, R²: 0.391861\n",
      "VALIDATION -> Loss: 0.010325, MSE: 0.020683, RMSE: 0.143816, R²: 0.573125\n",
      "Iteration 18/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014383, MSE: 0.028766, RMSE: 0.169606, R²: 0.392381\n",
      "VALIDATION -> Loss: 0.010075, MSE: 0.020184, RMSE: 0.142071, R²: 0.583420\n",
      "Iteration 19/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014376, MSE: 0.028755, RMSE: 0.169574, R²: 0.392609\n",
      "VALIDATION -> Loss: 0.010213, MSE: 0.020460, RMSE: 0.143037, R²: 0.577736\n",
      "Iteration 20/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014234, MSE: 0.028473, RMSE: 0.168739, R²: 0.398573\n",
      "VALIDATION -> Loss: 0.009810, MSE: 0.019652, RMSE: 0.140184, R²: 0.594410\n",
      "Iteration 21/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014217, MSE: 0.028429, RMSE: 0.168608, R²: 0.399508\n",
      "VALIDATION -> Loss: 0.010047, MSE: 0.020125, RMSE: 0.141864, R²: 0.584632\n",
      "Iteration 22/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014162, MSE: 0.028320, RMSE: 0.168285, R²: 0.401809\n",
      "VALIDATION -> Loss: 0.010142, MSE: 0.020317, RMSE: 0.142539, R²: 0.580672\n",
      "Iteration 23/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014112, MSE: 0.028223, RMSE: 0.167998, R²: 0.403846\n",
      "VALIDATION -> Loss: 0.009791, MSE: 0.019617, RMSE: 0.140059, R²: 0.595134\n",
      "Iteration 24/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014105, MSE: 0.028202, RMSE: 0.167935, R²: 0.404290\n",
      "VALIDATION -> Loss: 0.009848, MSE: 0.019727, RMSE: 0.140452, R²: 0.592860\n",
      "Iteration 25/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014022, MSE: 0.028042, RMSE: 0.167457, R²: 0.407683\n",
      "VALIDATION -> Loss: 0.009745, MSE: 0.019524, RMSE: 0.139727, R²: 0.597055\n",
      "Iteration 26/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013996, MSE: 0.027990, RMSE: 0.167302, R²: 0.408774\n",
      "VALIDATION -> Loss: 0.009608, MSE: 0.019249, RMSE: 0.138742, R²: 0.602711\n",
      "Iteration 27/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014000, MSE: 0.028004, RMSE: 0.167344, R²: 0.408478\n",
      "VALIDATION -> Loss: 0.009878, MSE: 0.019789, RMSE: 0.140675, R²: 0.591570\n",
      "Iteration 28/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014009, MSE: 0.028018, RMSE: 0.167385, R²: 0.408191\n",
      "VALIDATION -> Loss: 0.010243, MSE: 0.020521, RMSE: 0.143251, R²: 0.576473\n",
      "Iteration 29/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013995, MSE: 0.027990, RMSE: 0.167301, R²: 0.408783\n",
      "VALIDATION -> Loss: 0.009644, MSE: 0.019320, RMSE: 0.138996, R²: 0.601259\n",
      "Iteration 30/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013880, MSE: 0.027756, RMSE: 0.166603, R²: 0.413708\n",
      "VALIDATION -> Loss: 0.009507, MSE: 0.019047, RMSE: 0.138011, R²: 0.606889\n",
      "Iteration 31/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013887, MSE: 0.027777, RMSE: 0.166664, R²: 0.413279\n",
      "VALIDATION -> Loss: 0.009720, MSE: 0.019472, RMSE: 0.139543, R²: 0.598114\n",
      "Iteration 32/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013872, MSE: 0.027745, RMSE: 0.166567, R²: 0.413957\n",
      "VALIDATION -> Loss: 0.009985, MSE: 0.020004, RMSE: 0.141436, R²: 0.587135\n",
      "Iteration 33/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013873, MSE: 0.027751, RMSE: 0.166585, R²: 0.413830\n",
      "VALIDATION -> Loss: 0.009822, MSE: 0.019679, RMSE: 0.140284, R²: 0.593836\n",
      "Iteration 34/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013807, MSE: 0.027616, RMSE: 0.166181, R²: 0.416669\n",
      "VALIDATION -> Loss: 0.009489, MSE: 0.019009, RMSE: 0.137875, R²: 0.607667\n",
      "Iteration 35/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013795, MSE: 0.027589, RMSE: 0.166098, R²: 0.417253\n",
      "VALIDATION -> Loss: 0.009589, MSE: 0.019211, RMSE: 0.138605, R²: 0.603497\n",
      "Iteration 36/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013762, MSE: 0.027519, RMSE: 0.165888, R²: 0.418724\n",
      "VALIDATION -> Loss: 0.009673, MSE: 0.019380, RMSE: 0.139213, R²: 0.600012\n",
      "Iteration 37/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013786, MSE: 0.027571, RMSE: 0.166045, R²: 0.417625\n",
      "VALIDATION -> Loss: 0.009721, MSE: 0.019476, RMSE: 0.139556, R²: 0.598038\n",
      "Iteration 38/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013752, MSE: 0.027512, RMSE: 0.165867, R²: 0.418874\n",
      "VALIDATION -> Loss: 0.009811, MSE: 0.019656, RMSE: 0.140199, R²: 0.594327\n",
      "Iteration 39/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013728, MSE: 0.027441, RMSE: 0.165653, R²: 0.420370\n",
      "VALIDATION -> Loss: 0.009565, MSE: 0.019164, RMSE: 0.138436, R²: 0.604466\n",
      "Iteration 40/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013656, MSE: 0.027313, RMSE: 0.165267, R²: 0.423074\n",
      "VALIDATION -> Loss: 0.009634, MSE: 0.019301, RMSE: 0.138927, R²: 0.601653\n",
      "Iteration 41/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013711, MSE: 0.027410, RMSE: 0.165560, R²: 0.421021\n",
      "VALIDATION -> Loss: 0.009774, MSE: 0.019584, RMSE: 0.139944, R²: 0.595803\n",
      "Iteration 42/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013665, MSE: 0.027331, RMSE: 0.165321, R²: 0.422694\n",
      "VALIDATION -> Loss: 0.009575, MSE: 0.019188, RMSE: 0.138519, R²: 0.603988\n",
      "Iteration 43/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013661, MSE: 0.027317, RMSE: 0.165280, R²: 0.422981\n",
      "VALIDATION -> Loss: 0.009625, MSE: 0.019284, RMSE: 0.138869, R²: 0.601989\n",
      "Iteration 44/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013618, MSE: 0.027242, RMSE: 0.165052, R²: 0.424569\n",
      "VALIDATION -> Loss: 0.009620, MSE: 0.019273, RMSE: 0.138827, R²: 0.602225\n",
      "Iteration 45/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013592, MSE: 0.027184, RMSE: 0.164875, R²: 0.425808\n",
      "VALIDATION -> Loss: 0.009733, MSE: 0.019499, RMSE: 0.139641, R²: 0.597551\n",
      "Iteration 46/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013590, MSE: 0.027179, RMSE: 0.164859, R²: 0.425914\n",
      "VALIDATION -> Loss: 0.009468, MSE: 0.018970, RMSE: 0.137730, R²: 0.608487\n",
      "Iteration 47/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013574, MSE: 0.027143, RMSE: 0.164751, R²: 0.426670\n",
      "VALIDATION -> Loss: 0.009569, MSE: 0.019171, RMSE: 0.138461, R²: 0.604323\n",
      "Iteration 48/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013538, MSE: 0.027063, RMSE: 0.164508, R²: 0.428358\n",
      "VALIDATION -> Loss: 0.009698, MSE: 0.019428, RMSE: 0.139383, R²: 0.599035\n",
      "Iteration 49/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013533, MSE: 0.027065, RMSE: 0.164516, R²: 0.428305\n",
      "VALIDATION -> Loss: 0.009912, MSE: 0.019857, RMSE: 0.140913, R²: 0.590181\n",
      "Iteration 50/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013552, MSE: 0.027102, RMSE: 0.164627, R²: 0.427531\n",
      "VALIDATION -> Loss: 0.009507, MSE: 0.019048, RMSE: 0.138014, R²: 0.606875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(50): \n",
    "    print(f\"Iteration {i+1}/50 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4452\n",
      "MAE: 0.1217\n",
      "MSE: 0.0263\n",
      "RMSE: 0.1621\n",
      "Precisión: 0.6199\n",
      "Recall: 0.9309\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SECOND MODEL**\n",
    "# Adding to the first model the data from TMBD \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb = pd.read_csv(\"ml-latest-small/movie_info_tmdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "df = tmdb.merge(links, on=\"tmdbId\", how=\"left\")\n",
    "df = df.merge(ratings, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df2 = df.copy()\n",
    "df2 = df2.drop(columns=[\"title\"])\n",
    "df2[\"release_date\"] = pd.to_datetime(df2[\"release_date\"], errors=\"coerce\").dt.year\n",
    "df2 = pd.get_dummies(df2, columns=[\"original_language\"], dtype=float)\n",
    "columns_to_convert = [col for col in df2.columns if col != \"title\"]\n",
    "df2[columns_to_convert] = df2[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "numerical_col = [col for col in df2.columns if col not in [\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61074, 58) (61074, 1)\n",
      "(20358, 58) (20358, 1)\n",
      "(20358, 58) (20358, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\"])\n",
    "df_shuffle = df_shuffle.dropna()\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.6), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.6):int(len(df_shuffle) * 0.8), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.8):, :]\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns if col not in [\"rating\", \"timestamp\", \"title\"]]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmdbId                False\n",
      "ratingTmdb            False\n",
      "release_date          False\n",
      "votes                 False\n",
      "budget                False\n",
      "                      ...  \n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "Length: 65, dtype: bool\n",
      "   tmdbId  ratingTmdb  release_date    votes      budget      revenue  \\\n",
      "0     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "1     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "2     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "3     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "4     862         8.0        1995.0  18705.0  30000000.0  394436586.0   \n",
      "\n",
      "   runtime  movieId  imdbId  userId  ...  original_language_sr  \\\n",
      "0     81.0        1  114709     1.0  ...                   0.0   \n",
      "1     81.0        1  114709     5.0  ...                   0.0   \n",
      "2     81.0        1  114709     7.0  ...                   0.0   \n",
      "3     81.0        1  114709    15.0  ...                   0.0   \n",
      "4     81.0        1  114709    17.0  ...                   0.0   \n",
      "\n",
      "   original_language_sv  original_language_ta  original_language_th  \\\n",
      "0                   0.0                   0.0                   0.0   \n",
      "1                   0.0                   0.0                   0.0   \n",
      "2                   0.0                   0.0                   0.0   \n",
      "3                   0.0                   0.0                   0.0   \n",
      "4                   0.0                   0.0                   0.0   \n",
      "\n",
      "   original_language_tn  original_language_tr  original_language_vi  \\\n",
      "0                   0.0                   0.0                   0.0   \n",
      "1                   0.0                   0.0                   0.0   \n",
      "2                   0.0                   0.0                   0.0   \n",
      "3                   0.0                   0.0                   0.0   \n",
      "4                   0.0                   0.0                   0.0   \n",
      "\n",
      "   original_language_wo  original_language_xx  original_language_zh  \n",
      "0                   0.0                   0.0                   0.0  \n",
      "1                   0.0                   0.0                   0.0  \n",
      "2                   0.0                   0.0                   0.0  \n",
      "3                   0.0                   0.0                   0.0  \n",
      "4                   0.0                   0.0                   0.0  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_train).any())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61074, 65)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_col = df_train.select_dtypes(include=['number']).columns\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.transform(df_val[numerical_col])  # Usar transform en validación\n",
    "df_test[numerical_col] = scaler.transform(df_test[numerical_col]) \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy())  \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.060176, MSE: 0.120752, RMSE: 0.347494, R²: -1.543586\n",
      "VALIDATION -> Loss: 0.018633, MSE: 0.037276, RMSE: 0.193070, R²: 0.209375\n",
      "Iteration 2/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.025117, MSE: 0.050271, RMSE: 0.224212, R²: -0.058942\n",
      "VALIDATION -> Loss: 0.013532, MSE: 0.027070, RMSE: 0.164529, R²: 0.425852\n",
      "Iteration 3/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020738, MSE: 0.041486, RMSE: 0.203680, R²: 0.126124\n",
      "VALIDATION -> Loss: 0.012881, MSE: 0.025767, RMSE: 0.160520, R²: 0.453488\n",
      "Iteration 4/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019653, MSE: 0.039307, RMSE: 0.198260, R²: 0.172011\n",
      "VALIDATION -> Loss: 0.012506, MSE: 0.025017, RMSE: 0.158168, R²: 0.469389\n",
      "Iteration 5/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018903, MSE: 0.037814, RMSE: 0.194458, R²: 0.203463\n",
      "VALIDATION -> Loss: 0.012372, MSE: 0.024750, RMSE: 0.157320, R²: 0.475060\n",
      "Iteration 6/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018423, MSE: 0.036836, RMSE: 0.191927, R²: 0.224066\n",
      "VALIDATION -> Loss: 0.011999, MSE: 0.024003, RMSE: 0.154930, R²: 0.490888\n",
      "Iteration 7/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017969, MSE: 0.035948, RMSE: 0.189598, R²: 0.242780\n",
      "VALIDATION -> Loss: 0.011987, MSE: 0.023980, RMSE: 0.154854, R²: 0.491390\n",
      "Iteration 8/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017630, MSE: 0.035275, RMSE: 0.187817, R²: 0.256941\n",
      "VALIDATION -> Loss: 0.012074, MSE: 0.024153, RMSE: 0.155413, R²: 0.487711\n",
      "Iteration 9/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017350, MSE: 0.034685, RMSE: 0.186238, R²: 0.269381\n",
      "VALIDATION -> Loss: 0.011954, MSE: 0.023916, RMSE: 0.154647, R²: 0.492746\n",
      "Iteration 10/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017147, MSE: 0.034294, RMSE: 0.185186, R²: 0.277615\n",
      "VALIDATION -> Loss: 0.012403, MSE: 0.024811, RMSE: 0.157517, R²: 0.473747\n",
      "Iteration 11/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017066, MSE: 0.034134, RMSE: 0.184753, R²: 0.280991\n",
      "VALIDATION -> Loss: 0.011824, MSE: 0.023654, RMSE: 0.153799, R²: 0.498297\n",
      "Iteration 12/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016918, MSE: 0.033874, RMSE: 0.184048, R²: 0.286469\n",
      "VALIDATION -> Loss: 0.011582, MSE: 0.023172, RMSE: 0.152222, R²: 0.508530\n",
      "Iteration 13/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016676, MSE: 0.033370, RMSE: 0.182675, R²: 0.297070\n",
      "VALIDATION -> Loss: 0.011523, MSE: 0.023052, RMSE: 0.151830, R²: 0.511060\n",
      "Iteration 14/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016500, MSE: 0.032994, RMSE: 0.181644, R²: 0.304988\n",
      "VALIDATION -> Loss: 0.011848, MSE: 0.023702, RMSE: 0.153955, R²: 0.497275\n",
      "Iteration 15/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016348, MSE: 0.032699, RMSE: 0.180828, R²: 0.311218\n",
      "VALIDATION -> Loss: 0.011544, MSE: 0.023093, RMSE: 0.151965, R²: 0.510187\n",
      "Iteration 16/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016240, MSE: 0.032474, RMSE: 0.180206, R²: 0.315946\n",
      "VALIDATION -> Loss: 0.011531, MSE: 0.023069, RMSE: 0.151885, R²: 0.510707\n",
      "Iteration 17/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016092, MSE: 0.032191, RMSE: 0.179420, R²: 0.321903\n",
      "VALIDATION -> Loss: 0.011554, MSE: 0.023115, RMSE: 0.152035, R²: 0.509740\n",
      "Iteration 18/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015927, MSE: 0.031862, RMSE: 0.178500, R²: 0.328837\n",
      "VALIDATION -> Loss: 0.011680, MSE: 0.023368, RMSE: 0.152864, R²: 0.504373\n",
      "Iteration 19/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015824, MSE: 0.031643, RMSE: 0.177884, R²: 0.333462\n",
      "VALIDATION -> Loss: 0.011357, MSE: 0.022721, RMSE: 0.150736, R²: 0.518081\n",
      "Iteration 20/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015614, MSE: 0.031240, RMSE: 0.176747, R²: 0.341951\n",
      "VALIDATION -> Loss: 0.011408, MSE: 0.022823, RMSE: 0.151074, R²: 0.515916\n",
      "Iteration 21/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015578, MSE: 0.031155, RMSE: 0.176508, R²: 0.343731\n",
      "VALIDATION -> Loss: 0.011333, MSE: 0.022674, RMSE: 0.150578, R²: 0.519090\n",
      "Iteration 22/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015682, MSE: 0.031374, RMSE: 0.177128, R²: 0.339113\n",
      "VALIDATION -> Loss: 0.011346, MSE: 0.022699, RMSE: 0.150661, R²: 0.518556\n",
      "Iteration 23/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015467, MSE: 0.030928, RMSE: 0.175863, R²: 0.348520\n",
      "VALIDATION -> Loss: 0.011355, MSE: 0.022718, RMSE: 0.150724, R²: 0.518157\n",
      "Iteration 24/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015276, MSE: 0.030541, RMSE: 0.174761, R²: 0.356659\n",
      "VALIDATION -> Loss: 0.011363, MSE: 0.022733, RMSE: 0.150773, R²: 0.517842\n",
      "Iteration 25/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015380, MSE: 0.030754, RMSE: 0.175368, R²: 0.352185\n",
      "VALIDATION -> Loss: 0.011420, MSE: 0.022848, RMSE: 0.151155, R²: 0.515394\n",
      "Iteration 26/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015134, MSE: 0.030276, RMSE: 0.174001, R²: 0.362239\n",
      "VALIDATION -> Loss: 0.011323, MSE: 0.022654, RMSE: 0.150513, R²: 0.519505\n",
      "Iteration 27/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015100, MSE: 0.030183, RMSE: 0.173733, R²: 0.364207\n",
      "VALIDATION -> Loss: 0.011354, MSE: 0.022715, RMSE: 0.150716, R²: 0.518206\n",
      "Iteration 28/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014963, MSE: 0.029939, RMSE: 0.173028, R²: 0.369354\n",
      "VALIDATION -> Loss: 0.011249, MSE: 0.022505, RMSE: 0.150017, R²: 0.522666\n",
      "Iteration 29/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014902, MSE: 0.029822, RMSE: 0.172690, R²: 0.371820\n",
      "VALIDATION -> Loss: 0.011288, MSE: 0.022583, RMSE: 0.150278, R²: 0.521004\n",
      "Iteration 30/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014915, MSE: 0.029822, RMSE: 0.172691, R²: 0.371809\n",
      "VALIDATION -> Loss: 0.011314, MSE: 0.022635, RMSE: 0.150450, R²: 0.519906\n",
      "Iteration 31/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014768, MSE: 0.029552, RMSE: 0.171906, R²: 0.377507\n",
      "VALIDATION -> Loss: 0.011291, MSE: 0.022591, RMSE: 0.150302, R²: 0.520848\n",
      "Iteration 32/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014704, MSE: 0.029400, RMSE: 0.171465, R²: 0.380699\n",
      "VALIDATION -> Loss: 0.011297, MSE: 0.022601, RMSE: 0.150336, R²: 0.520633\n",
      "Iteration 33/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014656, MSE: 0.029312, RMSE: 0.171207, R²: 0.382557\n",
      "VALIDATION -> Loss: 0.011344, MSE: 0.022697, RMSE: 0.150654, R²: 0.518605\n",
      "Iteration 34/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014585, MSE: 0.029155, RMSE: 0.170750, R²: 0.385854\n",
      "VALIDATION -> Loss: 0.011466, MSE: 0.022942, RMSE: 0.151465, R²: 0.513406\n",
      "Iteration 35/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014509, MSE: 0.029047, RMSE: 0.170432, R²: 0.388139\n",
      "VALIDATION -> Loss: 0.011293, MSE: 0.022593, RMSE: 0.150309, R²: 0.520806\n",
      "Iteration 36/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014451, MSE: 0.028904, RMSE: 0.170012, R²: 0.391150\n",
      "VALIDATION -> Loss: 0.011429, MSE: 0.022865, RMSE: 0.151213, R²: 0.515025\n",
      "Iteration 37/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014424, MSE: 0.028832, RMSE: 0.169801, R²: 0.392657\n",
      "VALIDATION -> Loss: 0.011310, MSE: 0.022627, RMSE: 0.150423, R²: 0.520078\n",
      "Iteration 38/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014352, MSE: 0.028715, RMSE: 0.169455, R²: 0.395130\n",
      "VALIDATION -> Loss: 0.011224, MSE: 0.022456, RMSE: 0.149854, R²: 0.523705\n",
      "Iteration 39/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014300, MSE: 0.028567, RMSE: 0.169017, R²: 0.398251\n",
      "VALIDATION -> Loss: 0.011252, MSE: 0.022512, RMSE: 0.150040, R²: 0.522518\n",
      "Iteration 40/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014309, MSE: 0.028632, RMSE: 0.169209, R²: 0.396887\n",
      "VALIDATION -> Loss: 0.011178, MSE: 0.022363, RMSE: 0.149542, R²: 0.525683\n",
      "Iteration 41/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014238, MSE: 0.028461, RMSE: 0.168704, R²: 0.400484\n",
      "VALIDATION -> Loss: 0.011318, MSE: 0.022644, RMSE: 0.150479, R²: 0.519723\n",
      "Iteration 42/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014150, MSE: 0.028289, RMSE: 0.168194, R²: 0.404103\n",
      "VALIDATION -> Loss: 0.011478, MSE: 0.022964, RMSE: 0.151538, R²: 0.512937\n",
      "Iteration 43/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014105, MSE: 0.028221, RMSE: 0.167990, R²: 0.405541\n",
      "VALIDATION -> Loss: 0.011307, MSE: 0.022622, RMSE: 0.150406, R²: 0.520188\n",
      "Iteration 44/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013987, MSE: 0.027975, RMSE: 0.167257, R²: 0.410724\n",
      "VALIDATION -> Loss: 0.011207, MSE: 0.022422, RMSE: 0.149739, R²: 0.524434\n",
      "Iteration 45/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014019, MSE: 0.028040, RMSE: 0.167450, R²: 0.409358\n",
      "VALIDATION -> Loss: 0.011260, MSE: 0.022526, RMSE: 0.150087, R²: 0.522218\n",
      "Iteration 46/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013958, MSE: 0.027914, RMSE: 0.167075, R²: 0.411999\n",
      "VALIDATION -> Loss: 0.011326, MSE: 0.022660, RMSE: 0.150533, R²: 0.519374\n",
      "Iteration 47/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013845, MSE: 0.027694, RMSE: 0.166415, R²: 0.416638\n",
      "VALIDATION -> Loss: 0.011256, MSE: 0.022521, RMSE: 0.150072, R²: 0.522318\n",
      "Iteration 48/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013908, MSE: 0.027821, RMSE: 0.166797, R²: 0.413956\n",
      "VALIDATION -> Loss: 0.011237, MSE: 0.022481, RMSE: 0.149938, R²: 0.523169\n",
      "Iteration 49/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013811, MSE: 0.027621, RMSE: 0.166195, R²: 0.418182\n",
      "VALIDATION -> Loss: 0.011364, MSE: 0.022737, RMSE: 0.150787, R²: 0.517751\n",
      "Iteration 50/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013816, MSE: 0.027633, RMSE: 0.166230, R²: 0.417932\n",
      "VALIDATION -> Loss: 0.011172, MSE: 0.022351, RMSE: 0.149501, R²: 0.525944\n",
      "Iteration 51/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013766, MSE: 0.027545, RMSE: 0.165966, R²: 0.419786\n",
      "VALIDATION -> Loss: 0.011238, MSE: 0.022485, RMSE: 0.149951, R²: 0.523084\n",
      "Iteration 52/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013871, MSE: 0.027734, RMSE: 0.166536, R²: 0.415787\n",
      "VALIDATION -> Loss: 0.011217, MSE: 0.022442, RMSE: 0.149807, R²: 0.524002\n",
      "Iteration 53/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013774, MSE: 0.027550, RMSE: 0.165983, R²: 0.419665\n",
      "VALIDATION -> Loss: 0.011173, MSE: 0.022355, RMSE: 0.149516, R²: 0.525850\n",
      "Iteration 54/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013711, MSE: 0.027433, RMSE: 0.165628, R²: 0.422142\n",
      "VALIDATION -> Loss: 0.011313, MSE: 0.022634, RMSE: 0.150446, R²: 0.519929\n",
      "Iteration 55/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013631, MSE: 0.027276, RMSE: 0.165153, R²: 0.425450\n",
      "VALIDATION -> Loss: 0.011186, MSE: 0.022380, RMSE: 0.149599, R²: 0.525324\n",
      "Iteration 56/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013682, MSE: 0.027380, RMSE: 0.165468, R²: 0.423261\n",
      "VALIDATION -> Loss: 0.011277, MSE: 0.022564, RMSE: 0.150212, R²: 0.521422\n",
      "Iteration 57/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013607, MSE: 0.027214, RMSE: 0.164966, R²: 0.426752\n",
      "VALIDATION -> Loss: 0.011299, MSE: 0.022607, RMSE: 0.150357, R²: 0.520500\n",
      "Iteration 58/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013641, MSE: 0.027293, RMSE: 0.165205, R²: 0.425094\n",
      "VALIDATION -> Loss: 0.011228, MSE: 0.022466, RMSE: 0.149886, R²: 0.523500\n",
      "Iteration 59/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013575, MSE: 0.027148, RMSE: 0.164766, R²: 0.428143\n",
      "VALIDATION -> Loss: 0.011337, MSE: 0.022683, RMSE: 0.150608, R²: 0.518896\n",
      "Iteration 60/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013531, MSE: 0.027073, RMSE: 0.164539, R²: 0.429718\n",
      "VALIDATION -> Loss: 0.011269, MSE: 0.022547, RMSE: 0.150155, R²: 0.521784\n",
      "Iteration 61/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013560, MSE: 0.027113, RMSE: 0.164660, R²: 0.428875\n",
      "VALIDATION -> Loss: 0.011310, MSE: 0.022629, RMSE: 0.150430, R²: 0.520035\n",
      "Iteration 62/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013543, MSE: 0.027091, RMSE: 0.164594, R²: 0.429334\n",
      "VALIDATION -> Loss: 0.011159, MSE: 0.022327, RMSE: 0.149421, R²: 0.526451\n",
      "Iteration 63/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013471, MSE: 0.026939, RMSE: 0.164130, R²: 0.432549\n",
      "VALIDATION -> Loss: 0.011334, MSE: 0.022676, RMSE: 0.150584, R²: 0.519049\n",
      "Iteration 64/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013486, MSE: 0.026974, RMSE: 0.164239, R²: 0.431798\n",
      "VALIDATION -> Loss: 0.011235, MSE: 0.022478, RMSE: 0.149928, R²: 0.523230\n",
      "Iteration 65/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013471, MSE: 0.026922, RMSE: 0.164078, R²: 0.432907\n",
      "VALIDATION -> Loss: 0.011275, MSE: 0.022558, RMSE: 0.150193, R²: 0.521543\n",
      "Iteration 66/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013484, MSE: 0.026964, RMSE: 0.164209, R²: 0.432006\n",
      "VALIDATION -> Loss: 0.011137, MSE: 0.022282, RMSE: 0.149273, R²: 0.527390\n",
      "Iteration 67/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013425, MSE: 0.026863, RMSE: 0.163900, R²: 0.434139\n",
      "VALIDATION -> Loss: 0.011183, MSE: 0.022374, RMSE: 0.149580, R²: 0.525443\n",
      "Iteration 68/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013414, MSE: 0.026839, RMSE: 0.163826, R²: 0.434650\n",
      "VALIDATION -> Loss: 0.011236, MSE: 0.022480, RMSE: 0.149932, R²: 0.523208\n",
      "Iteration 69/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013459, MSE: 0.026926, RMSE: 0.164093, R²: 0.432808\n",
      "VALIDATION -> Loss: 0.011256, MSE: 0.022519, RMSE: 0.150063, R²: 0.522372\n",
      "Iteration 70/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013390, MSE: 0.026782, RMSE: 0.163653, R²: 0.435846\n",
      "VALIDATION -> Loss: 0.011233, MSE: 0.022474, RMSE: 0.149915, R²: 0.523318\n",
      "Iteration 71/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013408, MSE: 0.026816, RMSE: 0.163755, R²: 0.435137\n",
      "VALIDATION -> Loss: 0.011223, MSE: 0.022455, RMSE: 0.149851, R²: 0.523723\n",
      "Iteration 72/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013402, MSE: 0.026791, RMSE: 0.163681, R²: 0.435649\n",
      "VALIDATION -> Loss: 0.011336, MSE: 0.022680, RMSE: 0.150599, R²: 0.518953\n",
      "Iteration 73/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013424, MSE: 0.026863, RMSE: 0.163898, R²: 0.434149\n",
      "VALIDATION -> Loss: 0.011239, MSE: 0.022485, RMSE: 0.149950, R²: 0.523095\n",
      "Iteration 74/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013392, MSE: 0.026787, RMSE: 0.163666, R²: 0.435753\n",
      "VALIDATION -> Loss: 0.011294, MSE: 0.022596, RMSE: 0.150319, R²: 0.520745\n",
      "Iteration 75/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013333, MSE: 0.026666, RMSE: 0.163297, R²: 0.438293\n",
      "VALIDATION -> Loss: 0.011281, MSE: 0.022571, RMSE: 0.150237, R²: 0.521263\n",
      "Iteration 76/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013359, MSE: 0.026715, RMSE: 0.163447, R²: 0.437265\n",
      "VALIDATION -> Loss: 0.011159, MSE: 0.022327, RMSE: 0.149423, R²: 0.526440\n",
      "Iteration 77/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013298, MSE: 0.026598, RMSE: 0.163089, R²: 0.439722\n",
      "VALIDATION -> Loss: 0.011196, MSE: 0.022402, RMSE: 0.149671, R²: 0.524862\n",
      "Iteration 78/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013383, MSE: 0.026753, RMSE: 0.163563, R²: 0.436466\n",
      "VALIDATION -> Loss: 0.011349, MSE: 0.022707, RMSE: 0.150689, R²: 0.518377\n",
      "Iteration 79/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013350, MSE: 0.026705, RMSE: 0.163417, R²: 0.437469\n",
      "VALIDATION -> Loss: 0.011316, MSE: 0.022639, RMSE: 0.150462, R²: 0.519827\n",
      "Iteration 80/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013332, MSE: 0.026664, RMSE: 0.163292, R²: 0.438330\n",
      "VALIDATION -> Loss: 0.011241, MSE: 0.022489, RMSE: 0.149963, R²: 0.523011\n",
      "Iteration 81/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013308, MSE: 0.026601, RMSE: 0.163097, R²: 0.439669\n",
      "VALIDATION -> Loss: 0.011298, MSE: 0.022605, RMSE: 0.150349, R²: 0.520554\n",
      "Iteration 82/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013394, MSE: 0.026772, RMSE: 0.163623, R²: 0.436052\n",
      "VALIDATION -> Loss: 0.011212, MSE: 0.022434, RMSE: 0.149779, R²: 0.524177\n",
      "Iteration 83/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013228, MSE: 0.026460, RMSE: 0.162666, R²: 0.442628\n",
      "VALIDATION -> Loss: 0.011273, MSE: 0.022554, RMSE: 0.150179, R²: 0.521635\n",
      "Iteration 84/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013282, MSE: 0.026557, RMSE: 0.162965, R²: 0.440579\n",
      "VALIDATION -> Loss: 0.011326, MSE: 0.022659, RMSE: 0.150528, R²: 0.519408\n",
      "Iteration 85/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013232, MSE: 0.026458, RMSE: 0.162659, R²: 0.442677\n",
      "VALIDATION -> Loss: 0.011214, MSE: 0.022435, RMSE: 0.149783, R²: 0.524155\n",
      "Iteration 86/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013270, MSE: 0.026532, RMSE: 0.162886, R²: 0.441120\n",
      "VALIDATION -> Loss: 0.011249, MSE: 0.022506, RMSE: 0.150021, R²: 0.522642\n",
      "Iteration 87/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013247, MSE: 0.026481, RMSE: 0.162729, R²: 0.442198\n",
      "VALIDATION -> Loss: 0.011334, MSE: 0.022675, RMSE: 0.150581, R²: 0.519071\n",
      "Iteration 88/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013274, MSE: 0.026547, RMSE: 0.162931, R²: 0.440806\n",
      "VALIDATION -> Loss: 0.011210, MSE: 0.022428, RMSE: 0.149760, R²: 0.524301\n",
      "Iteration 89/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013268, MSE: 0.026532, RMSE: 0.162888, R²: 0.441108\n",
      "VALIDATION -> Loss: 0.011252, MSE: 0.022511, RMSE: 0.150038, R²: 0.522535\n",
      "Iteration 90/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013243, MSE: 0.026495, RMSE: 0.162773, R²: 0.441895\n",
      "VALIDATION -> Loss: 0.011249, MSE: 0.022505, RMSE: 0.150017, R²: 0.522668\n",
      "Iteration 91/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013313, MSE: 0.026628, RMSE: 0.163181, R²: 0.439094\n",
      "VALIDATION -> Loss: 0.011290, MSE: 0.022586, RMSE: 0.150285, R²: 0.520959\n",
      "Iteration 92/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013220, MSE: 0.026447, RMSE: 0.162626, R²: 0.442901\n",
      "VALIDATION -> Loss: 0.011358, MSE: 0.022725, RMSE: 0.150747, R²: 0.518011\n",
      "Iteration 93/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013196, MSE: 0.026377, RMSE: 0.162410, R²: 0.444379\n",
      "VALIDATION -> Loss: 0.011249, MSE: 0.022504, RMSE: 0.150013, R²: 0.522690\n",
      "Iteration 94/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013222, MSE: 0.026451, RMSE: 0.162636, R²: 0.442830\n",
      "VALIDATION -> Loss: 0.011255, MSE: 0.022518, RMSE: 0.150061, R²: 0.522387\n",
      "Iteration 95/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013217, MSE: 0.026438, RMSE: 0.162598, R²: 0.443093\n",
      "VALIDATION -> Loss: 0.011500, MSE: 0.023007, RMSE: 0.151681, R²: 0.512018\n",
      "Iteration 96/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013180, MSE: 0.026369, RMSE: 0.162384, R²: 0.444555\n",
      "VALIDATION -> Loss: 0.011214, MSE: 0.022436, RMSE: 0.149785, R²: 0.524139\n",
      "Iteration 97/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013206, MSE: 0.026415, RMSE: 0.162526, R²: 0.443584\n",
      "VALIDATION -> Loss: 0.011344, MSE: 0.022697, RMSE: 0.150655, R²: 0.518598\n",
      "Iteration 98/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013181, MSE: 0.026361, RMSE: 0.162360, R²: 0.444724\n",
      "VALIDATION -> Loss: 0.011229, MSE: 0.022465, RMSE: 0.149885, R²: 0.523506\n",
      "Iteration 99/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013214, MSE: 0.026427, RMSE: 0.162563, R²: 0.443333\n",
      "VALIDATION -> Loss: 0.011270, MSE: 0.022547, RMSE: 0.150156, R²: 0.521783\n",
      "Iteration 100/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013136, MSE: 0.026299, RMSE: 0.162169, R²: 0.446025\n",
      "VALIDATION -> Loss: 0.011356, MSE: 0.022719, RMSE: 0.150729, R²: 0.518122\n"
     ]
    }
   ],
   "source": [
    "for i in range(100): \n",
    "    print(f\"Iteration {i+1}/50 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4637\n",
      "MAE: 0.1192\n",
      "MSE: 0.0255\n",
      "RMSE: 0.1596\n",
      "Precisión: 0.6020\n",
      "Recall: 0.9477\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THIRD MODEL**\n",
    "# Adding to the first model the data from IMDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(\"ml-latest-small/movie_info_imdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "imdb['imdbId'] = imdb['imdbId'].str.replace('tt', '', regex=False).astype(int)\n",
    "columns_to_drop = ['Actors', 'Awards', 'DVD', 'Director', 'Genre', 'Title', 'Type', 'Website', 'Year', 'Poster', 'Production', 'Rated', 'Plot', 'Writer', 'Response', 'Ratings']\n",
    "imdb = imdb.drop(columns=columns_to_drop)\n",
    "\n",
    "imdb = imdb.astype({\n",
    "    \"BoxOffice\": \"string\",  \n",
    "    \"Country\": \"string\",\n",
    "    \"Language\": \"string\",\n",
    "    \"Metascore\": \"float\",\n",
    "    \"Released\": \"string\",\n",
    "    \"Runtime\": \"string\",\n",
    "    \"imdbRating\": \"float\",\n",
    "    \"imdbVotes\": \"string\"\n",
    "})\n",
    "\n",
    "imdb['BoxOffice'] = imdb['BoxOffice'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "imdb['Released'] = imdb['Released'].str.extract(r'(\\d{4})').astype(float)\n",
    "imdb['Runtime'] = imdb['Runtime'].str.extract(r'(\\d+)').astype(float)\n",
    "imdb['imdbVotes'] = imdb['imdbVotes'].str.replace(',', '', regex=True).astype(float)\n",
    "\n",
    "imdb = imdb.merge(links, on=\"imdbId\", how=\"left\")\n",
    "imdb = imdb.merge(ratings, on=\"movieId\", how=\"left\")\n",
    "\n",
    "imdb = imdb.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Language', 'Metascore', 'Released', 'Runtime',\n",
      "       'imdbRating', 'imdbVotes', 'movieId', 'tmdbId',\n",
      "       ...\n",
      "       'Country_United States, United Kingdom, Switzerland, Panama',\n",
      "       'Country_United States, Venezuela',\n",
      "       'Country_United States, Vietnam, United Kingdom, Canada, Denmark',\n",
      "       'Country_United States, West Germany', 'Country_West Germany',\n",
      "       'Country_West Germany, France',\n",
      "       'Country_West Germany, France, United Kingdom',\n",
      "       'Country_West Germany, Italy, France',\n",
      "       'Country_West Germany, United States',\n",
      "       'Country_Yugoslavia, United States'],\n",
      "      dtype='object', length=931)\n"
     ]
    }
   ],
   "source": [
    "df2 = imdb\n",
    "df2 = pd.get_dummies(df2, columns=[\"Country\"], dtype=float)\n",
    "\n",
    "print(df2.columns)\n",
    "df2[\"Language\"] = df2[\"Language\"].str.split(\",\")\n",
    "moviesExploded = df2.explode(\"Language\")\n",
    "moviesExploded[\"Language\"] = moviesExploded[\"Language\"].str.strip()\n",
    "movies_dummies = pd.get_dummies(moviesExploded[\"Language\"], dtype=int)\n",
    "movies_dummies = moviesExploded[[\"movieId\"]].join(movies_dummies).groupby(\"movieId\").max()\n",
    "df2 = df2.drop(columns=[\"Language\"]).merge(movies_dummies, on=\"movieId\")\n",
    "\n",
    "columns_to_convert = [col for col in df2.columns]\n",
    "df2[columns_to_convert] = df2[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1087) (54776, 1)\n",
      "(18259, 1087) (18259, 1)\n",
      "(18259, 1087) (18259, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\"])\n",
    "df_shuffle = df_shuffle.dropna()\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.6), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.6):int(len(df_shuffle) * 0.8), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.8):, :]\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbId                False\n",
      "BoxOffice             False\n",
      "Metascore             False\n",
      "Released              False\n",
      "Runtime               False\n",
      "                      ...  \n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "Length: 1093, dtype: bool\n",
      "   imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  imdbVotes  \\\n",
      "0  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "1  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "2  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "3  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "4  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "\n",
      "   movieId  tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  Washoe  Welsh  \\\n",
      "0      1.0   862.0     1.0  ...         0     0           0       0      0   \n",
      "1      1.0   862.0     5.0  ...         0     0           0       0      0   \n",
      "2      1.0   862.0     7.0  ...         0     0           0       0      0   \n",
      "3      1.0   862.0    15.0  ...         0     0           0       0      0   \n",
      "4      1.0   862.0    17.0  ...         0     0           0       0      0   \n",
      "\n",
      "   Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0      0      0        0       0     0  \n",
      "1      0      0        0       0     0  \n",
      "2      0      0        0       0     0  \n",
      "3      0      0        0       0     0  \n",
      "4      0      0        0       0     0  \n",
      "\n",
      "[5 rows x 1088 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_train).any())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1093)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_col = df_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.transform(df_val[numerical_col])  \n",
    "df_test[numerical_col] = scaler.transform(df_test[numerical_col]) \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1092, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy())  \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.049663, MSE: 0.099712, RMSE: 0.315772, R²: -1.120909\n",
      "VALIDATION -> Loss: 0.019124, MSE: 0.038265, RMSE: 0.195613, R²: 0.188712\n",
      "Iteration 2/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.025902, MSE: 0.051784, RMSE: 0.227561, R²: -0.101470\n",
      "VALIDATION -> Loss: 0.015297, MSE: 0.030611, RMSE: 0.174961, R²: 0.350976\n",
      "Iteration 3/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.023384, MSE: 0.046781, RMSE: 0.216290, R²: 0.004945\n",
      "VALIDATION -> Loss: 0.015854, MSE: 0.031722, RMSE: 0.178108, R²: 0.327418\n",
      "Iteration 4/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.022214, MSE: 0.044449, RMSE: 0.210830, R²: 0.054544\n",
      "VALIDATION -> Loss: 0.014447, MSE: 0.028911, RMSE: 0.170034, R²: 0.387019\n",
      "Iteration 5/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.021357, MSE: 0.042712, RMSE: 0.206669, R²: 0.091497\n",
      "VALIDATION -> Loss: 0.013339, MSE: 0.026689, RMSE: 0.163367, R²: 0.434143\n",
      "Iteration 6/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020850, MSE: 0.041648, RMSE: 0.204078, R²: 0.114139\n",
      "VALIDATION -> Loss: 0.013352, MSE: 0.026714, RMSE: 0.163445, R²: 0.433605\n",
      "Iteration 7/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020197, MSE: 0.040388, RMSE: 0.200967, R²: 0.140935\n",
      "VALIDATION -> Loss: 0.013392, MSE: 0.026794, RMSE: 0.163688, R²: 0.431915\n",
      "Iteration 8/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019646, MSE: 0.039295, RMSE: 0.198230, R²: 0.164178\n",
      "VALIDATION -> Loss: 0.013354, MSE: 0.026719, RMSE: 0.163458, R²: 0.433510\n",
      "Iteration 9/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019584, MSE: 0.039184, RMSE: 0.197948, R²: 0.166551\n",
      "VALIDATION -> Loss: 0.013105, MSE: 0.026222, RMSE: 0.161931, R²: 0.444045\n",
      "Iteration 10/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018943, MSE: 0.037891, RMSE: 0.194656, R²: 0.194042\n",
      "VALIDATION -> Loss: 0.013121, MSE: 0.026252, RMSE: 0.162025, R²: 0.443403\n",
      "Iteration 11/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018527, MSE: 0.037052, RMSE: 0.192490, R²: 0.211883\n",
      "VALIDATION -> Loss: 0.013453, MSE: 0.026915, RMSE: 0.164059, R²: 0.429340\n",
      "Iteration 12/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018048, MSE: 0.036093, RMSE: 0.189981, R²: 0.232296\n",
      "VALIDATION -> Loss: 0.013027, MSE: 0.026065, RMSE: 0.161446, R²: 0.447375\n",
      "Iteration 13/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017829, MSE: 0.035661, RMSE: 0.188842, R²: 0.241472\n",
      "VALIDATION -> Loss: 0.013058, MSE: 0.026129, RMSE: 0.161644, R²: 0.446018\n",
      "Iteration 14/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017479, MSE: 0.034945, RMSE: 0.186936, R²: 0.256703\n",
      "VALIDATION -> Loss: 0.013036, MSE: 0.026079, RMSE: 0.161489, R²: 0.447075\n",
      "Iteration 15/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017234, MSE: 0.034493, RMSE: 0.185724, R²: 0.266311\n",
      "VALIDATION -> Loss: 0.012903, MSE: 0.025815, RMSE: 0.160671, R²: 0.452669\n",
      "Iteration 16/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016894, MSE: 0.033751, RMSE: 0.183713, R²: 0.282114\n",
      "VALIDATION -> Loss: 0.012838, MSE: 0.025682, RMSE: 0.160257, R²: 0.455485\n",
      "Iteration 17/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016675, MSE: 0.033372, RMSE: 0.182679, R²: 0.290173\n",
      "VALIDATION -> Loss: 0.013294, MSE: 0.026595, RMSE: 0.163080, R²: 0.436128\n",
      "Iteration 18/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016340, MSE: 0.032688, RMSE: 0.180798, R²: 0.304714\n",
      "VALIDATION -> Loss: 0.013311, MSE: 0.026623, RMSE: 0.163166, R²: 0.435535\n",
      "Iteration 19/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016143, MSE: 0.032291, RMSE: 0.179696, R²: 0.313165\n",
      "VALIDATION -> Loss: 0.013222, MSE: 0.026446, RMSE: 0.162621, R²: 0.439300\n",
      "Iteration 20/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015935, MSE: 0.031902, RMSE: 0.178612, R²: 0.321426\n",
      "VALIDATION -> Loss: 0.013491, MSE: 0.026989, RMSE: 0.164284, R²: 0.427770\n",
      "Iteration 21/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015601, MSE: 0.031209, RMSE: 0.176662, R²: 0.336166\n",
      "VALIDATION -> Loss: 0.013216, MSE: 0.026441, RMSE: 0.162606, R²: 0.439402\n",
      "Iteration 22/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015584, MSE: 0.031154, RMSE: 0.176504, R²: 0.337351\n",
      "VALIDATION -> Loss: 0.013058, MSE: 0.026121, RMSE: 0.161619, R²: 0.446185\n",
      "Iteration 23/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015278, MSE: 0.030557, RMSE: 0.174807, R²: 0.350033\n",
      "VALIDATION -> Loss: 0.012944, MSE: 0.025892, RMSE: 0.160910, R²: 0.451034\n",
      "Iteration 24/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015205, MSE: 0.030445, RMSE: 0.174484, R²: 0.352433\n",
      "VALIDATION -> Loss: 0.012851, MSE: 0.025715, RMSE: 0.160358, R²: 0.454799\n",
      "Iteration 25/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015120, MSE: 0.030211, RMSE: 0.173813, R²: 0.357403\n",
      "VALIDATION -> Loss: 0.012853, MSE: 0.025711, RMSE: 0.160346, R²: 0.454877\n",
      "Iteration 26/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014937, MSE: 0.029868, RMSE: 0.172823, R²: 0.364704\n",
      "VALIDATION -> Loss: 0.012833, MSE: 0.025670, RMSE: 0.160218, R²: 0.455748\n",
      "Iteration 27/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014784, MSE: 0.029593, RMSE: 0.172027, R²: 0.370542\n",
      "VALIDATION -> Loss: 0.013124, MSE: 0.026257, RMSE: 0.162041, R²: 0.443295\n",
      "Iteration 28/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014597, MSE: 0.029171, RMSE: 0.170795, R²: 0.379526\n",
      "VALIDATION -> Loss: 0.012907, MSE: 0.025828, RMSE: 0.160712, R²: 0.452383\n",
      "Iteration 29/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014546, MSE: 0.029096, RMSE: 0.170575, R²: 0.381119\n",
      "VALIDATION -> Loss: 0.012772, MSE: 0.025555, RMSE: 0.159860, R²: 0.458179\n",
      "Iteration 30/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014390, MSE: 0.028792, RMSE: 0.169681, R²: 0.387591\n",
      "VALIDATION -> Loss: 0.012964, MSE: 0.025931, RMSE: 0.161032, R²: 0.450205\n",
      "Iteration 31/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014231, MSE: 0.028471, RMSE: 0.168734, R²: 0.394405\n",
      "VALIDATION -> Loss: 0.012906, MSE: 0.025821, RMSE: 0.160688, R²: 0.452547\n",
      "Iteration 32/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014234, MSE: 0.028445, RMSE: 0.168657, R²: 0.394963\n",
      "VALIDATION -> Loss: 0.013018, MSE: 0.026040, RMSE: 0.161369, R²: 0.447898\n",
      "Iteration 33/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014082, MSE: 0.028171, RMSE: 0.167843, R²: 0.400788\n",
      "VALIDATION -> Loss: 0.013254, MSE: 0.026516, RMSE: 0.162836, R²: 0.437814\n",
      "Iteration 34/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013982, MSE: 0.027980, RMSE: 0.167273, R²: 0.404847\n",
      "VALIDATION -> Loss: 0.012747, MSE: 0.025500, RMSE: 0.159688, R²: 0.459346\n",
      "Iteration 35/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013924, MSE: 0.027844, RMSE: 0.166866, R²: 0.407740\n",
      "VALIDATION -> Loss: 0.012903, MSE: 0.025818, RMSE: 0.160681, R²: 0.452599\n",
      "Iteration 36/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013819, MSE: 0.027635, RMSE: 0.166236, R²: 0.412203\n",
      "VALIDATION -> Loss: 0.012826, MSE: 0.025660, RMSE: 0.160189, R²: 0.455947\n",
      "Iteration 37/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013714, MSE: 0.027454, RMSE: 0.165693, R²: 0.416037\n",
      "VALIDATION -> Loss: 0.012844, MSE: 0.025698, RMSE: 0.160305, R²: 0.455157\n",
      "Iteration 38/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013642, MSE: 0.027279, RMSE: 0.165163, R²: 0.419768\n",
      "VALIDATION -> Loss: 0.012820, MSE: 0.025648, RMSE: 0.160151, R²: 0.456200\n",
      "Iteration 39/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013577, MSE: 0.027179, RMSE: 0.164860, R²: 0.421899\n",
      "VALIDATION -> Loss: 0.012748, MSE: 0.025504, RMSE: 0.159700, R²: 0.459262\n",
      "Iteration 40/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013539, MSE: 0.027067, RMSE: 0.164522, R²: 0.424267\n",
      "VALIDATION -> Loss: 0.012833, MSE: 0.025680, RMSE: 0.160250, R²: 0.455533\n",
      "Iteration 41/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013447, MSE: 0.026874, RMSE: 0.163934, R²: 0.428375\n",
      "VALIDATION -> Loss: 0.012730, MSE: 0.025469, RMSE: 0.159589, R²: 0.460010\n",
      "Iteration 42/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013364, MSE: 0.026736, RMSE: 0.163512, R²: 0.431312\n",
      "VALIDATION -> Loss: 0.012941, MSE: 0.025891, RMSE: 0.160906, R²: 0.451063\n",
      "Iteration 43/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013384, MSE: 0.026782, RMSE: 0.163652, R²: 0.430339\n",
      "VALIDATION -> Loss: 0.012869, MSE: 0.025743, RMSE: 0.160445, R²: 0.454203\n",
      "Iteration 44/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013288, MSE: 0.026602, RMSE: 0.163100, R²: 0.434174\n",
      "VALIDATION -> Loss: 0.012875, MSE: 0.025754, RMSE: 0.160481, R²: 0.453957\n",
      "Iteration 45/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013235, MSE: 0.026493, RMSE: 0.162766, R²: 0.436487\n",
      "VALIDATION -> Loss: 0.012915, MSE: 0.025839, RMSE: 0.160744, R²: 0.452166\n",
      "Iteration 46/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013179, MSE: 0.026389, RMSE: 0.162447, R²: 0.438693\n",
      "VALIDATION -> Loss: 0.012949, MSE: 0.025905, RMSE: 0.160949, R²: 0.450771\n",
      "Iteration 47/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013109, MSE: 0.026190, RMSE: 0.161835, R²: 0.442920\n",
      "VALIDATION -> Loss: 0.012815, MSE: 0.025636, RMSE: 0.160113, R²: 0.456459\n",
      "Iteration 48/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013130, MSE: 0.026239, RMSE: 0.161984, R²: 0.441891\n",
      "VALIDATION -> Loss: 0.012958, MSE: 0.025921, RMSE: 0.161000, R²: 0.450423\n",
      "Iteration 49/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013040, MSE: 0.026053, RMSE: 0.161408, R²: 0.445851\n",
      "VALIDATION -> Loss: 0.012915, MSE: 0.025837, RMSE: 0.160739, R²: 0.452204\n",
      "Iteration 50/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013017, MSE: 0.026047, RMSE: 0.161392, R²: 0.445964\n",
      "VALIDATION -> Loss: 0.013093, MSE: 0.026193, RMSE: 0.161841, R²: 0.444665\n"
     ]
    }
   ],
   "source": [
    "for i in range(50): \n",
    "    print(f\"Iteration {i+1}/50 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4849\n",
      "MAE: 0.1184\n",
      "MSE: 0.0242\n",
      "RMSE: 0.1556\n",
      "Precisión: 0.6555\n",
      "Recall: 0.9182\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FOURTH MODEL**\n",
    "# Adding to the first model the data from IMDB and TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Country', 'Language', 'Metascore', 'Released',\n",
      "       'Runtime', 'imdbRating', 'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       'rating', 'timestamp'],\n",
      "      dtype='object')\n",
      "<bound method NDFrame.describe of          imdbId    BoxOffice                               Country  \\\n",
      "0        114709  223225679.0                         United States   \n",
      "1        114709  223225679.0                         United States   \n",
      "2        114709  223225679.0                         United States   \n",
      "3        114709  223225679.0                         United States   \n",
      "4        114709  223225679.0                         United States   \n",
      "...         ...          ...                                   ...   \n",
      "100799  4912910  220159104.0  United States, China, France, Norway   \n",
      "100800  4912910  220159104.0  United States, China, France, Norway   \n",
      "100801  7690670   20545116.0                         United States   \n",
      "100803  7349662   49275340.0                  United States, China   \n",
      "100804  4073790   12695691.0                         United States   \n",
      "\n",
      "                           Language  Metascore  Released  Runtime  imdbRating  \\\n",
      "0                           English       96.0    1995.0     81.0         8.3   \n",
      "1                           English       96.0    1995.0     81.0         8.3   \n",
      "2                           English       96.0    1995.0     81.0         8.3   \n",
      "3                           English       96.0    1995.0     81.0         8.3   \n",
      "4                           English       96.0    1995.0     81.0         8.3   \n",
      "...                             ...        ...       ...      ...         ...   \n",
      "100799              English, French       87.0    2018.0    147.0         7.7   \n",
      "100800              English, French       87.0    2018.0    147.0         7.7   \n",
      "100801  English, Spanish, Cantonese       52.0    2018.0    116.0         5.3   \n",
      "100803                      English       83.0    2018.0    135.0         7.5   \n",
      "100804                      English       39.0    2018.0    104.0         5.7   \n",
      "\n",
      "        imdbVotes   movieId    tmdbId  userId  rating     timestamp  \n",
      "0       1112586.0       1.0     862.0     1.0     4.0  9.649827e+08  \n",
      "1       1112586.0       1.0     862.0     5.0     4.0  8.474350e+08  \n",
      "2       1112586.0       1.0     862.0     7.0     4.0  1.106636e+09  \n",
      "3       1112586.0       1.0     862.0    15.0     2.0  1.510578e+09  \n",
      "4       1112586.0       1.0     862.0    17.0     4.0  1.305696e+09  \n",
      "...           ...       ...       ...     ...     ...           ...  \n",
      "100799   391467.0  189333.0  353081.0   184.0     4.0  1.537110e+09  \n",
      "100800   391467.0  189333.0  353081.0   248.0     3.0  1.534602e+09  \n",
      "100801     7799.0  189381.0  500475.0   318.0     2.0  1.536098e+09  \n",
      "100803   300208.0  189713.0  487558.0   462.0     2.0  1.536467e+09  \n",
      "100804    38486.0  190183.0  445651.0    50.0     3.0  1.533302e+09  \n",
      "\n",
      "[91294 rows x 14 columns]>\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv(\"ml-latest-small/movie_info_imdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "imdb['imdbId'] = imdb['imdbId'].str.replace('tt', '', regex=False).astype(int)\n",
    "columns_to_drop = ['Actors', 'Awards', 'DVD', 'Director', 'Genre', 'Title', 'Type', 'Website', 'Year', 'Poster', 'Production', 'Rated', 'Plot', 'Writer', 'Response', 'Ratings']\n",
    "imdb = imdb.drop(columns=columns_to_drop)\n",
    "\n",
    "imdb = imdb.astype({\n",
    "    \"BoxOffice\": \"string\",  \n",
    "    \"Country\": \"string\",\n",
    "    \"Language\": \"string\",\n",
    "    \"Metascore\": \"float\",\n",
    "    \"Released\": \"string\",\n",
    "    \"Runtime\": \"string\",\n",
    "    \"imdbRating\": \"float\",\n",
    "    \"imdbVotes\": \"string\"\n",
    "})\n",
    "\n",
    "imdb['BoxOffice'] = imdb['BoxOffice'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "imdb['Released'] = imdb['Released'].str.extract(r'(\\d{4})').astype(float)\n",
    "imdb['Runtime'] = imdb['Runtime'].str.extract(r'(\\d+)').astype(float)\n",
    "imdb['imdbVotes'] = imdb['imdbVotes'].str.replace(',', '', regex=True).astype(float)\n",
    "\n",
    "\n",
    "tmdb = pd.read_csv(\"ml-latest-small/movie_info_tmdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "tmdb = tmdb.drop(columns=[\"title\", \"original_language\"])\n",
    "tmdb[\"release_date\"] = pd.to_datetime(tmdb[\"release_date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "imdb = imdb.merge(links, on=\"imdbId\", how=\"left\")\n",
    "imdb = imdb.merge(ratings, on=\"movieId\", how=\"left\")\n",
    "df2 = imdb.merge(tmdb, on=\"tmdbId\", how=\"left\")\n",
    "\n",
    "imdb = imdb.dropna()\n",
    "print(imdb.columns)\n",
    "\n",
    "print(imdb.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating',\n",
      "       'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       ...\n",
      "       'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa',\n",
      "       'Yiddish', 'Yoruba', 'Zulu'],\n",
      "      dtype='object', length=1088)\n",
      "<bound method NDFrame.head of         imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  \\\n",
      "0       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "1       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "2       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "3       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "4       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "...        ...          ...        ...       ...      ...         ...   \n",
      "91289  4912910  220159104.0       87.0    2018.0    147.0         7.7   \n",
      "91290  4912910  220159104.0       87.0    2018.0    147.0         7.7   \n",
      "91291  7690670   20545116.0       52.0    2018.0    116.0         5.3   \n",
      "91292  7349662   49275340.0       83.0    2018.0    135.0         7.5   \n",
      "91293  4073790   12695691.0       39.0    2018.0    104.0         5.7   \n",
      "\n",
      "       imdbVotes   movieId    tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  \\\n",
      "0      1112586.0       1.0     862.0     1.0  ...         0     0           0   \n",
      "1      1112586.0       1.0     862.0     5.0  ...         0     0           0   \n",
      "2      1112586.0       1.0     862.0     7.0  ...         0     0           0   \n",
      "3      1112586.0       1.0     862.0    15.0  ...         0     0           0   \n",
      "4      1112586.0       1.0     862.0    17.0  ...         0     0           0   \n",
      "...          ...       ...       ...     ...  ...       ...   ...         ...   \n",
      "91289   391467.0  189333.0  353081.0   184.0  ...         0     0           0   \n",
      "91290   391467.0  189333.0  353081.0   248.0  ...         0     0           0   \n",
      "91291     7799.0  189381.0  500475.0   318.0  ...         0     0           0   \n",
      "91292   300208.0  189713.0  487558.0   462.0  ...         0     0           0   \n",
      "91293    38486.0  190183.0  445651.0    50.0  ...         0     0           0   \n",
      "\n",
      "       Washoe  Welsh  Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0           0      0      0      0        0       0     0  \n",
      "1           0      0      0      0        0       0     0  \n",
      "2           0      0      0      0        0       0     0  \n",
      "3           0      0      0      0        0       0     0  \n",
      "4           0      0      0      0        0       0     0  \n",
      "...       ...    ...    ...    ...      ...     ...   ...  \n",
      "91289       0      0      0      0        0       0     0  \n",
      "91290       0      0      0      0        0       0     0  \n",
      "91291       0      0      0      0        0       0     0  \n",
      "91292       0      0      0      0        0       0     0  \n",
      "91293       0      0      0      0        0       0     0  \n",
      "\n",
      "[91294 rows x 1088 columns]>\n"
     ]
    }
   ],
   "source": [
    "df2 = imdb\n",
    "\n",
    "df2 = pd.get_dummies(df2, columns=[\"Country\"], dtype=float)\n",
    "\n",
    "df2[\"Language\"] = df2[\"Language\"].str.split(\",\")\n",
    "moviesExploded = df2.explode(\"Language\")\n",
    "moviesExploded[\"Language\"] = moviesExploded[\"Language\"].str.strip()\n",
    "movies_dummies = pd.get_dummies(moviesExploded[\"Language\"], dtype=int)\n",
    "movies_dummies = moviesExploded[[\"movieId\"]].join(movies_dummies).groupby(\"movieId\").max()\n",
    "df2 = df2.drop(columns=[\"Language\"]).merge(movies_dummies, on=\"movieId\")\n",
    "\n",
    "columns_to_convert = [col for col in df2.columns]\n",
    "df2[columns_to_convert] = df2[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df2 = df2.dropna()\n",
    "\n",
    "print(df2.columns)\n",
    "print(df2.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1087) (54776, 1)\n",
      "(18259, 1087) (18259, 1)\n",
      "(18259, 1087) (18259, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\"])\n",
    "\n",
    "df_shuffle = df_shuffle.dropna()\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.6), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.6):int(len(df_shuffle) * 0.8), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.8):, :]\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating',\n",
      "       'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       ...\n",
      "       'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa',\n",
      "       'Yiddish', 'Yoruba', 'Zulu'],\n",
      "      dtype='object', length=1088)\n",
      "imdbId                False\n",
      "BoxOffice             False\n",
      "Metascore             False\n",
      "Released              False\n",
      "Runtime               False\n",
      "                      ...  \n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "Length: 1093, dtype: bool\n",
      "   imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  imdbVotes  \\\n",
      "0  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "1  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "2  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "3  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "4  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "\n",
      "   movieId  tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  Washoe  Welsh  \\\n",
      "0      1.0   862.0     1.0  ...         0     0           0       0      0   \n",
      "1      1.0   862.0     5.0  ...         0     0           0       0      0   \n",
      "2      1.0   862.0     7.0  ...         0     0           0       0      0   \n",
      "3      1.0   862.0    15.0  ...         0     0           0       0      0   \n",
      "4      1.0   862.0    17.0  ...         0     0           0       0      0   \n",
      "\n",
      "   Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0      0      0        0       0     0  \n",
      "1      0      0        0       0     0  \n",
      "2      0      0        0       0     0  \n",
      "3      0      0        0       0     0  \n",
      "4      0      0        0       0     0  \n",
      "\n",
      "[5 rows x 1088 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_train).any())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1093)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_col = df_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.transform(df_val[numerical_col])  \n",
    "df_test[numerical_col] = scaler.transform(df_test[numerical_col]) \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1092, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy())  \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.112031, MSE: 0.226171, RMSE: 0.475574, R²: -3.810732\n",
      "VALIDATION -> Loss: 0.029011, MSE: 0.058039, RMSE: 0.240912, R²: -0.230538\n",
      "Iteration 2/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.040232, MSE: 0.080550, RMSE: 0.283813, R²: -0.713323\n",
      "VALIDATION -> Loss: 0.016536, MSE: 0.033087, RMSE: 0.181899, R²: 0.298481\n",
      "Iteration 3/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.035301, MSE: 0.070579, RMSE: 0.265667, R²: -0.501242\n",
      "VALIDATION -> Loss: 0.015475, MSE: 0.030958, RMSE: 0.175948, R²: 0.343631\n",
      "Iteration 4/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.033386, MSE: 0.066765, RMSE: 0.258390, R²: -0.420126\n",
      "VALIDATION -> Loss: 0.014151, MSE: 0.028317, RMSE: 0.168276, R²: 0.399626\n",
      "Iteration 5/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.031927, MSE: 0.063914, RMSE: 0.252812, R²: -0.359469\n",
      "VALIDATION -> Loss: 0.013792, MSE: 0.027600, RMSE: 0.166131, R²: 0.414833\n",
      "Iteration 6/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.029627, MSE: 0.059249, RMSE: 0.243411, R²: -0.260244\n",
      "VALIDATION -> Loss: 0.013789, MSE: 0.027595, RMSE: 0.166117, R²: 0.414931\n",
      "Iteration 7/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.028946, MSE: 0.057909, RMSE: 0.240643, R²: -0.231750\n",
      "VALIDATION -> Loss: 0.013342, MSE: 0.026701, RMSE: 0.163403, R²: 0.433895\n",
      "Iteration 8/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.028028, MSE: 0.056076, RMSE: 0.236804, R²: -0.192762\n",
      "VALIDATION -> Loss: 0.017464, MSE: 0.034942, RMSE: 0.186929, R²: 0.259149\n",
      "Iteration 9/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.027468, MSE: 0.054903, RMSE: 0.234313, R²: -0.167799\n",
      "VALIDATION -> Loss: 0.013546, MSE: 0.027104, RMSE: 0.164633, R²: 0.425337\n",
      "Iteration 10/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.026467, MSE: 0.052990, RMSE: 0.230194, R²: -0.127106\n",
      "VALIDATION -> Loss: 0.013904, MSE: 0.027820, RMSE: 0.166794, R²: 0.410153\n",
      "Iteration 11/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.025866, MSE: 0.051758, RMSE: 0.227504, R²: -0.100915\n",
      "VALIDATION -> Loss: 0.012911, MSE: 0.025835, RMSE: 0.160732, R²: 0.452247\n",
      "Iteration 12/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.025573, MSE: 0.051158, RMSE: 0.226181, R²: -0.088146\n",
      "VALIDATION -> Loss: 0.014097, MSE: 0.028205, RMSE: 0.167945, R²: 0.401988\n",
      "Iteration 13/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.024590, MSE: 0.049200, RMSE: 0.221811, R²: -0.046507\n",
      "VALIDATION -> Loss: 0.012896, MSE: 0.025802, RMSE: 0.160630, R²: 0.452945\n",
      "Iteration 14/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.023892, MSE: 0.047762, RMSE: 0.218546, R²: -0.015922\n",
      "VALIDATION -> Loss: 0.013248, MSE: 0.026506, RMSE: 0.162808, R²: 0.438010\n",
      "Iteration 15/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.023344, MSE: 0.046685, RMSE: 0.216067, R²: 0.006994\n",
      "VALIDATION -> Loss: 0.012987, MSE: 0.025981, RMSE: 0.161185, R²: 0.449159\n",
      "Iteration 16/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.022805, MSE: 0.045637, RMSE: 0.213629, R²: 0.029277\n",
      "VALIDATION -> Loss: 0.013987, MSE: 0.027984, RMSE: 0.167285, R²: 0.406675\n",
      "Iteration 17/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.022233, MSE: 0.044466, RMSE: 0.210869, R²: 0.054198\n",
      "VALIDATION -> Loss: 0.013062, MSE: 0.026132, RMSE: 0.161653, R²: 0.445954\n",
      "Iteration 18/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.021853, MSE: 0.043741, RMSE: 0.209143, R²: 0.069617\n",
      "VALIDATION -> Loss: 0.013230, MSE: 0.026470, RMSE: 0.162696, R²: 0.438779\n",
      "Iteration 19/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020999, MSE: 0.042020, RMSE: 0.204988, R²: 0.106213\n",
      "VALIDATION -> Loss: 0.012958, MSE: 0.025920, RMSE: 0.160998, R²: 0.450439\n",
      "Iteration 20/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020571, MSE: 0.041170, RMSE: 0.202903, R²: 0.124306\n",
      "VALIDATION -> Loss: 0.013113, MSE: 0.026233, RMSE: 0.161967, R²: 0.443798\n",
      "Iteration 21/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020388, MSE: 0.040816, RMSE: 0.202029, R²: 0.131837\n",
      "VALIDATION -> Loss: 0.012881, MSE: 0.025767, RMSE: 0.160522, R²: 0.453683\n",
      "Iteration 22/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019832, MSE: 0.039691, RMSE: 0.199225, R²: 0.155765\n",
      "VALIDATION -> Loss: 0.012976, MSE: 0.025956, RMSE: 0.161109, R²: 0.449677\n",
      "Iteration 23/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019400, MSE: 0.038802, RMSE: 0.196982, R²: 0.174672\n",
      "VALIDATION -> Loss: 0.013039, MSE: 0.026079, RMSE: 0.161491, R²: 0.447066\n",
      "Iteration 24/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019170, MSE: 0.038330, RMSE: 0.195779, R²: 0.184716\n",
      "VALIDATION -> Loss: 0.013206, MSE: 0.026421, RMSE: 0.162546, R²: 0.439819\n",
      "Iteration 25/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018621, MSE: 0.037206, RMSE: 0.192890, R²: 0.208606\n",
      "VALIDATION -> Loss: 0.013535, MSE: 0.027078, RMSE: 0.164554, R²: 0.425892\n",
      "Iteration 26/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018199, MSE: 0.036410, RMSE: 0.190815, R²: 0.225538\n",
      "VALIDATION -> Loss: 0.013334, MSE: 0.026677, RMSE: 0.163331, R²: 0.434391\n",
      "Iteration 27/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017995, MSE: 0.035981, RMSE: 0.189686, R²: 0.234672\n",
      "VALIDATION -> Loss: 0.013060, MSE: 0.026125, RMSE: 0.161632, R²: 0.446099\n",
      "Iteration 28/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017573, MSE: 0.035192, RMSE: 0.187595, R²: 0.251454\n",
      "VALIDATION -> Loss: 0.013395, MSE: 0.026794, RMSE: 0.163689, R²: 0.431913\n",
      "Iteration 29/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017280, MSE: 0.034560, RMSE: 0.185902, R²: 0.264902\n",
      "VALIDATION -> Loss: 0.012933, MSE: 0.025869, RMSE: 0.160839, R²: 0.451521\n",
      "Iteration 30/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017040, MSE: 0.034067, RMSE: 0.184572, R²: 0.275388\n",
      "VALIDATION -> Loss: 0.012988, MSE: 0.025981, RMSE: 0.161186, R²: 0.449150\n",
      "Iteration 31/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016822, MSE: 0.033651, RMSE: 0.183442, R²: 0.284232\n",
      "VALIDATION -> Loss: 0.012933, MSE: 0.025872, RMSE: 0.160847, R²: 0.451464\n",
      "Iteration 32/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016538, MSE: 0.033074, RMSE: 0.181861, R²: 0.296512\n",
      "VALIDATION -> Loss: 0.012965, MSE: 0.025940, RMSE: 0.161060, R²: 0.450015\n",
      "Iteration 33/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016291, MSE: 0.032586, RMSE: 0.180517, R²: 0.306875\n",
      "VALIDATION -> Loss: 0.012877, MSE: 0.025759, RMSE: 0.160497, R²: 0.453850\n",
      "Iteration 34/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016084, MSE: 0.032172, RMSE: 0.179365, R²: 0.315693\n",
      "VALIDATION -> Loss: 0.012824, MSE: 0.025654, RMSE: 0.160169, R²: 0.456079\n",
      "Iteration 35/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015719, MSE: 0.031477, RMSE: 0.177417, R²: 0.330476\n",
      "VALIDATION -> Loss: 0.012854, MSE: 0.025717, RMSE: 0.160365, R²: 0.454746\n",
      "Iteration 36/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015712, MSE: 0.031408, RMSE: 0.177224, R²: 0.331932\n",
      "VALIDATION -> Loss: 0.012745, MSE: 0.025494, RMSE: 0.159667, R²: 0.459482\n",
      "Iteration 37/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015526, MSE: 0.031091, RMSE: 0.176328, R²: 0.338673\n",
      "VALIDATION -> Loss: 0.012834, MSE: 0.025675, RMSE: 0.160234, R²: 0.455636\n",
      "Iteration 38/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015412, MSE: 0.030799, RMSE: 0.175497, R²: 0.344888\n",
      "VALIDATION -> Loss: 0.012730, MSE: 0.025466, RMSE: 0.159581, R²: 0.460065\n",
      "Iteration 39/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015163, MSE: 0.030288, RMSE: 0.174035, R²: 0.355759\n",
      "VALIDATION -> Loss: 0.012753, MSE: 0.025510, RMSE: 0.159720, R²: 0.459127\n",
      "Iteration 40/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015086, MSE: 0.030146, RMSE: 0.173626, R²: 0.358787\n",
      "VALIDATION -> Loss: 0.012818, MSE: 0.025641, RMSE: 0.160129, R²: 0.456352\n",
      "Iteration 41/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014786, MSE: 0.029598, RMSE: 0.172041, R²: 0.370441\n",
      "VALIDATION -> Loss: 0.012740, MSE: 0.025484, RMSE: 0.159638, R²: 0.459679\n",
      "Iteration 42/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014719, MSE: 0.029478, RMSE: 0.171691, R²: 0.372997\n",
      "VALIDATION -> Loss: 0.012857, MSE: 0.025719, RMSE: 0.160372, R²: 0.454703\n",
      "Iteration 43/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014653, MSE: 0.029285, RMSE: 0.171130, R²: 0.377087\n",
      "VALIDATION -> Loss: 0.012909, MSE: 0.025829, RMSE: 0.160713, R²: 0.452380\n",
      "Iteration 44/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014402, MSE: 0.028780, RMSE: 0.169648, R²: 0.387829\n",
      "VALIDATION -> Loss: 0.012944, MSE: 0.025890, RMSE: 0.160903, R²: 0.451081\n",
      "Iteration 45/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014290, MSE: 0.028581, RMSE: 0.169058, R²: 0.392082\n",
      "VALIDATION -> Loss: 0.012722, MSE: 0.025452, RMSE: 0.159536, R²: 0.460374\n",
      "Iteration 46/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014305, MSE: 0.028585, RMSE: 0.169071, R²: 0.391989\n",
      "VALIDATION -> Loss: 0.013065, MSE: 0.026140, RMSE: 0.161678, R²: 0.445786\n",
      "Iteration 47/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014136, MSE: 0.028263, RMSE: 0.168115, R²: 0.398844\n",
      "VALIDATION -> Loss: 0.013091, MSE: 0.026192, RMSE: 0.161839, R²: 0.444679\n",
      "Iteration 48/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013991, MSE: 0.027983, RMSE: 0.167281, R²: 0.404795\n",
      "VALIDATION -> Loss: 0.012820, MSE: 0.025645, RMSE: 0.160141, R²: 0.456271\n",
      "Iteration 49/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014038, MSE: 0.028058, RMSE: 0.167505, R²: 0.403197\n",
      "VALIDATION -> Loss: 0.012723, MSE: 0.025451, RMSE: 0.159534, R²: 0.460382\n",
      "Iteration 50/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013941, MSE: 0.027881, RMSE: 0.166975, R²: 0.406969\n",
      "VALIDATION -> Loss: 0.012846, MSE: 0.025695, RMSE: 0.160298, R²: 0.455204\n"
     ]
    }
   ],
   "source": [
    "for i in range(50): \n",
    "    print(f\"Iteration {i+1}/50 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4667\n",
      "MAE: 0.1214\n",
      "MSE: 0.0251\n",
      "RMSE: 0.1583\n",
      "Precisión: 0.6486\n",
      "Recall: 0.9138\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FIFTH MODEL**\n",
    "# Like the fourth one but using the 1M ratings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Country', 'Language', 'Metascore', 'Released',\n",
      "       'Runtime', 'imdbRating', 'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       'rating', 'timestamp'],\n",
      "      dtype='object')\n",
      "<bound method NDFrame.describe of          imdbId    BoxOffice                               Country  \\\n",
      "0        114709  223225679.0                         United States   \n",
      "1        114709  223225679.0                         United States   \n",
      "2        114709  223225679.0                         United States   \n",
      "3        114709  223225679.0                         United States   \n",
      "4        114709  223225679.0                         United States   \n",
      "...         ...          ...                                   ...   \n",
      "100799  4912910  220159104.0  United States, China, France, Norway   \n",
      "100800  4912910  220159104.0  United States, China, France, Norway   \n",
      "100801  7690670   20545116.0                         United States   \n",
      "100803  7349662   49275340.0                  United States, China   \n",
      "100804  4073790   12695691.0                         United States   \n",
      "\n",
      "                           Language  Metascore  Released  Runtime  imdbRating  \\\n",
      "0                           English       96.0    1995.0     81.0         8.3   \n",
      "1                           English       96.0    1995.0     81.0         8.3   \n",
      "2                           English       96.0    1995.0     81.0         8.3   \n",
      "3                           English       96.0    1995.0     81.0         8.3   \n",
      "4                           English       96.0    1995.0     81.0         8.3   \n",
      "...                             ...        ...       ...      ...         ...   \n",
      "100799              English, French       87.0    2018.0    147.0         7.7   \n",
      "100800              English, French       87.0    2018.0    147.0         7.7   \n",
      "100801  English, Spanish, Cantonese       52.0    2018.0    116.0         5.3   \n",
      "100803                      English       83.0    2018.0    135.0         7.5   \n",
      "100804                      English       39.0    2018.0    104.0         5.7   \n",
      "\n",
      "        imdbVotes   movieId    tmdbId  userId  rating     timestamp  \n",
      "0       1112586.0       1.0     862.0     1.0     4.0  9.649827e+08  \n",
      "1       1112586.0       1.0     862.0     5.0     4.0  8.474350e+08  \n",
      "2       1112586.0       1.0     862.0     7.0     4.0  1.106636e+09  \n",
      "3       1112586.0       1.0     862.0    15.0     2.0  1.510578e+09  \n",
      "4       1112586.0       1.0     862.0    17.0     4.0  1.305696e+09  \n",
      "...           ...       ...       ...     ...     ...           ...  \n",
      "100799   391467.0  189333.0  353081.0   184.0     4.0  1.537110e+09  \n",
      "100800   391467.0  189333.0  353081.0   248.0     3.0  1.534602e+09  \n",
      "100801     7799.0  189381.0  500475.0   318.0     2.0  1.536098e+09  \n",
      "100803   300208.0  189713.0  487558.0   462.0     2.0  1.536467e+09  \n",
      "100804    38486.0  190183.0  445651.0    50.0     3.0  1.533302e+09  \n",
      "\n",
      "[91294 rows x 14 columns]>\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv(\"ml-latest-small/movie_info_imdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "imdb['imdbId'] = imdb['imdbId'].str.replace('tt', '', regex=False).astype(int)\n",
    "columns_to_drop = ['Actors', 'Awards', 'DVD', 'Director', 'Genre', 'Title', 'Type', 'Website', 'Year', 'Poster', 'Production', 'Rated', 'Plot', 'Writer', 'Response', 'Ratings']\n",
    "imdb = imdb.drop(columns=columns_to_drop)\n",
    "\n",
    "imdb = imdb.astype({\n",
    "    \"BoxOffice\": \"string\", \n",
    "    \"Country\": \"string\",\n",
    "    \"Language\": \"string\",\n",
    "    \"Metascore\": \"float\",\n",
    "    \"Released\": \"string\",\n",
    "    \"Runtime\": \"string\",\n",
    "    \"imdbRating\": \"float\",\n",
    "    \"imdbVotes\": \"string\"\n",
    "})\n",
    "\n",
    "imdb['BoxOffice'] = imdb['BoxOffice'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "imdb['Released'] = imdb['Released'].str.extract(r'(\\d{4})').astype(float)\n",
    "imdb['Runtime'] = imdb['Runtime'].str.extract(r'(\\d+)').astype(float)\n",
    "imdb['imdbVotes'] = imdb['imdbVotes'].str.replace(',', '', regex=True).astype(float)\n",
    "\n",
    "tmdb = pd.read_csv(\"ml-latest-small/movie_info_tmdb.csv\", usecols=lambda column: column != \"origin_country\")\n",
    "tmdb = tmdb.drop(columns=[\"title\", \"original_language\"])\n",
    "tmdb[\"release_date\"] = pd.to_datetime(tmdb[\"release_date\"], errors=\"coerce\").dt.year\n",
    "\n",
    "imdb = imdb.merge(links, on=\"imdbId\", how=\"left\")\n",
    "imdb = imdb.merge(ratings, on=\"movieId\", how=\"left\")\n",
    "df2 = imdb.merge(tmdb, on=\"tmdbId\", how=\"left\")\n",
    "\n",
    "imdb = imdb.dropna()\n",
    "print(imdb.columns)\n",
    "print(imdb.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating',\n",
      "       'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       ...\n",
      "       'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa',\n",
      "       'Yiddish', 'Yoruba', 'Zulu'],\n",
      "      dtype='object', length=1088)\n",
      "<bound method NDFrame.head of         imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  \\\n",
      "0       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "1       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "2       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "3       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "4       114709  223225679.0       96.0    1995.0     81.0         8.3   \n",
      "...        ...          ...        ...       ...      ...         ...   \n",
      "91289  4912910  220159104.0       87.0    2018.0    147.0         7.7   \n",
      "91290  4912910  220159104.0       87.0    2018.0    147.0         7.7   \n",
      "91291  7690670   20545116.0       52.0    2018.0    116.0         5.3   \n",
      "91292  7349662   49275340.0       83.0    2018.0    135.0         7.5   \n",
      "91293  4073790   12695691.0       39.0    2018.0    104.0         5.7   \n",
      "\n",
      "       imdbVotes   movieId    tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  \\\n",
      "0      1112586.0       1.0     862.0     1.0  ...         0     0           0   \n",
      "1      1112586.0       1.0     862.0     5.0  ...         0     0           0   \n",
      "2      1112586.0       1.0     862.0     7.0  ...         0     0           0   \n",
      "3      1112586.0       1.0     862.0    15.0  ...         0     0           0   \n",
      "4      1112586.0       1.0     862.0    17.0  ...         0     0           0   \n",
      "...          ...       ...       ...     ...  ...       ...   ...         ...   \n",
      "91289   391467.0  189333.0  353081.0   184.0  ...         0     0           0   \n",
      "91290   391467.0  189333.0  353081.0   248.0  ...         0     0           0   \n",
      "91291     7799.0  189381.0  500475.0   318.0  ...         0     0           0   \n",
      "91292   300208.0  189713.0  487558.0   462.0  ...         0     0           0   \n",
      "91293    38486.0  190183.0  445651.0    50.0  ...         0     0           0   \n",
      "\n",
      "       Washoe  Welsh  Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0           0      0      0      0        0       0     0  \n",
      "1           0      0      0      0        0       0     0  \n",
      "2           0      0      0      0        0       0     0  \n",
      "3           0      0      0      0        0       0     0  \n",
      "4           0      0      0      0        0       0     0  \n",
      "...       ...    ...    ...    ...      ...     ...   ...  \n",
      "91289       0      0      0      0        0       0     0  \n",
      "91290       0      0      0      0        0       0     0  \n",
      "91291       0      0      0      0        0       0     0  \n",
      "91292       0      0      0      0        0       0     0  \n",
      "91293       0      0      0      0        0       0     0  \n",
      "\n",
      "[91294 rows x 1088 columns]>\n"
     ]
    }
   ],
   "source": [
    "df2 = imdb\n",
    "df2 = pd.get_dummies(df2, columns=[\"Country\"], dtype=float)\n",
    "\n",
    "df2[\"Language\"] = df2[\"Language\"].str.split(\",\")\n",
    "moviesExploded = df2.explode(\"Language\")\n",
    "moviesExploded[\"Language\"] = moviesExploded[\"Language\"].str.strip()\n",
    "movies_dummies = pd.get_dummies(moviesExploded[\"Language\"], dtype=int)\n",
    "movies_dummies = moviesExploded[[\"movieId\"]].join(movies_dummies).groupby(\"movieId\").max()\n",
    "df2 = df2.drop(columns=[\"Language\"]).merge(movies_dummies, on=\"movieId\")\n",
    "\n",
    "\n",
    "columns_to_convert = [col for col in df2.columns]\n",
    "df2[columns_to_convert] = df2[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "df2 = df2.dropna()\n",
    "\n",
    "print(df2.columns)\n",
    "print(df2.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1087) (54776, 1)\n",
      "(18259, 1087) (18259, 1)\n",
      "(18259, 1087) (18259, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "df_shuffle = df2.sample(frac=1, random_state=123).drop(columns=[\"timestamp\"])\n",
    "df_shuffle = df_shuffle.dropna()\n",
    "\n",
    "df_train = df_shuffle.iloc[:int(len(df_shuffle) * 0.6), :]\n",
    "df_val = df_shuffle.iloc[int(len(df_shuffle) * 0.6):int(len(df_shuffle) * 0.8), :]\n",
    "df_test = df_shuffle.iloc[int(len(df_shuffle) * 0.8):, :]\n",
    "\n",
    "scalers = {}\n",
    "\n",
    "feature_cols = [col for col in df_shuffle.columns]\n",
    "x_train, y_train = df_train[feature_cols].to_numpy(dtype=np.float32), df_train[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_val, y_val = df_val[feature_cols].to_numpy(dtype=np.float32), df_val[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "x_test, y_test = df_test[feature_cols].to_numpy(dtype=np.float32), df_test[\"rating\"].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train).float()\n",
    "x_val, y_val = torch.tensor(x_val), torch.tensor(y_val).float()\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['imdbId', 'BoxOffice', 'Metascore', 'Released', 'Runtime', 'imdbRating',\n",
      "       'imdbVotes', 'movieId', 'tmdbId', 'userId',\n",
      "       ...\n",
      "       'Ungwatsi', 'Urdu', 'Vietnamese', 'Washoe', 'Welsh', 'Wolof', 'Xhosa',\n",
      "       'Yiddish', 'Yoruba', 'Zulu'],\n",
      "      dtype='object', length=1088)\n",
      "imdbId                False\n",
      "BoxOffice             False\n",
      "Metascore             False\n",
      "Released              False\n",
      "Runtime               False\n",
      "                      ...  \n",
      "count_movie_rating    False\n",
      "std_movie_rating      False\n",
      "avg_user_rating       False\n",
      "count_user_rating     False\n",
      "std_user_rating       False\n",
      "Length: 1093, dtype: bool\n",
      "   imdbId    BoxOffice  Metascore  Released  Runtime  imdbRating  imdbVotes  \\\n",
      "0  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "1  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "2  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "3  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "4  114709  223225679.0       96.0    1995.0     81.0         8.3  1112586.0   \n",
      "\n",
      "   movieId  tmdbId  userId  ...  Ungwatsi  Urdu  Vietnamese  Washoe  Welsh  \\\n",
      "0      1.0   862.0     1.0  ...         0     0           0       0      0   \n",
      "1      1.0   862.0     5.0  ...         0     0           0       0      0   \n",
      "2      1.0   862.0     7.0  ...         0     0           0       0      0   \n",
      "3      1.0   862.0    15.0  ...         0     0           0       0      0   \n",
      "4      1.0   862.0    17.0  ...         0     0           0       0      0   \n",
      "\n",
      "   Wolof  Xhosa  Yiddish  Yoruba  Zulu  \n",
      "0      0      0        0       0     0  \n",
      "1      0      0        0       0     0  \n",
      "2      0      0        0       0     0  \n",
      "3      0      0        0       0     0  \n",
      "4      0      0        0       0     0  \n",
      "\n",
      "[5 rows x 1088 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_train = df_train.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_train = df_train.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_train = df_train.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_train = df_train.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_train[\"count_movie_rating\"] = df_train[\"count_movie_rating\"].fillna(0)\n",
    "df_train[\"count_user_rating\"] = df_train[\"count_user_rating\"].fillna(0)\n",
    "df_train[\"avg_movie_rating\"] = df_train[\"avg_movie_rating\"].fillna(0)\n",
    "df_train[\"avg_user_rating\"] = df_train[\"avg_user_rating\"].fillna(0)\n",
    "df_train[\"std_movie_rating\"] = df_train[\"std_movie_rating\"].fillna(0)\n",
    "df_train[\"std_user_rating\"] = df_train[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_val.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_val.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_val.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_val.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_val = df_val.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_val = df_val.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_val = df_val.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_val = df_val.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_val[\"count_movie_rating\"] = df_val[\"count_movie_rating\"].fillna(0)\n",
    "df_val[\"count_user_rating\"] = df_val[\"count_user_rating\"].fillna(0)\n",
    "df_val[\"avg_movie_rating\"] = df_val[\"avg_movie_rating\"].fillna(0)\n",
    "df_val[\"avg_user_rating\"] = df_val[\"avg_user_rating\"].fillna(0)\n",
    "df_val[\"std_movie_rating\"] = df_val[\"std_movie_rating\"].fillna(0)\n",
    "df_val[\"std_user_rating\"] = df_val[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "avg_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].mean().rename(\"avg_movie_rating\")\n",
    "count_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].count().rename(\"count_movie_rating\")\n",
    "std_movie_rating = df_train.groupby(\"movieId\")[\"rating\"].std().rename(\"std_movie_rating\")\n",
    "\n",
    "avg_user_rating = df_train.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_user_rating\")\n",
    "count_user_rating = df_train.groupby(\"userId\")[\"rating\"].count().rename(\"count_user_rating\")\n",
    "std_user_rating = df_train.groupby(\"userId\")[\"rating\"].std().rename(\"std_user_rating\")\n",
    "\n",
    "df_test = df_test.merge(avg_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(count_movie_rating, on=\"movieId\", how=\"left\")\n",
    "df_test = df_test.merge(std_movie_rating, on=\"movieId\", how=\"left\")\n",
    "\n",
    "df_test = df_test.merge(avg_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(count_user_rating, on=\"userId\", how=\"left\")\n",
    "df_test = df_test.merge(std_user_rating, on=\"userId\", how=\"left\")\n",
    "\n",
    "df_test[\"count_movie_rating\"] = df_test[\"count_movie_rating\"].fillna(0)\n",
    "df_test[\"count_user_rating\"] = df_test[\"count_user_rating\"].fillna(0)\n",
    "df_test[\"avg_movie_rating\"] = df_test[\"avg_movie_rating\"].fillna(0)\n",
    "df_test[\"avg_user_rating\"] = df_test[\"avg_user_rating\"].fillna(0)\n",
    "df_test[\"std_movie_rating\"] = df_test[\"std_movie_rating\"].fillna(0)\n",
    "df_test[\"std_user_rating\"] = df_test[\"std_user_rating\"].fillna(0)\n",
    "\n",
    "\n",
    "print(np.isnan(df_train).any())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54776, 1093)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "numerical_col = df_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "df_train[numerical_col] = scaler.fit_transform(df_train[numerical_col])\n",
    "df_val[numerical_col] = scaler.transform(df_val[numerical_col])  # Usar transform en validación\n",
    "df_test[numerical_col] = scaler.transform(df_test[numerical_col]) \n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col != \"rating\"]\n",
    "\n",
    "x_train = torch.tensor(df_train[feature_cols].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(df_train[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_val = torch.tensor(df_val[feature_cols].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "x_test = torch.tensor(df_test[feature_cols].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"rating\"].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1092, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "device = \"cuda\" \n",
    "model = NeuralNetwork().to(device)  #To change to the GPU\n",
    "\n",
    "lossFunction = torch.nn.HuberLoss() \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# TRAINING FUNCTION\n",
    "def train_loop(dataloader, model, lossFunction, optimizer):\n",
    "    train_size = len(dataloader.dataset)    \n",
    "    nbatches = len(dataloader)  \n",
    "\n",
    "    model.train()\n",
    "    loss_train = 0  \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for nbatch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        \n",
    "        loss = lossFunction(logits, y)\n",
    "        loss.backward()   \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "\n",
    "        all_preds.extend(logits.detach().cpu().numpy())  \n",
    "        all_targets.extend(y.cpu().numpy())  \n",
    "\n",
    "\n",
    "    avg_loss = loss_train / nbatches\n",
    "\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'TRAINING -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')\n",
    "\n",
    "\n",
    "# VALIDATION FUNCTION\n",
    "def val_loop(dataloader, model, lossFunction):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    nbatches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            loss_val += lossFunction(logits, y).item()\n",
    "            \n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_val / nbatches\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "    print(f'VALIDATION -> Loss: {avg_loss:.6f}, MSE: {mse:.6f}, RMSE: {rmse:.6f}, R²: {r2:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.051368, MSE: 0.103088, RMSE: 0.321073, R²: -1.192717\n",
      "VALIDATION -> Loss: 0.018047, MSE: 0.036114, RMSE: 0.190037, R²: 0.234312\n",
      "Iteration 2/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.027441, MSE: 0.054928, RMSE: 0.234367, R²: -0.168337\n",
      "VALIDATION -> Loss: 0.016460, MSE: 0.032939, RMSE: 0.181491, R²: 0.301628\n",
      "Iteration 3/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.024306, MSE: 0.048671, RMSE: 0.220615, R²: -0.035254\n",
      "VALIDATION -> Loss: 0.015325, MSE: 0.030668, RMSE: 0.175122, R²: 0.349782\n",
      "Iteration 4/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.022778, MSE: 0.045515, RMSE: 0.213342, R²: 0.031882\n",
      "VALIDATION -> Loss: 0.014125, MSE: 0.028265, RMSE: 0.168123, R²: 0.400714\n",
      "Iteration 5/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.022042, MSE: 0.044118, RMSE: 0.210042, R²: 0.061603\n",
      "VALIDATION -> Loss: 0.013755, MSE: 0.027521, RMSE: 0.165896, R²: 0.416491\n",
      "Iteration 6/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.021387, MSE: 0.042744, RMSE: 0.206747, R²: 0.090811\n",
      "VALIDATION -> Loss: 0.013683, MSE: 0.027381, RMSE: 0.165473, R²: 0.419457\n",
      "Iteration 7/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020780, MSE: 0.041547, RMSE: 0.203832, R²: 0.116271\n",
      "VALIDATION -> Loss: 0.013794, MSE: 0.027605, RMSE: 0.166149, R²: 0.414708\n",
      "Iteration 8/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.020391, MSE: 0.040752, RMSE: 0.201872, R²: 0.133182\n",
      "VALIDATION -> Loss: 0.013423, MSE: 0.026856, RMSE: 0.163878, R²: 0.430597\n",
      "Iteration 9/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019895, MSE: 0.039832, RMSE: 0.199580, R²: 0.152757\n",
      "VALIDATION -> Loss: 0.013183, MSE: 0.026379, RMSE: 0.162415, R²: 0.440722\n",
      "Iteration 10/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.019440, MSE: 0.038899, RMSE: 0.197228, R²: 0.172604\n",
      "VALIDATION -> Loss: 0.013147, MSE: 0.026305, RMSE: 0.162188, R²: 0.442281\n",
      "Iteration 11/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018939, MSE: 0.037888, RMSE: 0.194647, R²: 0.194118\n",
      "VALIDATION -> Loss: 0.012957, MSE: 0.025924, RMSE: 0.161008, R²: 0.450368\n",
      "Iteration 12/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018690, MSE: 0.037351, RMSE: 0.193263, R²: 0.205539\n",
      "VALIDATION -> Loss: 0.013812, MSE: 0.027633, RMSE: 0.166231, R²: 0.414131\n",
      "Iteration 13/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018221, MSE: 0.036409, RMSE: 0.190811, R²: 0.225574\n",
      "VALIDATION -> Loss: 0.012880, MSE: 0.025768, RMSE: 0.160525, R²: 0.453661\n",
      "Iteration 14/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.018023, MSE: 0.036039, RMSE: 0.189839, R²: 0.233442\n",
      "VALIDATION -> Loss: 0.013074, MSE: 0.026155, RMSE: 0.161727, R²: 0.445451\n",
      "Iteration 15/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017623, MSE: 0.035230, RMSE: 0.187696, R²: 0.250652\n",
      "VALIDATION -> Loss: 0.013284, MSE: 0.026579, RMSE: 0.163029, R²: 0.436479\n",
      "Iteration 16/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.017315, MSE: 0.034634, RMSE: 0.186101, R²: 0.263329\n",
      "VALIDATION -> Loss: 0.012802, MSE: 0.025606, RMSE: 0.160020, R²: 0.457092\n",
      "Iteration 17/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016912, MSE: 0.033784, RMSE: 0.183803, R²: 0.281411\n",
      "VALIDATION -> Loss: 0.012735, MSE: 0.025477, RMSE: 0.159616, R²: 0.459830\n",
      "Iteration 18/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016637, MSE: 0.033264, RMSE: 0.182385, R²: 0.292453\n",
      "VALIDATION -> Loss: 0.012763, MSE: 0.025532, RMSE: 0.159789, R²: 0.458661\n",
      "Iteration 19/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016422, MSE: 0.032831, RMSE: 0.181194, R²: 0.301667\n",
      "VALIDATION -> Loss: 0.012784, MSE: 0.025570, RMSE: 0.159906, R²: 0.457866\n",
      "Iteration 20/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016127, MSE: 0.032284, RMSE: 0.179678, R²: 0.313300\n",
      "VALIDATION -> Loss: 0.012921, MSE: 0.025849, RMSE: 0.160775, R²: 0.451958\n",
      "Iteration 21/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.016126, MSE: 0.032239, RMSE: 0.179553, R²: 0.314258\n",
      "VALIDATION -> Loss: 0.013184, MSE: 0.026373, RMSE: 0.162398, R²: 0.440836\n",
      "Iteration 22/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015759, MSE: 0.031489, RMSE: 0.177451, R²: 0.330218\n",
      "VALIDATION -> Loss: 0.013136, MSE: 0.026283, RMSE: 0.162119, R²: 0.442757\n",
      "Iteration 23/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015655, MSE: 0.031279, RMSE: 0.176860, R²: 0.334674\n",
      "VALIDATION -> Loss: 0.012884, MSE: 0.025772, RMSE: 0.160538, R²: 0.453574\n",
      "Iteration 24/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015618, MSE: 0.031231, RMSE: 0.176722, R²: 0.335709\n",
      "VALIDATION -> Loss: 0.012769, MSE: 0.025542, RMSE: 0.159819, R²: 0.458455\n",
      "Iteration 25/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015193, MSE: 0.030396, RMSE: 0.174346, R²: 0.353457\n",
      "VALIDATION -> Loss: 0.012689, MSE: 0.025384, RMSE: 0.159324, R²: 0.461805\n",
      "Iteration 26/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.015071, MSE: 0.030135, RMSE: 0.173595, R²: 0.359015\n",
      "VALIDATION -> Loss: 0.012907, MSE: 0.025821, RMSE: 0.160688, R²: 0.452547\n",
      "Iteration 27/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014849, MSE: 0.029711, RMSE: 0.172368, R²: 0.368042\n",
      "VALIDATION -> Loss: 0.012778, MSE: 0.025562, RMSE: 0.159882, R²: 0.458028\n",
      "Iteration 28/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014677, MSE: 0.029360, RMSE: 0.171348, R²: 0.375502\n",
      "VALIDATION -> Loss: 0.012906, MSE: 0.025822, RMSE: 0.160691, R²: 0.452530\n",
      "Iteration 29/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014575, MSE: 0.029144, RMSE: 0.170717, R²: 0.380094\n",
      "VALIDATION -> Loss: 0.013062, MSE: 0.026130, RMSE: 0.161648, R²: 0.445991\n",
      "Iteration 30/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014471, MSE: 0.028978, RMSE: 0.170230, R²: 0.383619\n",
      "VALIDATION -> Loss: 0.012746, MSE: 0.025495, RMSE: 0.159671, R²: 0.459461\n",
      "Iteration 31/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014402, MSE: 0.028824, RMSE: 0.169777, R²: 0.386896\n",
      "VALIDATION -> Loss: 0.012778, MSE: 0.025564, RMSE: 0.159889, R²: 0.457982\n",
      "Iteration 32/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014238, MSE: 0.028482, RMSE: 0.168767, R²: 0.394170\n",
      "VALIDATION -> Loss: 0.012827, MSE: 0.025659, RMSE: 0.160185, R²: 0.455973\n",
      "Iteration 33/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014087, MSE: 0.028199, RMSE: 0.167925, R²: 0.400201\n",
      "VALIDATION -> Loss: 0.012714, MSE: 0.025432, RMSE: 0.159475, R²: 0.460785\n",
      "Iteration 34/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.014026, MSE: 0.028025, RMSE: 0.167407, R²: 0.403896\n",
      "VALIDATION -> Loss: 0.013201, MSE: 0.026407, RMSE: 0.162501, R²: 0.440128\n",
      "Iteration 35/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013975, MSE: 0.027942, RMSE: 0.167160, R²: 0.405654\n",
      "VALIDATION -> Loss: 0.013105, MSE: 0.026212, RMSE: 0.161902, R²: 0.444247\n",
      "Iteration 36/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013861, MSE: 0.027735, RMSE: 0.166539, R²: 0.410063\n",
      "VALIDATION -> Loss: 0.012765, MSE: 0.025534, RMSE: 0.159794, R²: 0.458622\n",
      "Iteration 37/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013831, MSE: 0.027640, RMSE: 0.166254, R²: 0.412081\n",
      "VALIDATION -> Loss: 0.012702, MSE: 0.025407, RMSE: 0.159396, R²: 0.461316\n",
      "Iteration 38/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013677, MSE: 0.027383, RMSE: 0.165477, R²: 0.417563\n",
      "VALIDATION -> Loss: 0.013141, MSE: 0.026288, RMSE: 0.162136, R²: 0.442636\n",
      "Iteration 39/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013675, MSE: 0.027348, RMSE: 0.165372, R²: 0.418302\n",
      "VALIDATION -> Loss: 0.012892, MSE: 0.025790, RMSE: 0.160593, R²: 0.453197\n",
      "Iteration 40/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013558, MSE: 0.027125, RMSE: 0.164695, R²: 0.423051\n",
      "VALIDATION -> Loss: 0.012684, MSE: 0.025374, RMSE: 0.159291, R²: 0.462030\n",
      "Iteration 41/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013523, MSE: 0.027040, RMSE: 0.164440, R²: 0.424840\n",
      "VALIDATION -> Loss: 0.012794, MSE: 0.025587, RMSE: 0.159959, R²: 0.457504\n",
      "Iteration 42/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013393, MSE: 0.026814, RMSE: 0.163751, R²: 0.429647\n",
      "VALIDATION -> Loss: 0.012721, MSE: 0.025448, RMSE: 0.159526, R²: 0.460441\n",
      "Iteration 43/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013355, MSE: 0.026675, RMSE: 0.163323, R²: 0.432623\n",
      "VALIDATION -> Loss: 0.012941, MSE: 0.025887, RMSE: 0.160894, R²: 0.451148\n",
      "Iteration 44/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013299, MSE: 0.026577, RMSE: 0.163026, R²: 0.434688\n",
      "VALIDATION -> Loss: 0.012880, MSE: 0.025767, RMSE: 0.160523, R²: 0.453677\n",
      "Iteration 45/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013217, MSE: 0.026440, RMSE: 0.162603, R²: 0.437620\n",
      "VALIDATION -> Loss: 0.013071, MSE: 0.026147, RMSE: 0.161699, R²: 0.445637\n",
      "Iteration 46/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013168, MSE: 0.026343, RMSE: 0.162307, R²: 0.439666\n",
      "VALIDATION -> Loss: 0.012970, MSE: 0.025946, RMSE: 0.161077, R²: 0.449896\n",
      "Iteration 47/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013150, MSE: 0.026280, RMSE: 0.162110, R²: 0.441023\n",
      "VALIDATION -> Loss: 0.012765, MSE: 0.025532, RMSE: 0.159789, R²: 0.458660\n",
      "Iteration 48/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.013183, MSE: 0.026342, RMSE: 0.162302, R²: 0.439698\n",
      "VALIDATION -> Loss: 0.012814, MSE: 0.025632, RMSE: 0.160100, R²: 0.456552\n",
      "Iteration 49/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012991, MSE: 0.025949, RMSE: 0.161088, R²: 0.448051\n",
      "VALIDATION -> Loss: 0.012813, MSE: 0.025633, RMSE: 0.160103, R²: 0.456529\n",
      "Iteration 50/50 \n",
      "-----------------------------\n",
      "TRAINING -> Loss: 0.012998, MSE: 0.025983, RMSE: 0.161193, R²: 0.447325\n",
      "VALIDATION -> Loss: 0.012815, MSE: 0.025636, RMSE: 0.160112, R²: 0.456470\n"
     ]
    }
   ],
   "source": [
    "for i in range(50): \n",
    "    print(f\"Iteration {i+1}/50 \\n-----------------------------\")\n",
    "    train_loop(train_loader, model, lossFunction, optimizer)\n",
    "    val_loop(val_loader, model, lossFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.4953\n",
      "MAE: 0.1158\n",
      "MSE: 0.0237\n",
      "RMSE: 0.1540\n",
      "Precisión: 0.6180\n",
      "Recall: 0.9458\n",
      "NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(x_train)\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_train_pred_np = y_train_pred.cpu().numpy()\n",
    "\n",
    "# --- METRICS ---\n",
    "\n",
    "# R^2 Score\n",
    "ss_total = np.sum((y_train_np - np.mean(y_train_np)) ** 2)\n",
    "ss_residual = np.sum((y_train_np - y_train_pred_np) ** 2)\n",
    "r2_score = 1 - (ss_residual / ss_total) if ss_total != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "mae = np.mean(np.abs(y_train_np - y_train_pred_np))\n",
    "\n",
    "# MSE\n",
    "mse = np.mean((y_train_np - y_train_pred_np) ** 2)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# ACCURACY AND RECALL\n",
    "threshold = np.median(y_train_np)  \n",
    "\n",
    "y_train_bin = binarize(y_train_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "y_train_pred_bin = binarize(y_train_pred_np.reshape(-1, 1), threshold=threshold).flatten()\n",
    "\n",
    "precision = precision_score(y_train_bin, y_train_pred_bin)\n",
    "recall = recall_score(y_train_bin, y_train_pred_bin)\n",
    "\n",
    "# NDCG \n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  \n",
    "    y_true_sorted = np.take(y_true, order[:k])\n",
    "    \n",
    "    gains = 2 ** y_true_sorted - 1\n",
    "    discounts = np.log2(np.arange(2, len(y_true_sorted) + 2))\n",
    "    \n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best_dcg = dcg_score(y_true, y_true, k)  #\n",
    "    actual_dcg = dcg_score(y_true, y_score, k)\n",
    "    \n",
    "    return actual_dcg / best_dcg if best_dcg > 0 else 0\n",
    "\n",
    "ndcg = ndcg_score(y_train_np, y_train_pred_np)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Precisión: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
